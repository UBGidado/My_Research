{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL8IeHC0rRMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115eb4af-09ec-4f35-f108-6fa2683aa0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.12/dist-packages (3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch scikit-learn tqdm wget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip uninstall tensorflow tensorflow-gpu tf-keras -y\n",
        "!pip install tensorflow==2.19.0\n",
        "\n",
        "# Verify the installation\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"✅ TensorFlow 2.19.0 installed successfully!\")\n",
        "\n",
        "# Also install tensorflow-text for compatibility\n",
        "!pip install tensorflow-text==2.19.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2E3yjHANYYF",
        "outputId": "ffce0342-07cf-4114-c9ce-c4f9ca391fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tf-keras as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.19.0\n",
            "  Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tf_keras~=2.19, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.19.0\n",
            "TensorFlow version: 2.19.0\n",
            "✅ TensorFlow 2.19.0 installed successfully!\n",
            "Requirement already satisfied: tensorflow-text==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-text==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text==2.19.0) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "E_FN3ZGSs-iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nlpdata/dialogre.git\n",
        "%cd dialogre"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrnsM4zosfbf",
        "outputId": "8102e857-6a39-4d98-8f97-4f0fb3e7830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'dialogre' already exists and is not an empty directory.\n",
            "/content/dialogre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data_v2/en/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a5pi08Pv80M",
        "outputId": "5f6763ba-0d2e-4481-ed64-8e72e8600341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev.json  test.json  train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!cp data_v2/en/data/* data/\n",
        "!ls data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnI8d-WYw1BP",
        "outputId": "aa070b03-d9bc-4df5-9a0e-07a054103997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev.json  test.json  train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the pretrained model"
      ],
      "metadata": {
        "id": "dhJK4SNDyya-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Path where we’ll keep the model files\n",
        "BERT_BASE_DIR = \"/content/dialogre/bert/uncased_L-12_H-768_A-12\"\n",
        "\n",
        "# Download pretrained BERT (PyTorch version)\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Save them to the folder\n",
        "model.save_pretrained(BERT_BASE_DIR)\n",
        "tokenizer.save_pretrained(BERT_BASE_DIR)\n",
        "\n",
        "# Verify\n",
        "!ls -la $BERT_BASE_DIR"
      ],
      "metadata": {
        "id": "nioR4YPm31_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d25c8aa-2741-4a01-a7e6-916aff5d8190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1286692\n",
            "drwxr-xr-x 2 root root      4096 Aug 29 12:20 .\n",
            "drwxr-xr-x 6 root root      4096 Aug 29 11:49 ..\n",
            "-rw-r--r-- 1 root root       313 Aug 29 11:00 bert_config.json\n",
            "-rw-r--r-- 1 root root 440425712 Aug 29 11:00 bert_model.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      8528 Aug 29 11:00 bert_model.ckpt.index\n",
            "-rw-r--r-- 1 root root    904243 Aug 29 11:00 bert_model.ckpt.meta\n",
            "-rw-r--r-- 1 root root       618 Aug 29 12:43 config.json\n",
            "-rw-r--r-- 1 root root 437951328 Aug 29 12:43 model.safetensors\n",
            "-rw-r--r-- 1 root root 438005695 Aug 29 12:20 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root       125 Aug 29 12:43 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1272 Aug 29 12:43 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    231508 Aug 29 12:43 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data Directory in bert folder"
      ],
      "metadata": {
        "id": "Ca19PL9BIgaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_data_dir = \"/content/dialogre/bert/data\"\n",
        "if not os.path.exists(bert_data_dir):\n",
        "    print(\"Copying data directory to bert/ folder...\")\n",
        "    !cp -r /content/dialogre/data_v2/en/data /content/dialogre/bert/\n",
        "    print(\"Data copied successfully!\")\n",
        "else:\n",
        "    print(\"Data directory already exists in bert/ folder\")\n",
        "\n",
        "print(\"Contents of bert/data directory:\")\n",
        "!ls -la /content/dialogre/bert/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWzEAextGKRc",
        "outputId": "7f56273d-e513-47f1-c4b2-45bd81f334f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory already exists in bert/ folder\n",
            "Contents of bert/data directory:\n",
            "total 3740\n",
            "drwxr-xr-x 2 root root    4096 Aug 29 11:00 .\n",
            "drwxr-xr-x 6 root root    4096 Aug 29 11:49 ..\n",
            "-rw-r--r-- 1 root root  751932 Aug 29 11:00 dev.json\n",
            "-rw-r--r-- 1 root root  725696 Aug 29 11:00 test.json\n",
            "-rw-r--r-- 1 root root 2338606 Aug 29 11:00 train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert TF Checkpoint to PyTorch Format\n"
      ],
      "metadata": {
        "id": "VLRPwdLt6Flq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversion_script_path = \"/content/dialogre/bert/convert_tf_checkpoint_to_pytorch.py\"\n",
        "\n",
        "# Define paths for the conversion\n",
        "tf_checkpoint_path = os.path.join(BERT_BASE_DIR, \"bert_model.ckpt\")\n",
        "bert_config_file = os.path.join(BERT_BASE_DIR, \"bert_config.json\")\n",
        "pytorch_dump_path = os.path.join(BERT_BASE_DIR, \"pytorch_model.bin\")\n",
        "\n",
        "print(f\"TF checkpoint path: {tf_checkpoint_path}\")\n",
        "print(f\"BERT config file: {bert_config_file}\")\n",
        "print(f\"PyTorch output path: {pytorch_dump_path}\")\n",
        "\n",
        "# Run the conversion script exactly as specified in the README\n",
        "print(\"Running conversion script...\")\n",
        "!cd /content/dialogre/bert/ && python convert_tf_checkpoint_to_pytorch.py \\\n",
        "  --tf_checkpoint_path=\"$BERT_BASE_DIR/bert_model.ckpt\" \\\n",
        "  --bert_config_file=\"$BERT_BASE_DIR/bert_config.json\" \\\n",
        "  --pytorch_dump_path=\"$BERT_BASE_DIR/pytorch_model.bin\"\n",
        "\n",
        "print(\"Conversion completed!\")\n",
        "print(\"Files in BERT directory after conversion:\")\n",
        "!ls -la \"{BERT_BASE_DIR}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XrxQE1o9hTo",
        "outputId": "dd0e2c72-617f-49eb-c827-64cf038977f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF checkpoint path: /content/dialogre/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
            "BERT config file: /content/dialogre/bert/uncased_L-12_H-768_A-12/bert_config.json\n",
            "PyTorch output path: /content/dialogre/bert/uncased_L-12_H-768_A-12/pytorch_model.bin\n",
            "Running conversion script...\n",
            "2025-08-29 12:43:55.688728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756471435.720555   27148 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756471435.730098   27148 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756471435.752572   27148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756471435.752621   27148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756471435.752629   27148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756471435.752636   27148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Converting TensorFlow checkpoint from /content/dialogre/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
            "Loading bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Numpy array shape (512, 768)\n",
            "Loading bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Numpy array shape (2, 768)\n",
            "Loading bert/embeddings/word_embeddings with shape [30522, 768]\n",
            "2025-08-29 12:44:08.732854: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
            "Numpy array shape (30522, 768)\n",
            "Loading bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Numpy array shape (3072,)\n",
            "Loading bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Numpy array shape (768, 3072)\n",
            "Loading bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Numpy array shape (3072, 768)\n",
            "Loading bert/pooler/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading bert/pooler/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading cls/predictions/output_bias with shape [30522]\n",
            "Numpy array shape (30522,)\n",
            "Loading cls/predictions/transform/LayerNorm/beta with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading cls/predictions/transform/dense/bias with shape [768]\n",
            "Numpy array shape (768,)\n",
            "Loading cls/predictions/transform/dense/kernel with shape [768, 768]\n",
            "Numpy array shape (768, 768)\n",
            "Loading cls/seq_relationship/output_bias with shape [2]\n",
            "Numpy array shape (2,)\n",
            "Loading cls/seq_relationship/output_weights with shape [2, 768]\n",
            "Numpy array shape (2, 768)\n",
            "Loading embeddings/LayerNorm/beta\n",
            "Loading embeddings/LayerNorm/gamma\n",
            "Loading embeddings/position_embeddings\n",
            "Loading embeddings/token_type_embeddings\n",
            "Loading embeddings/word_embeddings\n",
            "Loading encoder/layer_0/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_0/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_0/attention/output/dense/bias\n",
            "Loading encoder/layer_0/attention/output/dense/kernel\n",
            "Loading encoder/layer_0/attention/self/key/bias\n",
            "Loading encoder/layer_0/attention/self/key/kernel\n",
            "Loading encoder/layer_0/attention/self/query/bias\n",
            "Loading encoder/layer_0/attention/self/query/kernel\n",
            "Loading encoder/layer_0/attention/self/value/bias\n",
            "Loading encoder/layer_0/attention/self/value/kernel\n",
            "Loading encoder/layer_0/intermediate/dense/bias\n",
            "Loading encoder/layer_0/intermediate/dense/kernel\n",
            "Loading encoder/layer_0/output/LayerNorm/beta\n",
            "Loading encoder/layer_0/output/LayerNorm/gamma\n",
            "Loading encoder/layer_0/output/dense/bias\n",
            "Loading encoder/layer_0/output/dense/kernel\n",
            "Loading encoder/layer_1/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_1/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_1/attention/output/dense/bias\n",
            "Loading encoder/layer_1/attention/output/dense/kernel\n",
            "Loading encoder/layer_1/attention/self/key/bias\n",
            "Loading encoder/layer_1/attention/self/key/kernel\n",
            "Loading encoder/layer_1/attention/self/query/bias\n",
            "Loading encoder/layer_1/attention/self/query/kernel\n",
            "Loading encoder/layer_1/attention/self/value/bias\n",
            "Loading encoder/layer_1/attention/self/value/kernel\n",
            "Loading encoder/layer_1/intermediate/dense/bias\n",
            "Loading encoder/layer_1/intermediate/dense/kernel\n",
            "Loading encoder/layer_1/output/LayerNorm/beta\n",
            "Loading encoder/layer_1/output/LayerNorm/gamma\n",
            "Loading encoder/layer_1/output/dense/bias\n",
            "Loading encoder/layer_1/output/dense/kernel\n",
            "Loading encoder/layer_10/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_10/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_10/attention/output/dense/bias\n",
            "Loading encoder/layer_10/attention/output/dense/kernel\n",
            "Loading encoder/layer_10/attention/self/key/bias\n",
            "Loading encoder/layer_10/attention/self/key/kernel\n",
            "Loading encoder/layer_10/attention/self/query/bias\n",
            "Loading encoder/layer_10/attention/self/query/kernel\n",
            "Loading encoder/layer_10/attention/self/value/bias\n",
            "Loading encoder/layer_10/attention/self/value/kernel\n",
            "Loading encoder/layer_10/intermediate/dense/bias\n",
            "Loading encoder/layer_10/intermediate/dense/kernel\n",
            "Loading encoder/layer_10/output/LayerNorm/beta\n",
            "Loading encoder/layer_10/output/LayerNorm/gamma\n",
            "Loading encoder/layer_10/output/dense/bias\n",
            "Loading encoder/layer_10/output/dense/kernel\n",
            "Loading encoder/layer_11/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_11/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_11/attention/output/dense/bias\n",
            "Loading encoder/layer_11/attention/output/dense/kernel\n",
            "Loading encoder/layer_11/attention/self/key/bias\n",
            "Loading encoder/layer_11/attention/self/key/kernel\n",
            "Loading encoder/layer_11/attention/self/query/bias\n",
            "Loading encoder/layer_11/attention/self/query/kernel\n",
            "Loading encoder/layer_11/attention/self/value/bias\n",
            "Loading encoder/layer_11/attention/self/value/kernel\n",
            "Loading encoder/layer_11/intermediate/dense/bias\n",
            "Loading encoder/layer_11/intermediate/dense/kernel\n",
            "Loading encoder/layer_11/output/LayerNorm/beta\n",
            "Loading encoder/layer_11/output/LayerNorm/gamma\n",
            "Loading encoder/layer_11/output/dense/bias\n",
            "Loading encoder/layer_11/output/dense/kernel\n",
            "Loading encoder/layer_2/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_2/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_2/attention/output/dense/bias\n",
            "Loading encoder/layer_2/attention/output/dense/kernel\n",
            "Loading encoder/layer_2/attention/self/key/bias\n",
            "Loading encoder/layer_2/attention/self/key/kernel\n",
            "Loading encoder/layer_2/attention/self/query/bias\n",
            "Loading encoder/layer_2/attention/self/query/kernel\n",
            "Loading encoder/layer_2/attention/self/value/bias\n",
            "Loading encoder/layer_2/attention/self/value/kernel\n",
            "Loading encoder/layer_2/intermediate/dense/bias\n",
            "Loading encoder/layer_2/intermediate/dense/kernel\n",
            "Loading encoder/layer_2/output/LayerNorm/beta\n",
            "Loading encoder/layer_2/output/LayerNorm/gamma\n",
            "Loading encoder/layer_2/output/dense/bias\n",
            "Loading encoder/layer_2/output/dense/kernel\n",
            "Loading encoder/layer_3/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_3/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_3/attention/output/dense/bias\n",
            "Loading encoder/layer_3/attention/output/dense/kernel\n",
            "Loading encoder/layer_3/attention/self/key/bias\n",
            "Loading encoder/layer_3/attention/self/key/kernel\n",
            "Loading encoder/layer_3/attention/self/query/bias\n",
            "Loading encoder/layer_3/attention/self/query/kernel\n",
            "Loading encoder/layer_3/attention/self/value/bias\n",
            "Loading encoder/layer_3/attention/self/value/kernel\n",
            "Loading encoder/layer_3/intermediate/dense/bias\n",
            "Loading encoder/layer_3/intermediate/dense/kernel\n",
            "Loading encoder/layer_3/output/LayerNorm/beta\n",
            "Loading encoder/layer_3/output/LayerNorm/gamma\n",
            "Loading encoder/layer_3/output/dense/bias\n",
            "Loading encoder/layer_3/output/dense/kernel\n",
            "Loading encoder/layer_4/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_4/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_4/attention/output/dense/bias\n",
            "Loading encoder/layer_4/attention/output/dense/kernel\n",
            "Loading encoder/layer_4/attention/self/key/bias\n",
            "Loading encoder/layer_4/attention/self/key/kernel\n",
            "Loading encoder/layer_4/attention/self/query/bias\n",
            "Loading encoder/layer_4/attention/self/query/kernel\n",
            "Loading encoder/layer_4/attention/self/value/bias\n",
            "Loading encoder/layer_4/attention/self/value/kernel\n",
            "Loading encoder/layer_4/intermediate/dense/bias\n",
            "Loading encoder/layer_4/intermediate/dense/kernel\n",
            "Loading encoder/layer_4/output/LayerNorm/beta\n",
            "Loading encoder/layer_4/output/LayerNorm/gamma\n",
            "Loading encoder/layer_4/output/dense/bias\n",
            "Loading encoder/layer_4/output/dense/kernel\n",
            "Loading encoder/layer_5/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_5/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_5/attention/output/dense/bias\n",
            "Loading encoder/layer_5/attention/output/dense/kernel\n",
            "Loading encoder/layer_5/attention/self/key/bias\n",
            "Loading encoder/layer_5/attention/self/key/kernel\n",
            "Loading encoder/layer_5/attention/self/query/bias\n",
            "Loading encoder/layer_5/attention/self/query/kernel\n",
            "Loading encoder/layer_5/attention/self/value/bias\n",
            "Loading encoder/layer_5/attention/self/value/kernel\n",
            "Loading encoder/layer_5/intermediate/dense/bias\n",
            "Loading encoder/layer_5/intermediate/dense/kernel\n",
            "Loading encoder/layer_5/output/LayerNorm/beta\n",
            "Loading encoder/layer_5/output/LayerNorm/gamma\n",
            "Loading encoder/layer_5/output/dense/bias\n",
            "Loading encoder/layer_5/output/dense/kernel\n",
            "Loading encoder/layer_6/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_6/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_6/attention/output/dense/bias\n",
            "Loading encoder/layer_6/attention/output/dense/kernel\n",
            "Loading encoder/layer_6/attention/self/key/bias\n",
            "Loading encoder/layer_6/attention/self/key/kernel\n",
            "Loading encoder/layer_6/attention/self/query/bias\n",
            "Loading encoder/layer_6/attention/self/query/kernel\n",
            "Loading encoder/layer_6/attention/self/value/bias\n",
            "Loading encoder/layer_6/attention/self/value/kernel\n",
            "Loading encoder/layer_6/intermediate/dense/bias\n",
            "Loading encoder/layer_6/intermediate/dense/kernel\n",
            "Loading encoder/layer_6/output/LayerNorm/beta\n",
            "Loading encoder/layer_6/output/LayerNorm/gamma\n",
            "Loading encoder/layer_6/output/dense/bias\n",
            "Loading encoder/layer_6/output/dense/kernel\n",
            "Loading encoder/layer_7/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_7/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_7/attention/output/dense/bias\n",
            "Loading encoder/layer_7/attention/output/dense/kernel\n",
            "Loading encoder/layer_7/attention/self/key/bias\n",
            "Loading encoder/layer_7/attention/self/key/kernel\n",
            "Loading encoder/layer_7/attention/self/query/bias\n",
            "Loading encoder/layer_7/attention/self/query/kernel\n",
            "Loading encoder/layer_7/attention/self/value/bias\n",
            "Loading encoder/layer_7/attention/self/value/kernel\n",
            "Loading encoder/layer_7/intermediate/dense/bias\n",
            "Loading encoder/layer_7/intermediate/dense/kernel\n",
            "Loading encoder/layer_7/output/LayerNorm/beta\n",
            "Loading encoder/layer_7/output/LayerNorm/gamma\n",
            "Loading encoder/layer_7/output/dense/bias\n",
            "Loading encoder/layer_7/output/dense/kernel\n",
            "Loading encoder/layer_8/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_8/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_8/attention/output/dense/bias\n",
            "Loading encoder/layer_8/attention/output/dense/kernel\n",
            "Loading encoder/layer_8/attention/self/key/bias\n",
            "Loading encoder/layer_8/attention/self/key/kernel\n",
            "Loading encoder/layer_8/attention/self/query/bias\n",
            "Loading encoder/layer_8/attention/self/query/kernel\n",
            "Loading encoder/layer_8/attention/self/value/bias\n",
            "Loading encoder/layer_8/attention/self/value/kernel\n",
            "Loading encoder/layer_8/intermediate/dense/bias\n",
            "Loading encoder/layer_8/intermediate/dense/kernel\n",
            "Loading encoder/layer_8/output/LayerNorm/beta\n",
            "Loading encoder/layer_8/output/LayerNorm/gamma\n",
            "Loading encoder/layer_8/output/dense/bias\n",
            "Loading encoder/layer_8/output/dense/kernel\n",
            "Loading encoder/layer_9/attention/output/LayerNorm/beta\n",
            "Loading encoder/layer_9/attention/output/LayerNorm/gamma\n",
            "Loading encoder/layer_9/attention/output/dense/bias\n",
            "Loading encoder/layer_9/attention/output/dense/kernel\n",
            "Loading encoder/layer_9/attention/self/key/bias\n",
            "Loading encoder/layer_9/attention/self/key/kernel\n",
            "Loading encoder/layer_9/attention/self/query/bias\n",
            "Loading encoder/layer_9/attention/self/query/kernel\n",
            "Loading encoder/layer_9/attention/self/value/bias\n",
            "Loading encoder/layer_9/attention/self/value/kernel\n",
            "Loading encoder/layer_9/intermediate/dense/bias\n",
            "Loading encoder/layer_9/intermediate/dense/kernel\n",
            "Loading encoder/layer_9/output/LayerNorm/beta\n",
            "Loading encoder/layer_9/output/LayerNorm/gamma\n",
            "Loading encoder/layer_9/output/dense/bias\n",
            "Loading encoder/layer_9/output/dense/kernel\n",
            "Loading pooler/dense/bias\n",
            "Loading pooler/dense/kernel\n",
            "Loading redictions/output_bias\n",
            "Skipping\n",
            "Loading redictions/transform/LayerNorm/beta\n",
            "Skipping\n",
            "Loading redictions/transform/LayerNorm/gamma\n",
            "Skipping\n",
            "Loading redictions/transform/dense/bias\n",
            "Skipping\n",
            "Loading redictions/transform/dense/kernel\n",
            "Skipping\n",
            "Loading eq_relationship/output_bias\n",
            "Skipping\n",
            "Loading eq_relationship/output_weights\n",
            "Skipping\n",
            "Conversion completed!\n",
            "Files in BERT directory after conversion:\n",
            "total 1286692\n",
            "drwxr-xr-x 2 root root      4096 Aug 29 12:20 .\n",
            "drwxr-xr-x 6 root root      4096 Aug 29 11:49 ..\n",
            "-rw-r--r-- 1 root root       313 Aug 29 11:00 bert_config.json\n",
            "-rw-r--r-- 1 root root 440425712 Aug 29 11:00 bert_model.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      8528 Aug 29 11:00 bert_model.ckpt.index\n",
            "-rw-r--r-- 1 root root    904243 Aug 29 11:00 bert_model.ckpt.meta\n",
            "-rw-r--r-- 1 root root       618 Aug 29 12:43 config.json\n",
            "-rw-r--r-- 1 root root 437951328 Aug 29 12:43 model.safetensors\n",
            "-rw-r--r-- 1 root root 438005695 Aug 29 12:44 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root       125 Aug 29 12:43 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1272 Aug 29 12:43 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    231508 Aug 29 12:43 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Session"
      ],
      "metadata": {
        "id": "gPAvGDMfc82c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch will use device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcBmNm-8eEDp",
        "outputId": "b86be50d-8b9d-4609-e748-1d32d052292a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch will use device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cea90d1c",
        "outputId": "6ce05e12-f7ce-4e3b-fd8d-b44b38b2e36d"
      },
      "source": [
        "import os\n",
        "if 'BERT_BASE_DIR' not in os.environ:\n",
        "    os.environ['BERT_BASE_DIR'] = '/content/dialogre/bert/uncased_L-12_H-768_A-12'\n",
        "    print(\"Setting BERT_BASE_DIR environment variable:\", os.environ['BERT_BASE_DIR'])\n",
        "else:\n",
        "    print(\"BERT_BASE_DIR environment variable is already set:\", os.environ['BERT_BASE_DIR'])\n",
        "\n",
        "# Execute the training, evaluation, and evaluation script\n",
        "!python ./bert/run_classifier.py \\\n",
        "  --task_name bert \\\n",
        "  --do_train --do_eval \\\n",
        "  --data_dir . \\\n",
        "  --vocab_file $BERT_BASE_DIR/vocab.txt \\\n",
        "  --bert_config_file $BERT_BASE_DIR/bert_config.json \\\n",
        "  --init_checkpoint $BERT_BASE_DIR/pytorch_model.bin \\\n",
        "  --max_seq_length 512 \\\n",
        "  --train_batch_size 24 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 20.0 \\\n",
        "  --output_dir bert_f1 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "&& rm bert_f1/model_best.pt \\\n",
        "&& cp -r bert_f1 bert_f1c \\\n",
        "&& python ./bert/run_classifier.py \\\n",
        "  --task_name bertf1c \\\n",
        "  --do_eval \\\n",
        "  --data_dir . \\\n",
        "  --vocab_file $BERT_BASE_DIR/vocab.txt \\\n",
        "  --bert_config_file $BERT_BASE_DIR/bert_config.json \\\n",
        "  --init_checkpoint $BERT_BASE_DIR/pytorch_model.bin \\\n",
        "  --max_seq_length 512 \\\n",
        "  --train_batch_size 24 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 20.0 \\\n",
        "  --output_dir bert_f1c \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "&& python evaluate.py \\\n",
        "  --f1dev bert_f1/logits_dev.txt \\\n",
        "  --f1test bert_f1/logits_test.txt \\\n",
        "  --f1cdev bert_f1c/logits_dev.txt \\\n",
        "  --f1ctest bert_f1c/logits_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT_BASE_DIR environment variable is already set: /content/dialogre/bert/uncased_L-12_H-768_A-12\n",
            "08/29/2025 12:44:16 - INFO - __main__ -   device cuda n_gpu 1 distributed training False\n",
            "08/29/2025 12:44:16 - INFO - __main__ -   5997,1914,1862\n",
            "#examples 1914\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   guid: dev-0\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey . speaker 3 : hey , man . what ' s up ? speaker 1 : maybe you can tell me . my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today . the first good thing she gets me in weeks . how could you not give me the message ? ! speaker 3 : well , i ' ll tell ya i do enjoy guilt , but , ah , it wasn ' t me . speaker 2 : yes , it was ! it was him ! uh huh ! okay , it was me ! speaker 1 : how is it you ? speaker 2 : well , it was just , it was all so crazy , you know . i mean , chandler was in the closet , counting to 10 , and he was up to 7 and i hadn ' t found a place to hide yet . i - i - i meant to tell you , and i wrote it all down on my hand . see , all of it . speaker 1 : yep , that ' s my audition . speaker 4 : see , now this is why i keep note ##pad ##s everywhere . speaker 2 : yep , and that ' s why we don ' t invite you to play . speaker 5 : what is the great tragedy here ? you go get yourself another appointment . speaker 1 : well , este ##lle tried , you know . the casting director told her that i missed my chance . speaker 2 : that is unfair . i ' ll call her and tell her it was totally my fault . speaker 1 : ph ##ee ##bs , you can ' t do that . the casting director doesn ' t talk to friends , she only talks to agents . speaker 2 : what a sad little life she must lead . okay , o ##oh . speaker 1 : what , what are you doing ? what are you doing ? speaker 2 : no , no , no , i know , i know , o ##oh . ' hi , this is kate ##lynn , from phoebe buff ##ay ' s office . um , is um , ann there for phoebe , she ' ll know what it ' s about . ' speaker 1 : hang up , hang up . speaker 2 : ' annie ! hi . listen we got a problem with joey tri ##bb ##iani , apparently he missed his audition . who did you speak to in my office ? este ##lle , no , i don ' t know what i ' m going to do with her . no . all right , so your husband leaves and burns down [SEP] este ##lle [SEP] speaker 1 [SEP]\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 1012 5882 1017 1024 4931 1010 2158 1012 2054 1005 1055 2039 1029 5882 1015 1024 2672 2017 2064 2425 2033 1012 2026 4005 2052 2066 2000 2113 2339 1045 2134 1005 1056 2265 2039 2012 1996 14597 1045 2134 1005 1056 2113 1045 2018 2651 1012 1996 2034 2204 2518 2016 4152 2033 1999 3134 1012 2129 2071 2017 2025 2507 2033 1996 4471 1029 999 5882 1017 1024 2092 1010 1045 1005 2222 2425 8038 1045 2079 5959 8056 1010 2021 1010 6289 1010 2009 2347 1005 1056 2033 1012 5882 1016 1024 2748 1010 2009 2001 999 2009 2001 2032 999 7910 9616 999 3100 1010 2009 2001 2033 999 5882 1015 1024 2129 2003 2009 2017 1029 5882 1016 1024 2092 1010 2009 2001 2074 1010 2009 2001 2035 2061 4689 1010 2017 2113 1012 1045 2812 1010 13814 2001 1999 1996 9346 1010 10320 2000 2184 1010 1998 2002 2001 2039 2000 1021 1998 1045 2910 1005 1056 2179 1037 2173 2000 5342 2664 1012 1045 1011 1045 1011 1045 3214 2000 2425 2017 1010 1998 1045 2626 2009 2035 2091 2006 2026 2192 1012 2156 1010 2035 1997 2009 1012 5882 1015 1024 15624 1010 2008 1005 1055 2026 14597 1012 5882 1018 1024 2156 1010 2085 2023 2003 2339 1045 2562 3602 15455 2015 7249 1012 5882 1016 1024 15624 1010 1998 2008 1005 1055 2339 2057 2123 1005 1056 13260 2017 2000 2377 1012 5882 1019 1024 2054 2003 1996 2307 10576 2182 1029 2017 2175 2131 4426 2178 6098 1012 5882 1015 1024 2092 1010 28517 6216 2699 1010 2017 2113 1012 1996 9179 2472 2409 2014 2008 1045 4771 2026 3382 1012 5882 1016 1024 2008 2003 15571 1012 1045 1005 2222 2655 2014 1998 2425 2014 2009 2001 6135 2026 6346 1012 5882 1015 1024 6887 4402 5910 1010 2017 2064 1005 1056 2079 2008 1012 1996 9179 2472 2987 1005 1056 2831 2000 2814 1010 2016 2069 7566 2000 6074 1012 5882 1016 1024 2054 1037 6517 2210 2166 2016 2442 2599 1012 3100 1010 1051 11631 1012 5882 1015 1024 2054 1010 2054 2024 2017 2725 1029 2054 2024 2017 2725 1029 5882 1016 1024 2053 1010 2053 1010 2053 1010 1045 2113 1010 1045 2113 1010 1051 11631 1012 1005 7632 1010 2023 2003 5736 27610 1010 2013 18188 23176 4710 1005 1055 2436 1012 8529 1010 2003 8529 1010 5754 2045 2005 18188 1010 2016 1005 2222 2113 2054 2009 1005 1055 2055 1012 1005 5882 1015 1024 6865 2039 1010 6865 2039 1012 5882 1016 1024 1005 8194 999 7632 1012 4952 2057 2288 1037 3291 2007 9558 13012 10322 25443 1010 4593 2002 4771 2010 14597 1012 2040 2106 2017 3713 2000 1999 2026 2436 1029 28517 6216 1010 2053 1010 1045 2123 1005 1056 2113 2054 1045 1005 1049 2183 2000 2079 2007 2014 1012 2053 1012 2035 2157 1010 2061 2115 3129 3727 1998 7641 2091 102 28517 6216 102 5882 1015 102\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   guid: dev-1\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey . speaker 3 : hey , man . what ' s up ? speaker 1 : maybe you can tell me . my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today . the first good thing she gets me in weeks . how could you not give me the message ? ! speaker 3 : well , i ' ll tell ya i do enjoy guilt , but , ah , it wasn ' t me . speaker 2 : yes , it was ! it was him ! uh huh ! okay , it was me ! speaker 1 : how is it you ? speaker 2 : well , it was just , it was all so crazy , you know . i mean , chandler was in the closet , counting to 10 , and he was up to 7 and i hadn ' t found a place to hide yet . i - i - i meant to tell you , and i wrote it all down on my hand . see , all of it . speaker 1 : yep , that ' s my audition . speaker 4 : see , now this is why i keep note ##pad ##s everywhere . speaker 2 : yep , and that ' s why we don ' t invite you to play . speaker 5 : what is the great tragedy here ? you go get yourself another appointment . speaker 1 : well , este ##lle tried , you know . the casting director told her that i missed my chance . speaker 2 : that is unfair . i ' ll call her and tell her it was totally my fault . speaker 1 : ph ##ee ##bs , you can ' t do that . the casting director doesn ' t talk to friends , she only talks to agents . speaker 2 : what a sad little life she must lead . okay , o ##oh . speaker 1 : what , what are you doing ? what are you doing ? speaker 2 : no , no , no , i know , i know , o ##oh . ' hi , this is kate ##lynn , from phoebe buff ##ay ' s office . um , is um , ann there for phoebe , she ' ll know what it ' s about . ' speaker 1 : hang up , hang up . speaker 2 : ' annie ! hi . listen we got a problem with joey tri ##bb ##iani , apparently he missed his audition . who did you speak to in my office ? este ##lle , no , i don ' t know what i ' m going to do with her . no . all right , so your husband leaves and burns down the [SEP] este ##lle [SEP] agent [SEP]\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 1012 5882 1017 1024 4931 1010 2158 1012 2054 1005 1055 2039 1029 5882 1015 1024 2672 2017 2064 2425 2033 1012 2026 4005 2052 2066 2000 2113 2339 1045 2134 1005 1056 2265 2039 2012 1996 14597 1045 2134 1005 1056 2113 1045 2018 2651 1012 1996 2034 2204 2518 2016 4152 2033 1999 3134 1012 2129 2071 2017 2025 2507 2033 1996 4471 1029 999 5882 1017 1024 2092 1010 1045 1005 2222 2425 8038 1045 2079 5959 8056 1010 2021 1010 6289 1010 2009 2347 1005 1056 2033 1012 5882 1016 1024 2748 1010 2009 2001 999 2009 2001 2032 999 7910 9616 999 3100 1010 2009 2001 2033 999 5882 1015 1024 2129 2003 2009 2017 1029 5882 1016 1024 2092 1010 2009 2001 2074 1010 2009 2001 2035 2061 4689 1010 2017 2113 1012 1045 2812 1010 13814 2001 1999 1996 9346 1010 10320 2000 2184 1010 1998 2002 2001 2039 2000 1021 1998 1045 2910 1005 1056 2179 1037 2173 2000 5342 2664 1012 1045 1011 1045 1011 1045 3214 2000 2425 2017 1010 1998 1045 2626 2009 2035 2091 2006 2026 2192 1012 2156 1010 2035 1997 2009 1012 5882 1015 1024 15624 1010 2008 1005 1055 2026 14597 1012 5882 1018 1024 2156 1010 2085 2023 2003 2339 1045 2562 3602 15455 2015 7249 1012 5882 1016 1024 15624 1010 1998 2008 1005 1055 2339 2057 2123 1005 1056 13260 2017 2000 2377 1012 5882 1019 1024 2054 2003 1996 2307 10576 2182 1029 2017 2175 2131 4426 2178 6098 1012 5882 1015 1024 2092 1010 28517 6216 2699 1010 2017 2113 1012 1996 9179 2472 2409 2014 2008 1045 4771 2026 3382 1012 5882 1016 1024 2008 2003 15571 1012 1045 1005 2222 2655 2014 1998 2425 2014 2009 2001 6135 2026 6346 1012 5882 1015 1024 6887 4402 5910 1010 2017 2064 1005 1056 2079 2008 1012 1996 9179 2472 2987 1005 1056 2831 2000 2814 1010 2016 2069 7566 2000 6074 1012 5882 1016 1024 2054 1037 6517 2210 2166 2016 2442 2599 1012 3100 1010 1051 11631 1012 5882 1015 1024 2054 1010 2054 2024 2017 2725 1029 2054 2024 2017 2725 1029 5882 1016 1024 2053 1010 2053 1010 2053 1010 1045 2113 1010 1045 2113 1010 1051 11631 1012 1005 7632 1010 2023 2003 5736 27610 1010 2013 18188 23176 4710 1005 1055 2436 1012 8529 1010 2003 8529 1010 5754 2045 2005 18188 1010 2016 1005 2222 2113 2054 2009 1005 1055 2055 1012 1005 5882 1015 1024 6865 2039 1010 6865 2039 1012 5882 1016 1024 1005 8194 999 7632 1012 4952 2057 2288 1037 3291 2007 9558 13012 10322 25443 1010 4593 2002 4771 2010 14597 1012 2040 2106 2017 3713 2000 1999 2026 2436 1029 28517 6216 1010 2053 1010 1045 2123 1005 1056 2113 2054 1045 1005 1049 2183 2000 2079 2007 2014 1012 2053 1012 2035 2157 1010 2061 2115 3129 3727 1998 7641 2091 1996 102 28517 6216 102 4005 102\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   guid: dev-2\n",
            "08/29/2025 12:44:19 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey . speaker 3 : hey , man . what ' s up ? speaker 1 : maybe you can tell me . my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today . the first good thing she gets me in weeks . how could you not give me the message ? ! speaker 3 : well , i ' ll tell ya i do enjoy guilt , but , ah , it wasn ' t me . speaker 2 : yes , it was ! it was him ! uh huh ! okay , it was me ! speaker 1 : how is it you ? speaker 2 : well , it was just , it was all so crazy , you know . i mean , chandler was in the closet , counting to 10 , and he was up to 7 and i hadn ' t found a place to hide yet . i - i - i meant to tell you , and i wrote it all down on my hand . see , all of it . speaker 1 : yep , that ' s my audition . speaker 4 : see , now this is why i keep note ##pad ##s everywhere . speaker 2 : yep , and that ' s why we don ' t invite you to play . speaker 5 : what is the great tragedy here ? you go get yourself another appointment . speaker 1 : well , este ##lle tried , you know . the casting director told her that i missed my chance . speaker 2 : that is unfair . i ' ll call her and tell her it was totally my fault . speaker 1 : ph ##ee ##bs , you can ' t do that . the casting director doesn ' t talk to friends , she only talks to agents . speaker 2 : what a sad little life she must lead . okay , o ##oh . speaker 1 : what , what are you doing ? what are you doing ? speaker 2 : no , no , no , i know , i know , o ##oh . ' hi , this is kate ##lynn , from phoebe buff ##ay ' s office . um , is um , ann there for phoebe , she ' ll know what it ' s about . ' speaker 1 : hang up , hang up . speaker 2 : ' annie ! hi . listen we got a problem with joey tri ##bb ##iani , apparently he missed his audition . who did you speak to in my office ? este ##lle , no , i don ' t know what i ' m going to do with her . no . all right , so your husband leaves and burns down the [SEP] annie [SEP] casting director [SEP]\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 1012 5882 1017 1024 4931 1010 2158 1012 2054 1005 1055 2039 1029 5882 1015 1024 2672 2017 2064 2425 2033 1012 2026 4005 2052 2066 2000 2113 2339 1045 2134 1005 1056 2265 2039 2012 1996 14597 1045 2134 1005 1056 2113 1045 2018 2651 1012 1996 2034 2204 2518 2016 4152 2033 1999 3134 1012 2129 2071 2017 2025 2507 2033 1996 4471 1029 999 5882 1017 1024 2092 1010 1045 1005 2222 2425 8038 1045 2079 5959 8056 1010 2021 1010 6289 1010 2009 2347 1005 1056 2033 1012 5882 1016 1024 2748 1010 2009 2001 999 2009 2001 2032 999 7910 9616 999 3100 1010 2009 2001 2033 999 5882 1015 1024 2129 2003 2009 2017 1029 5882 1016 1024 2092 1010 2009 2001 2074 1010 2009 2001 2035 2061 4689 1010 2017 2113 1012 1045 2812 1010 13814 2001 1999 1996 9346 1010 10320 2000 2184 1010 1998 2002 2001 2039 2000 1021 1998 1045 2910 1005 1056 2179 1037 2173 2000 5342 2664 1012 1045 1011 1045 1011 1045 3214 2000 2425 2017 1010 1998 1045 2626 2009 2035 2091 2006 2026 2192 1012 2156 1010 2035 1997 2009 1012 5882 1015 1024 15624 1010 2008 1005 1055 2026 14597 1012 5882 1018 1024 2156 1010 2085 2023 2003 2339 1045 2562 3602 15455 2015 7249 1012 5882 1016 1024 15624 1010 1998 2008 1005 1055 2339 2057 2123 1005 1056 13260 2017 2000 2377 1012 5882 1019 1024 2054 2003 1996 2307 10576 2182 1029 2017 2175 2131 4426 2178 6098 1012 5882 1015 1024 2092 1010 28517 6216 2699 1010 2017 2113 1012 1996 9179 2472 2409 2014 2008 1045 4771 2026 3382 1012 5882 1016 1024 2008 2003 15571 1012 1045 1005 2222 2655 2014 1998 2425 2014 2009 2001 6135 2026 6346 1012 5882 1015 1024 6887 4402 5910 1010 2017 2064 1005 1056 2079 2008 1012 1996 9179 2472 2987 1005 1056 2831 2000 2814 1010 2016 2069 7566 2000 6074 1012 5882 1016 1024 2054 1037 6517 2210 2166 2016 2442 2599 1012 3100 1010 1051 11631 1012 5882 1015 1024 2054 1010 2054 2024 2017 2725 1029 2054 2024 2017 2725 1029 5882 1016 1024 2053 1010 2053 1010 2053 1010 1045 2113 1010 1045 2113 1010 1051 11631 1012 1005 7632 1010 2023 2003 5736 27610 1010 2013 18188 23176 4710 1005 1055 2436 1012 8529 1010 2003 8529 1010 5754 2045 2005 18188 1010 2016 1005 2222 2113 2054 2009 1005 1055 2055 1012 1005 5882 1015 1024 6865 2039 1010 6865 2039 1012 5882 1016 1024 1005 8194 999 7632 1012 4952 2057 2288 1037 3291 2007 9558 13012 10322 25443 1010 4593 2002 4771 2010 14597 1012 2040 2106 2017 3713 2000 1999 2026 2436 1029 28517 6216 1010 2053 1010 1045 2123 1005 1056 2113 2054 1045 1005 1049 2183 2000 2079 2007 2014 1012 2053 1012 2035 2157 1010 2061 2115 3129 3727 1998 7641 2091 1996 102 8194 102 9179 2472 102\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   guid: dev-3\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey . speaker 3 : hey , man . what ' s up ? speaker 1 : maybe you can tell me . my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today . the first good thing she gets me in weeks . how could you not give me the message ? ! speaker 3 : well , i ' ll tell ya i do enjoy guilt , but , ah , it wasn ' t me . speaker 2 : yes , it was ! it was him ! uh huh ! okay , it was me ! speaker 1 : how is it you ? speaker 2 : well , it was just , it was all so crazy , you know . i mean , chandler was in the closet , counting to 10 , and he was up to 7 and i hadn ' t found a place to hide yet . i - i - i meant to tell you , and i wrote it all down on my hand . see , all of it . speaker 1 : yep , that ' s my audition . speaker 4 : see , now this is why i keep note ##pad ##s everywhere . speaker 2 : yep , and that ' s why we don ' t invite you to play . speaker 5 : what is the great tragedy here ? you go get yourself another appointment . speaker 1 : well , este ##lle tried , you know . the casting director told her that i missed my chance . speaker 2 : that is unfair . i ' ll call her and tell her it was totally my fault . speaker 1 : ph ##ee ##bs , you can ' t do that . the casting director doesn ' t talk to friends , she only talks to agents . speaker 2 : what a sad little life she must lead . okay , o ##oh . speaker 1 : what , what are you doing ? what are you doing ? speaker 2 : no , no , no , i know , i know , o ##oh . ' hi , this is kate ##lynn , from phoebe buff ##ay ' s office . um , is um , ann there for phoebe , she ' ll know what it ' s about . ' speaker 1 : hang up , hang up . speaker 2 : ' annie ! hi . listen we got a problem with joey tri ##bb ##iani , apparently he missed his audition . who did you speak to in my office ? este ##lle , no , i don ' t know what i ' m going to do with her . no . all right , so your husband leaves and burns down [SEP] speaker 1 [SEP] speaker 2 [SEP]\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 1012 5882 1017 1024 4931 1010 2158 1012 2054 1005 1055 2039 1029 5882 1015 1024 2672 2017 2064 2425 2033 1012 2026 4005 2052 2066 2000 2113 2339 1045 2134 1005 1056 2265 2039 2012 1996 14597 1045 2134 1005 1056 2113 1045 2018 2651 1012 1996 2034 2204 2518 2016 4152 2033 1999 3134 1012 2129 2071 2017 2025 2507 2033 1996 4471 1029 999 5882 1017 1024 2092 1010 1045 1005 2222 2425 8038 1045 2079 5959 8056 1010 2021 1010 6289 1010 2009 2347 1005 1056 2033 1012 5882 1016 1024 2748 1010 2009 2001 999 2009 2001 2032 999 7910 9616 999 3100 1010 2009 2001 2033 999 5882 1015 1024 2129 2003 2009 2017 1029 5882 1016 1024 2092 1010 2009 2001 2074 1010 2009 2001 2035 2061 4689 1010 2017 2113 1012 1045 2812 1010 13814 2001 1999 1996 9346 1010 10320 2000 2184 1010 1998 2002 2001 2039 2000 1021 1998 1045 2910 1005 1056 2179 1037 2173 2000 5342 2664 1012 1045 1011 1045 1011 1045 3214 2000 2425 2017 1010 1998 1045 2626 2009 2035 2091 2006 2026 2192 1012 2156 1010 2035 1997 2009 1012 5882 1015 1024 15624 1010 2008 1005 1055 2026 14597 1012 5882 1018 1024 2156 1010 2085 2023 2003 2339 1045 2562 3602 15455 2015 7249 1012 5882 1016 1024 15624 1010 1998 2008 1005 1055 2339 2057 2123 1005 1056 13260 2017 2000 2377 1012 5882 1019 1024 2054 2003 1996 2307 10576 2182 1029 2017 2175 2131 4426 2178 6098 1012 5882 1015 1024 2092 1010 28517 6216 2699 1010 2017 2113 1012 1996 9179 2472 2409 2014 2008 1045 4771 2026 3382 1012 5882 1016 1024 2008 2003 15571 1012 1045 1005 2222 2655 2014 1998 2425 2014 2009 2001 6135 2026 6346 1012 5882 1015 1024 6887 4402 5910 1010 2017 2064 1005 1056 2079 2008 1012 1996 9179 2472 2987 1005 1056 2831 2000 2814 1010 2016 2069 7566 2000 6074 1012 5882 1016 1024 2054 1037 6517 2210 2166 2016 2442 2599 1012 3100 1010 1051 11631 1012 5882 1015 1024 2054 1010 2054 2024 2017 2725 1029 2054 2024 2017 2725 1029 5882 1016 1024 2053 1010 2053 1010 2053 1010 1045 2113 1010 1045 2113 1010 1051 11631 1012 1005 7632 1010 2023 2003 5736 27610 1010 2013 18188 23176 4710 1005 1055 2436 1012 8529 1010 2003 8529 1010 5754 2045 2005 18188 1010 2016 1005 2222 2113 2054 2009 1005 1055 2055 1012 1005 5882 1015 1024 6865 2039 1010 6865 2039 1012 5882 1016 1024 1005 8194 999 7632 1012 4952 2057 2288 1037 3291 2007 9558 13012 10322 25443 1010 4593 2002 4771 2010 14597 1012 2040 2106 2017 3713 2000 1999 2026 2436 1029 28517 6216 1010 2053 1010 1045 2123 1005 1056 2113 2054 1045 1005 1049 2183 2000 2079 2007 2014 1012 2053 1012 2035 2157 1010 2061 2115 3129 3727 1998 7641 2091 102 5882 1015 102 5882 1016 102\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   guid: dev-4\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey . speaker 3 : hey , man . what ' s up ? speaker 1 : maybe you can tell me . my agent would like to know why i didn ' t show up at the audition i didn ' t know i had today . the first good thing she gets me in weeks . how could you not give me the message ? ! speaker 3 : well , i ' ll tell ya i do enjoy guilt , but , ah , it wasn ' t me . speaker 2 : yes , it was ! it was him ! uh huh ! okay , it was me ! speaker 1 : how is it you ? speaker 2 : well , it was just , it was all so crazy , you know . i mean , chandler was in the closet , counting to 10 , and he was up to 7 and i hadn ' t found a place to hide yet . i - i - i meant to tell you , and i wrote it all down on my hand . see , all of it . speaker 1 : yep , that ' s my audition . speaker 4 : see , now this is why i keep note ##pad ##s everywhere . speaker 2 : yep , and that ' s why we don ' t invite you to play . speaker 5 : what is the great tragedy here ? you go get yourself another appointment . speaker 1 : well , este ##lle tried , you know . the casting director told her that i missed my chance . speaker 2 : that is unfair . i ' ll call her and tell her it was totally my fault . speaker 1 : ph ##ee ##bs , you can ' t do that . the casting director doesn ' t talk to friends , she only talks to agents . speaker 2 : what a sad little life she must lead . okay , o ##oh . speaker 1 : what , what are you doing ? what are you doing ? speaker 2 : no , no , no , i know , i know , o ##oh . ' hi , this is kate ##lynn , from phoebe buff ##ay ' s office . um , is um , ann there for phoebe , she ' ll know what it ' s about . ' speaker 1 : hang up , hang up . speaker 2 : ' annie ! hi . listen we got a problem with joey tri ##bb ##iani , apparently he missed his audition . who did you speak to in my office ? este ##lle , no , i don ' t know what i ' m going to do with her . no . all right , so your husband leaves and [SEP] speaker 1 [SEP] joey tri ##bb ##iani [SEP]\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 1012 5882 1017 1024 4931 1010 2158 1012 2054 1005 1055 2039 1029 5882 1015 1024 2672 2017 2064 2425 2033 1012 2026 4005 2052 2066 2000 2113 2339 1045 2134 1005 1056 2265 2039 2012 1996 14597 1045 2134 1005 1056 2113 1045 2018 2651 1012 1996 2034 2204 2518 2016 4152 2033 1999 3134 1012 2129 2071 2017 2025 2507 2033 1996 4471 1029 999 5882 1017 1024 2092 1010 1045 1005 2222 2425 8038 1045 2079 5959 8056 1010 2021 1010 6289 1010 2009 2347 1005 1056 2033 1012 5882 1016 1024 2748 1010 2009 2001 999 2009 2001 2032 999 7910 9616 999 3100 1010 2009 2001 2033 999 5882 1015 1024 2129 2003 2009 2017 1029 5882 1016 1024 2092 1010 2009 2001 2074 1010 2009 2001 2035 2061 4689 1010 2017 2113 1012 1045 2812 1010 13814 2001 1999 1996 9346 1010 10320 2000 2184 1010 1998 2002 2001 2039 2000 1021 1998 1045 2910 1005 1056 2179 1037 2173 2000 5342 2664 1012 1045 1011 1045 1011 1045 3214 2000 2425 2017 1010 1998 1045 2626 2009 2035 2091 2006 2026 2192 1012 2156 1010 2035 1997 2009 1012 5882 1015 1024 15624 1010 2008 1005 1055 2026 14597 1012 5882 1018 1024 2156 1010 2085 2023 2003 2339 1045 2562 3602 15455 2015 7249 1012 5882 1016 1024 15624 1010 1998 2008 1005 1055 2339 2057 2123 1005 1056 13260 2017 2000 2377 1012 5882 1019 1024 2054 2003 1996 2307 10576 2182 1029 2017 2175 2131 4426 2178 6098 1012 5882 1015 1024 2092 1010 28517 6216 2699 1010 2017 2113 1012 1996 9179 2472 2409 2014 2008 1045 4771 2026 3382 1012 5882 1016 1024 2008 2003 15571 1012 1045 1005 2222 2655 2014 1998 2425 2014 2009 2001 6135 2026 6346 1012 5882 1015 1024 6887 4402 5910 1010 2017 2064 1005 1056 2079 2008 1012 1996 9179 2472 2987 1005 1056 2831 2000 2814 1010 2016 2069 7566 2000 6074 1012 5882 1016 1024 2054 1037 6517 2210 2166 2016 2442 2599 1012 3100 1010 1051 11631 1012 5882 1015 1024 2054 1010 2054 2024 2017 2725 1029 2054 2024 2017 2725 1029 5882 1016 1024 2053 1010 2053 1010 2053 1010 1045 2113 1010 1045 2113 1010 1051 11631 1012 1005 7632 1010 2023 2003 5736 27610 1010 2013 18188 23176 4710 1005 1055 2436 1012 8529 1010 2003 8529 1010 5754 2045 2005 18188 1010 2016 1005 2222 2113 2054 2009 1005 1055 2055 1012 1005 5882 1015 1024 6865 2039 1010 6865 2039 1012 5882 1016 1024 1005 8194 999 7632 1012 4952 2057 2288 1037 3291 2007 9558 13012 10322 25443 1010 4593 2002 4771 2010 14597 1012 2040 2106 2017 3713 2000 1999 2026 2436 1029 28517 6216 1010 2053 1010 1045 2123 1005 1056 2113 2054 1045 1005 1049 2183 2000 2079 2007 2014 1012 2053 1012 2035 2157 1010 2061 2115 3129 3727 1998 102 5882 1015 102 9558 13012 10322 25443 102\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            "#features 1914\n",
            "#examples 5997\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   guid: train-0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey , sophie ! speaker 2 : hey , ra ##ch ! speaker 3 : hey . speaker 2 : hey . speaker 1 : thanks for lunch , chandler . y ' know , you didn ' t have to walk me all the way back up here . speaker 3 : oh , that ' s - that ' s okay , no problem . speaker 1 : honey um , honey , you do realise that we don ' t keep the women ' s linger ##ie here in the office ? speaker 3 : yes , i realise that . speaker 1 : summer catalogue ! speaker 3 : that ' s the stuff ! [SEP] speaker 1 [SEP] ra ##ch [SEP]\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 1010 8234 999 5882 1016 1024 4931 1010 10958 2818 999 5882 1017 1024 4931 1012 5882 1016 1024 4931 1012 5882 1015 1024 4283 2005 6265 1010 13814 1012 1061 1005 2113 1010 2017 2134 1005 1056 2031 2000 3328 2033 2035 1996 2126 2067 2039 2182 1012 5882 1017 1024 2821 1010 2008 1005 1055 1011 2008 1005 1055 3100 1010 2053 3291 1012 5882 1015 1024 6861 8529 1010 6861 1010 2017 2079 19148 2008 2057 2123 1005 1056 2562 1996 2308 1005 1055 26577 2666 2182 1999 1996 2436 1029 5882 1017 1024 2748 1010 1045 19148 2008 1012 5882 1015 1024 2621 10161 999 5882 1017 1024 2008 1005 1055 1996 4933 999 102 5882 1015 102 10958 2818 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   guid: train-1\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey , sophie ! speaker 2 : hey , ra ##ch ! speaker 3 : hey . speaker 2 : hey . speaker 1 : thanks for lunch , chandler . y ' know , you didn ' t have to walk me all the way back up here . speaker 3 : oh , that ' s - that ' s okay , no problem . speaker 1 : honey um , honey , you do realise that we don ' t keep the women ' s linger ##ie here in the office ? speaker 3 : yes , i realise that . speaker 1 : summer catalogue ! speaker 3 : that ' s the stuff ! [SEP] speaker 1 [SEP] honey [SEP]\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 1010 8234 999 5882 1016 1024 4931 1010 10958 2818 999 5882 1017 1024 4931 1012 5882 1016 1024 4931 1012 5882 1015 1024 4283 2005 6265 1010 13814 1012 1061 1005 2113 1010 2017 2134 1005 1056 2031 2000 3328 2033 2035 1996 2126 2067 2039 2182 1012 5882 1017 1024 2821 1010 2008 1005 1055 1011 2008 1005 1055 3100 1010 2053 3291 1012 5882 1015 1024 6861 8529 1010 6861 1010 2017 2079 19148 2008 2057 2123 1005 1056 2562 1996 2308 1005 1055 26577 2666 2182 1999 1996 2436 1029 5882 1017 1024 2748 1010 1045 19148 2008 1012 5882 1015 1024 2621 10161 999 5882 1017 1024 2008 1005 1055 1996 4933 999 102 5882 1015 102 6861 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   guid: train-2\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey , sophie ! speaker 2 : hey , ra ##ch ! speaker 3 : hey . speaker 2 : hey . speaker 1 : thanks for lunch , chandler . y ' know , you didn ' t have to walk me all the way back up here . speaker 3 : oh , that ' s - that ' s okay , no problem . speaker 1 : honey um , honey , you do realise that we don ' t keep the women ' s linger ##ie here in the office ? speaker 3 : yes , i realise that . speaker 1 : summer catalogue ! speaker 3 : that ' s the stuff ! [SEP] speaker 3 [SEP] honey [SEP]\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 1010 8234 999 5882 1016 1024 4931 1010 10958 2818 999 5882 1017 1024 4931 1012 5882 1016 1024 4931 1012 5882 1015 1024 4283 2005 6265 1010 13814 1012 1061 1005 2113 1010 2017 2134 1005 1056 2031 2000 3328 2033 2035 1996 2126 2067 2039 2182 1012 5882 1017 1024 2821 1010 2008 1005 1055 1011 2008 1005 1055 3100 1010 2053 3291 1012 5882 1015 1024 6861 8529 1010 6861 1010 2017 2079 19148 2008 2057 2123 1005 1056 2562 1996 2308 1005 1055 26577 2666 2182 1999 1996 2436 1029 5882 1017 1024 2748 1010 1045 19148 2008 1012 5882 1015 1024 2621 10161 999 5882 1017 1024 2008 1005 1055 1996 4933 999 102 5882 1017 102 6861 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   guid: train-3\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey ! speaker 1 : you ’ ll never believe what just happened , ross just totally blew me off and he didn ’ t even tell me why ! speaker 2 : oh ##hh well . y ’ know what honey ? the best thing to do to get over a guy is to start dating someone else . oh ! there is this great guy you will love at work named bob ! he ’ s a real up - and - come ##r in human resources . speaker 1 : y ’ know , thanks for trying to cheer me up , but i ’ m not gonna date some random guy from your work . speaker 2 : it ’ s not random , it ’ s bob . speaker 1 : it ’ s probably because not mature enough . or smart enough . maybe he doesn ’ t like the way i dress — no that can ’ t be it . it ’ s really gotta be the smart thing . oh i ’ m so stupid ! i ’ m just like this incredibly pretty stupid girl ! speaker 2 : no honey , okay , okay , you wanna know why ross canceled the date ? because i asked him to . speaker 1 : you asked him too ? ! speaker 2 : hm - mmm . speaker 1 : why ? ! speaker 2 : because you are my sister and ross and i have this huge history … speaker 1 : i don ’ t understand , do you want to go out with ross ? speaker 2 : no . speaker 1 : you don ’ t want him , but you don ’ t want me to have him ? speaker 2 : y ’ know bob in human resources … speaker 1 : u ##gh ! i cannot believe you did this too me ! you had me doubt ##ing how smart i was ! you had me doubt ##ing my fashion sense ! speaker 2 : look , this is not that big of a deal ! you just don ’ t date ross ! there ’ s a million other guys out there , you just … speaker 1 : hey ! you have no right to tell me what to do . speaker 2 : i ’ m not telling you what to do ! i am telling you what not to do ! speaker 1 : why are you so jealous of me ? speaker 2 : jill this is not about me being jealous of you ! this is about you being a brat ! wanting what you can ’ t have ! speaker 1 : can ’ t have ? ! excuse me , the only thing i can ’ t have is dairy ! speaker 2 : all right , all right , well [SEP] speaker 2 [SEP] speaker 1 [SEP]\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 999 5882 1015 1024 2017 1521 2222 2196 2903 2054 2074 3047 1010 5811 2074 6135 8682 2033 2125 1998 2002 2134 1521 1056 2130 2425 2033 2339 999 5882 1016 1024 2821 23644 2092 1012 1061 1521 2113 2054 6861 1029 1996 2190 2518 2000 2079 2000 2131 2058 1037 3124 2003 2000 2707 5306 2619 2842 1012 2821 999 2045 2003 2023 2307 3124 2017 2097 2293 2012 2147 2315 3960 999 2002 1521 1055 1037 2613 2039 1011 1998 1011 2272 2099 1999 2529 4219 1012 5882 1015 1024 1061 1521 2113 1010 4283 2005 2667 2000 15138 2033 2039 1010 2021 1045 1521 1049 2025 6069 3058 2070 6721 3124 2013 2115 2147 1012 5882 1016 1024 2009 1521 1055 2025 6721 1010 2009 1521 1055 3960 1012 5882 1015 1024 2009 1521 1055 2763 2138 2025 9677 2438 1012 2030 6047 2438 1012 2672 2002 2987 1521 1056 2066 1996 2126 1045 4377 1517 2053 2008 2064 1521 1056 2022 2009 1012 2009 1521 1055 2428 10657 2022 1996 6047 2518 1012 2821 1045 1521 1049 2061 5236 999 1045 1521 1049 2074 2066 2023 11757 3492 5236 2611 999 5882 1016 1024 2053 6861 1010 3100 1010 3100 1010 2017 10587 2113 2339 5811 13261 1996 3058 1029 2138 1045 2356 2032 2000 1012 5882 1015 1024 2017 2356 2032 2205 1029 999 5882 1016 1024 20287 1011 25391 1012 5882 1015 1024 2339 1029 999 5882 1016 1024 2138 2017 2024 2026 2905 1998 5811 1998 1045 2031 2023 4121 2381 1529 5882 1015 1024 1045 2123 1521 1056 3305 1010 2079 2017 2215 2000 2175 2041 2007 5811 1029 5882 1016 1024 2053 1012 5882 1015 1024 2017 2123 1521 1056 2215 2032 1010 2021 2017 2123 1521 1056 2215 2033 2000 2031 2032 1029 5882 1016 1024 1061 1521 2113 3960 1999 2529 4219 1529 5882 1015 1024 1057 5603 999 1045 3685 2903 2017 2106 2023 2205 2033 999 2017 2018 2033 4797 2075 2129 6047 1045 2001 999 2017 2018 2033 4797 2075 2026 4827 3168 999 5882 1016 1024 2298 1010 2023 2003 2025 2008 2502 1997 1037 3066 999 2017 2074 2123 1521 1056 3058 5811 999 2045 1521 1055 1037 2454 2060 4364 2041 2045 1010 2017 2074 1529 5882 1015 1024 4931 999 2017 2031 2053 2157 2000 2425 2033 2054 2000 2079 1012 5882 1016 1024 1045 1521 1049 2025 4129 2017 2054 2000 2079 999 1045 2572 4129 2017 2054 2025 2000 2079 999 5882 1015 1024 2339 2024 2017 2061 9981 1997 2033 1029 5882 1016 1024 10454 2023 2003 2025 2055 2033 2108 9981 1997 2017 999 2023 2003 2055 2017 2108 1037 28557 999 5782 2054 2017 2064 1521 1056 2031 999 5882 1015 1024 2064 1521 1056 2031 1029 999 8016 2033 1010 1996 2069 2518 1045 2064 1521 1056 2031 2003 11825 999 5882 1016 1024 2035 2157 1010 2035 2157 1010 2092 102 5882 1016 102 5882 1015 102\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   *** Example ***\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   guid: train-4\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   tokens: [CLS] speaker 1 : hey ! speaker 2 : hey ! speaker 1 : you ’ ll never believe what just happened , ross just totally blew me off and he didn ’ t even tell me why ! speaker 2 : oh ##hh well . y ’ know what honey ? the best thing to do to get over a guy is to start dating someone else . oh ! there is this great guy you will love at work named bob ! he ’ s a real up - and - come ##r in human resources . speaker 1 : y ’ know , thanks for trying to cheer me up , but i ’ m not gonna date some random guy from your work . speaker 2 : it ’ s not random , it ’ s bob . speaker 1 : it ’ s probably because not mature enough . or smart enough . maybe he doesn ’ t like the way i dress — no that can ’ t be it . it ’ s really gotta be the smart thing . oh i ’ m so stupid ! i ’ m just like this incredibly pretty stupid girl ! speaker 2 : no honey , okay , okay , you wanna know why ross canceled the date ? because i asked him to . speaker 1 : you asked him too ? ! speaker 2 : hm - mmm . speaker 1 : why ? ! speaker 2 : because you are my sister and ross and i have this huge history … speaker 1 : i don ’ t understand , do you want to go out with ross ? speaker 2 : no . speaker 1 : you don ’ t want him , but you don ’ t want me to have him ? speaker 2 : y ’ know bob in human resources … speaker 1 : u ##gh ! i cannot believe you did this too me ! you had me doubt ##ing how smart i was ! you had me doubt ##ing my fashion sense ! speaker 2 : look , this is not that big of a deal ! you just don ’ t date ross ! there ’ s a million other guys out there , you just … speaker 1 : hey ! you have no right to tell me what to do . speaker 2 : i ’ m not telling you what to do ! i am telling you what not to do ! speaker 1 : why are you so jealous of me ? speaker 2 : jill this is not about me being jealous of you ! this is about you being a brat ! wanting what you can ’ t have ! speaker 1 : can ’ t have ? ! excuse me , the only thing i can ’ t have is dairy ! speaker 2 : all right , all right , well you [SEP] speaker 2 [SEP] ross [SEP]\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_ids: 101 5882 1015 1024 4931 999 5882 1016 1024 4931 999 5882 1015 1024 2017 1521 2222 2196 2903 2054 2074 3047 1010 5811 2074 6135 8682 2033 2125 1998 2002 2134 1521 1056 2130 2425 2033 2339 999 5882 1016 1024 2821 23644 2092 1012 1061 1521 2113 2054 6861 1029 1996 2190 2518 2000 2079 2000 2131 2058 1037 3124 2003 2000 2707 5306 2619 2842 1012 2821 999 2045 2003 2023 2307 3124 2017 2097 2293 2012 2147 2315 3960 999 2002 1521 1055 1037 2613 2039 1011 1998 1011 2272 2099 1999 2529 4219 1012 5882 1015 1024 1061 1521 2113 1010 4283 2005 2667 2000 15138 2033 2039 1010 2021 1045 1521 1049 2025 6069 3058 2070 6721 3124 2013 2115 2147 1012 5882 1016 1024 2009 1521 1055 2025 6721 1010 2009 1521 1055 3960 1012 5882 1015 1024 2009 1521 1055 2763 2138 2025 9677 2438 1012 2030 6047 2438 1012 2672 2002 2987 1521 1056 2066 1996 2126 1045 4377 1517 2053 2008 2064 1521 1056 2022 2009 1012 2009 1521 1055 2428 10657 2022 1996 6047 2518 1012 2821 1045 1521 1049 2061 5236 999 1045 1521 1049 2074 2066 2023 11757 3492 5236 2611 999 5882 1016 1024 2053 6861 1010 3100 1010 3100 1010 2017 10587 2113 2339 5811 13261 1996 3058 1029 2138 1045 2356 2032 2000 1012 5882 1015 1024 2017 2356 2032 2205 1029 999 5882 1016 1024 20287 1011 25391 1012 5882 1015 1024 2339 1029 999 5882 1016 1024 2138 2017 2024 2026 2905 1998 5811 1998 1045 2031 2023 4121 2381 1529 5882 1015 1024 1045 2123 1521 1056 3305 1010 2079 2017 2215 2000 2175 2041 2007 5811 1029 5882 1016 1024 2053 1012 5882 1015 1024 2017 2123 1521 1056 2215 2032 1010 2021 2017 2123 1521 1056 2215 2033 2000 2031 2032 1029 5882 1016 1024 1061 1521 2113 3960 1999 2529 4219 1529 5882 1015 1024 1057 5603 999 1045 3685 2903 2017 2106 2023 2205 2033 999 2017 2018 2033 4797 2075 2129 6047 1045 2001 999 2017 2018 2033 4797 2075 2026 4827 3168 999 5882 1016 1024 2298 1010 2023 2003 2025 2008 2502 1997 1037 3066 999 2017 2074 2123 1521 1056 3058 5811 999 2045 1521 1055 1037 2454 2060 4364 2041 2045 1010 2017 2074 1529 5882 1015 1024 4931 999 2017 2031 2053 2157 2000 2425 2033 2054 2000 2079 1012 5882 1016 1024 1045 1521 1049 2025 4129 2017 2054 2000 2079 999 1045 2572 4129 2017 2054 2025 2000 2079 999 5882 1015 1024 2339 2024 2017 2061 9981 1997 2033 1029 5882 1016 1024 10454 2023 2003 2025 2055 2033 2108 9981 1997 2017 999 2023 2003 2055 2017 2108 1037 28557 999 5782 2054 2017 2064 1521 1056 2031 999 5882 1015 1024 2064 1521 1056 2031 1029 999 8016 2033 1010 1996 2069 2518 1045 2064 1521 1056 2031 2003 11825 999 5882 1016 1024 2035 2157 1010 2035 2157 1010 2092 2017 102 5882 1016 102 5811 102\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "08/29/2025 12:44:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            "#features 5997\n",
            "08/29/2025 12:44:33 - INFO - __main__ -   ***** Running training *****\n",
            "08/29/2025 12:44:33 - INFO - __main__ -     Num examples = 5997\n",
            "08/29/2025 12:44:33 - INFO - __main__ -     Batch size = 12\n",
            "08/29/2025 12:44:33 - INFO - __main__ -     Num steps = 4997\n",
            "Epoch:   0% 0/20 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/500 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/500 [00:02<17:46,  2.14s/it]\u001b[A/content/dialogre/bert/optimization.py:131: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1691.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 2/500 [00:03<14:19,  1.73s/it]\u001b[A\n",
            "Iteration:   1% 3/500 [00:04<12:03,  1.45s/it]\u001b[A\n",
            "Iteration:   1% 4/500 [00:05<11:14,  1.36s/it]\u001b[A\n",
            "Iteration:   1% 5/500 [00:07<10:34,  1.28s/it]\u001b[A\n",
            "Iteration:   1% 6/500 [00:08<10:22,  1.26s/it]\u001b[A\n",
            "Iteration:   1% 7/500 [00:09<10:02,  1.22s/it]\u001b[A\n",
            "Iteration:   2% 8/500 [00:10<10:02,  1.22s/it]\u001b[A\n",
            "Iteration:   2% 9/500 [00:11<09:47,  1.20s/it]\u001b[A\n",
            "Iteration:   2% 10/500 [00:13<09:52,  1.21s/it]\u001b[A\n",
            "Iteration:   2% 11/500 [00:14<09:42,  1.19s/it]\u001b[A\n",
            "Iteration:   2% 12/500 [00:15<09:53,  1.22s/it]\u001b[A\n",
            "Iteration:   3% 13/500 [00:16<09:42,  1.20s/it]\u001b[A\n",
            "Iteration:   3% 14/500 [00:17<09:47,  1.21s/it]\u001b[A\n",
            "Iteration:   3% 15/500 [00:18<09:38,  1.19s/it]\u001b[A\n",
            "Iteration:   3% 16/500 [00:20<09:43,  1.21s/it]\u001b[A\n",
            "Iteration:   3% 17/500 [00:21<09:34,  1.19s/it]\u001b[A\n",
            "Iteration:   4% 18/500 [00:22<09:43,  1.21s/it]\u001b[A\n",
            "Iteration:   4% 19/500 [00:23<09:34,  1.20s/it]\u001b[A\n",
            "Iteration:   4% 20/500 [00:25<09:40,  1.21s/it]\u001b[A\n",
            "Iteration:   4% 21/500 [00:26<09:32,  1.19s/it]\u001b[A\n",
            "Iteration:   4% 22/500 [00:27<09:42,  1.22s/it]\u001b[A\n",
            "Iteration:   5% 23/500 [00:28<09:34,  1.20s/it]\u001b[A\n",
            "Iteration:   5% 24/500 [00:29<09:37,  1.21s/it]\u001b[A\n",
            "Iteration:   5% 25/500 [00:31<09:28,  1.20s/it]\u001b[A\n",
            "Iteration:   5% 26/500 [00:32<09:37,  1.22s/it]\u001b[A\n",
            "Iteration:   5% 27/500 [00:33<09:29,  1.20s/it]\u001b[A\n",
            "Iteration:   6% 28/500 [00:34<09:37,  1.22s/it]\u001b[A\n",
            "Iteration:   6% 29/500 [00:35<09:30,  1.21s/it]\u001b[A\n",
            "Iteration:   6% 30/500 [00:37<09:37,  1.23s/it]\u001b[A\n",
            "Iteration:   6% 31/500 [00:38<09:30,  1.22s/it]\u001b[A\n",
            "Iteration:   6% 32/500 [00:39<09:39,  1.24s/it]\u001b[A\n",
            "Iteration:   7% 33/500 [00:40<09:32,  1.23s/it]\u001b[A\n",
            "Iteration:   7% 34/500 [00:42<09:35,  1.23s/it]\u001b[A\n",
            "Iteration:   7% 35/500 [00:43<09:29,  1.22s/it]\u001b[A\n",
            "Iteration:   7% 36/500 [00:44<09:34,  1.24s/it]\u001b[A\n",
            "Iteration:   7% 37/500 [00:45<09:26,  1.22s/it]\u001b[A\n",
            "Iteration:   8% 38/500 [00:47<09:32,  1.24s/it]\u001b[A\n",
            "Iteration:   8% 39/500 [00:48<09:26,  1.23s/it]\u001b[A\n",
            "Iteration:   8% 40/500 [00:49<09:32,  1.24s/it]\u001b[A\n",
            "Iteration:   8% 41/500 [00:50<09:25,  1.23s/it]\u001b[A\n",
            "Iteration:   8% 42/500 [00:52<09:37,  1.26s/it]\u001b[A\n",
            "Iteration:   9% 43/500 [00:53<09:28,  1.24s/it]\u001b[A\n",
            "Iteration:   9% 44/500 [00:54<09:31,  1.25s/it]\u001b[A\n",
            "Iteration:   9% 45/500 [00:55<09:22,  1.24s/it]\u001b[A\n",
            "Iteration:   9% 46/500 [00:57<09:31,  1.26s/it]\u001b[A\n",
            "Iteration:   9% 47/500 [00:58<09:23,  1.24s/it]\u001b[A\n",
            "Iteration:  10% 48/500 [00:59<09:28,  1.26s/it]\u001b[A\n",
            "Iteration:  10% 49/500 [01:00<09:21,  1.25s/it]\u001b[A\n",
            "Iteration:  10% 50/500 [01:02<09:27,  1.26s/it]\u001b[A\n",
            "Iteration:  10% 51/500 [01:03<09:21,  1.25s/it]\u001b[A\n",
            "Iteration:  10% 52/500 [01:04<09:37,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 53/500 [01:05<09:27,  1.27s/it]\u001b[A\n",
            "Iteration:  11% 54/500 [01:07<09:30,  1.28s/it]\u001b[A\n",
            "Iteration:  11% 55/500 [01:08<09:22,  1.26s/it]\u001b[A\n",
            "Iteration:  11% 56/500 [01:09<09:26,  1.28s/it]\u001b[A\n",
            "Iteration:  11% 57/500 [01:10<09:20,  1.26s/it]\u001b[A\n",
            "Iteration:  12% 58/500 [01:12<09:25,  1.28s/it]\u001b[A\n",
            "Iteration:  12% 59/500 [01:13<09:17,  1.26s/it]\u001b[A\n",
            "Iteration:  12% 60/500 [01:14<09:23,  1.28s/it]\u001b[A\n",
            "Iteration:  12% 61/500 [01:16<09:18,  1.27s/it]\u001b[A\n",
            "Iteration:  12% 62/500 [01:17<09:53,  1.35s/it]\u001b[A\n",
            "Iteration:  13% 63/500 [01:18<09:37,  1.32s/it]\u001b[A\n",
            "Iteration:  13% 64/500 [01:20<10:08,  1.40s/it]\u001b[A\n",
            "Iteration:  13% 65/500 [01:21<09:47,  1.35s/it]\u001b[A\n",
            "Iteration:  13% 66/500 [01:23<10:16,  1.42s/it]\u001b[A\n",
            "Iteration:  13% 67/500 [01:24<09:52,  1.37s/it]\u001b[A\n",
            "Iteration:  14% 68/500 [01:25<09:46,  1.36s/it]\u001b[A\n",
            "Iteration:  14% 69/500 [01:27<09:33,  1.33s/it]\u001b[A\n",
            "Iteration:  14% 70/500 [01:28<09:37,  1.34s/it]\u001b[A\n",
            "Iteration:  14% 71/500 [01:29<09:25,  1.32s/it]\u001b[A\n",
            "Iteration:  14% 72/500 [01:31<09:26,  1.32s/it]\u001b[A\n",
            "Iteration:  15% 73/500 [01:32<09:18,  1.31s/it]\u001b[A\n",
            "Iteration:  15% 74/500 [01:33<09:23,  1.32s/it]\u001b[A\n",
            "Iteration:  15% 75/500 [01:34<09:14,  1.30s/it]\u001b[A\n",
            "Iteration:  15% 76/500 [01:36<09:19,  1.32s/it]\u001b[A\n",
            "Iteration:  15% 77/500 [01:37<09:13,  1.31s/it]\u001b[A\n",
            "Iteration:  16% 78/500 [01:38<09:17,  1.32s/it]\u001b[A\n",
            "Iteration:  16% 79/500 [01:40<09:09,  1.30s/it]\u001b[A\n",
            "Iteration:  16% 80/500 [01:41<09:19,  1.33s/it]\u001b[A\n",
            "Iteration:  16% 81/500 [01:42<09:08,  1.31s/it]\u001b[A\n",
            "Iteration:  16% 82/500 [01:44<09:11,  1.32s/it]\u001b[A\n",
            "Iteration:  17% 83/500 [01:45<09:04,  1.31s/it]\u001b[A\n",
            "Iteration:  17% 84/500 [01:46<09:08,  1.32s/it]\u001b[A\n",
            "Iteration:  17% 85/500 [01:48<08:59,  1.30s/it]\u001b[A\n",
            "Iteration:  17% 86/500 [01:49<09:04,  1.32s/it]\u001b[A\n",
            "Iteration:  17% 87/500 [01:50<08:55,  1.30s/it]\u001b[A\n",
            "Iteration:  18% 88/500 [01:52<09:01,  1.31s/it]\u001b[A\n",
            "Iteration:  18% 89/500 [01:53<08:52,  1.30s/it]\u001b[A\n",
            "Iteration:  18% 90/500 [01:54<08:55,  1.31s/it]\u001b[A\n",
            "Iteration:  18% 91/500 [01:55<08:46,  1.29s/it]\u001b[A\n",
            "Iteration:  18% 92/500 [01:57<08:50,  1.30s/it]\u001b[A\n",
            "Iteration:  19% 93/500 [01:58<08:41,  1.28s/it]\u001b[A\n",
            "Iteration:  19% 94/500 [01:59<08:44,  1.29s/it]\u001b[A\n",
            "Iteration:  19% 95/500 [02:01<08:37,  1.28s/it]\u001b[A\n",
            "Iteration:  19% 96/500 [02:02<08:42,  1.29s/it]\u001b[A\n",
            "Iteration:  19% 97/500 [02:03<08:32,  1.27s/it]\u001b[A\n",
            "Iteration:  20% 98/500 [02:04<08:40,  1.30s/it]\u001b[A\n",
            "Iteration:  20% 99/500 [02:06<08:31,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 100/500 [02:07<08:39,  1.30s/it]\u001b[A\n",
            "Iteration:  20% 101/500 [02:08<08:29,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 102/500 [02:10<08:33,  1.29s/it]\u001b[A\n",
            "Iteration:  21% 103/500 [02:11<08:25,  1.27s/it]\u001b[A\n",
            "Iteration:  21% 104/500 [02:12<08:28,  1.28s/it]\u001b[A\n",
            "Iteration:  21% 105/500 [02:13<08:20,  1.27s/it]\u001b[A\n",
            "Iteration:  21% 106/500 [02:15<08:24,  1.28s/it]\u001b[A\n",
            "Iteration:  21% 107/500 [02:16<08:15,  1.26s/it]\u001b[A\n",
            "Iteration:  22% 108/500 [02:17<08:24,  1.29s/it]\u001b[A\n",
            "Iteration:  22% 109/500 [02:18<08:14,  1.27s/it]\u001b[A\n",
            "Iteration:  22% 110/500 [02:20<08:18,  1.28s/it]\u001b[A\n",
            "Iteration:  22% 111/500 [02:21<08:12,  1.27s/it]\u001b[A\n",
            "Iteration:  22% 112/500 [02:22<08:19,  1.29s/it]\u001b[A\n",
            "Iteration:  23% 113/500 [02:24<08:11,  1.27s/it]\u001b[A\n",
            "Iteration:  23% 114/500 [02:25<08:15,  1.28s/it]\u001b[A\n",
            "Iteration:  23% 115/500 [02:26<08:08,  1.27s/it]\u001b[A\n",
            "Iteration:  23% 116/500 [02:27<08:16,  1.29s/it]\u001b[A\n",
            "Iteration:  23% 117/500 [02:29<08:07,  1.27s/it]\u001b[A\n",
            "Iteration:  24% 118/500 [02:30<08:10,  1.28s/it]\u001b[A\n",
            "Iteration:  24% 119/500 [02:31<08:03,  1.27s/it]\u001b[A\n",
            "Iteration:  24% 120/500 [02:33<08:07,  1.28s/it]\u001b[A\n",
            "Iteration:  24% 121/500 [02:34<08:01,  1.27s/it]\u001b[A\n",
            "Iteration:  24% 122/500 [02:35<08:04,  1.28s/it]\u001b[A\n",
            "Iteration:  25% 123/500 [02:36<07:58,  1.27s/it]\u001b[A\n",
            "Iteration:  25% 124/500 [02:38<08:03,  1.29s/it]\u001b[A\n",
            "Iteration:  25% 125/500 [02:39<07:55,  1.27s/it]\u001b[A\n",
            "Iteration:  25% 126/500 [02:40<08:03,  1.29s/it]\u001b[A\n",
            "Iteration:  25% 127/500 [02:41<07:55,  1.27s/it]\u001b[A\n",
            "Iteration:  26% 128/500 [02:43<08:00,  1.29s/it]\u001b[A\n",
            "Iteration:  26% 129/500 [02:44<07:52,  1.27s/it]\u001b[A\n",
            "Iteration:  26% 130/500 [02:45<07:58,  1.29s/it]\u001b[A\n",
            "Iteration:  26% 131/500 [02:47<07:51,  1.28s/it]\u001b[A\n",
            "Iteration:  26% 132/500 [02:48<07:57,  1.30s/it]\u001b[A\n",
            "Iteration:  27% 133/500 [02:49<07:52,  1.29s/it]\u001b[A\n",
            "Iteration:  27% 134/500 [02:51<07:56,  1.30s/it]\u001b[A\n",
            "Iteration:  27% 135/500 [02:52<07:50,  1.29s/it]\u001b[A\n",
            "Iteration:  27% 136/500 [02:53<07:54,  1.30s/it]\u001b[A\n",
            "Iteration:  27% 137/500 [02:54<07:47,  1.29s/it]\u001b[A\n",
            "Iteration:  28% 138/500 [02:56<07:53,  1.31s/it]\u001b[A\n",
            "Iteration:  28% 139/500 [02:57<07:44,  1.29s/it]\u001b[A\n",
            "Iteration:  28% 140/500 [02:58<07:48,  1.30s/it]\u001b[A\n",
            "Iteration:  28% 141/500 [03:00<07:41,  1.28s/it]\u001b[A\n",
            "Iteration:  28% 142/500 [03:01<07:45,  1.30s/it]\u001b[A\n",
            "Iteration:  29% 143/500 [03:02<07:39,  1.29s/it]\u001b[A\n",
            "Iteration:  29% 144/500 [03:03<07:41,  1.30s/it]\u001b[A\n",
            "Iteration:  29% 145/500 [03:05<07:35,  1.28s/it]\u001b[A\n",
            "Iteration:  29% 146/500 [03:06<07:43,  1.31s/it]\u001b[A\n",
            "Iteration:  29% 147/500 [03:07<07:33,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 148/500 [03:09<07:35,  1.29s/it]\u001b[A\n",
            "Iteration:  30% 149/500 [03:10<07:29,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 150/500 [03:11<07:33,  1.30s/it]\u001b[A\n",
            "Iteration:  30% 151/500 [03:12<07:26,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 152/500 [03:14<07:31,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 153/500 [03:15<07:23,  1.28s/it]\u001b[A\n",
            "Iteration:  31% 154/500 [03:16<07:30,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 155/500 [03:18<07:24,  1.29s/it]\u001b[A\n",
            "Iteration:  31% 156/500 [03:19<07:26,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 157/500 [03:20<07:21,  1.29s/it]\u001b[A\n",
            "Iteration:  32% 158/500 [03:22<07:25,  1.30s/it]\u001b[A\n",
            "Iteration:  32% 159/500 [03:23<07:18,  1.29s/it]\u001b[A\n",
            "Iteration:  32% 160/500 [03:24<07:20,  1.30s/it]\u001b[A\n",
            "Iteration:  32% 161/500 [03:25<07:14,  1.28s/it]\u001b[A\n",
            "Iteration:  32% 162/500 [03:27<07:19,  1.30s/it]\u001b[A\n",
            "Iteration:  33% 163/500 [03:28<07:12,  1.28s/it]\u001b[A\n",
            "Iteration:  33% 164/500 [03:29<07:16,  1.30s/it]\u001b[A\n",
            "Iteration:  33% 165/500 [03:31<07:09,  1.28s/it]\u001b[A\n",
            "Iteration:  33% 166/500 [03:32<07:13,  1.30s/it]\u001b[A\n",
            "Iteration:  33% 167/500 [03:33<07:05,  1.28s/it]\u001b[A\n",
            "Iteration:  34% 168/500 [03:34<07:08,  1.29s/it]\u001b[A\n",
            "Iteration:  34% 169/500 [03:36<07:01,  1.27s/it]\u001b[A\n",
            "Iteration:  34% 170/500 [03:37<07:06,  1.29s/it]\u001b[A\n",
            "Iteration:  34% 171/500 [03:38<07:00,  1.28s/it]\u001b[A\n",
            "Iteration:  34% 172/500 [03:40<07:03,  1.29s/it]\u001b[A\n",
            "Iteration:  35% 173/500 [03:41<06:56,  1.27s/it]\u001b[A\n",
            "Iteration:  35% 174/500 [03:42<07:03,  1.30s/it]\u001b[A\n",
            "Iteration:  35% 175/500 [03:43<06:56,  1.28s/it]\u001b[A\n",
            "Iteration:  35% 176/500 [03:45<06:59,  1.30s/it]\u001b[A\n",
            "Iteration:  35% 177/500 [03:46<06:53,  1.28s/it]\u001b[A\n",
            "Iteration:  36% 178/500 [03:47<06:56,  1.29s/it]\u001b[A\n",
            "Iteration:  36% 179/500 [03:49<06:50,  1.28s/it]\u001b[A\n",
            "Iteration:  36% 180/500 [03:50<06:52,  1.29s/it]\u001b[A\n",
            "Iteration:  36% 181/500 [03:51<06:47,  1.28s/it]\u001b[A\n",
            "Iteration:  36% 182/500 [03:52<06:49,  1.29s/it]\u001b[A\n",
            "Iteration:  37% 183/500 [03:54<06:43,  1.27s/it]\u001b[A\n",
            "Iteration:  37% 184/500 [03:55<06:51,  1.30s/it]\u001b[A\n",
            "Iteration:  37% 185/500 [03:56<06:43,  1.28s/it]\u001b[A\n",
            "Iteration:  37% 186/500 [03:58<06:44,  1.29s/it]\u001b[A\n",
            "Iteration:  37% 187/500 [03:59<06:39,  1.28s/it]\u001b[A\n",
            "Iteration:  38% 188/500 [04:00<06:42,  1.29s/it]\u001b[A\n",
            "Iteration:  38% 189/500 [04:01<06:36,  1.27s/it]\u001b[A\n",
            "Iteration:  38% 190/500 [04:03<06:39,  1.29s/it]\u001b[A\n",
            "Iteration:  38% 191/500 [04:04<06:34,  1.28s/it]\u001b[A\n",
            "Iteration:  38% 192/500 [04:05<06:39,  1.30s/it]\u001b[A\n",
            "Iteration:  39% 193/500 [04:07<06:33,  1.28s/it]\u001b[A\n",
            "Iteration:  39% 194/500 [04:08<06:35,  1.29s/it]\u001b[A\n",
            "Iteration:  39% 195/500 [04:09<06:28,  1.27s/it]\u001b[A\n",
            "Iteration:  39% 196/500 [04:10<06:31,  1.29s/it]\u001b[A\n",
            "Iteration:  39% 197/500 [04:12<06:26,  1.28s/it]\u001b[A\n",
            "Iteration:  40% 198/500 [04:13<06:30,  1.29s/it]\u001b[A\n",
            "Iteration:  40% 199/500 [04:14<06:24,  1.28s/it]\u001b[A\n",
            "Iteration:  40% 200/500 [04:16<06:27,  1.29s/it]\u001b[A\n",
            "Iteration:  40% 201/500 [04:17<06:22,  1.28s/it]\u001b[A\n",
            "Iteration:  40% 202/500 [04:18<06:27,  1.30s/it]\u001b[A\n",
            "Iteration:  41% 203/500 [04:19<06:20,  1.28s/it]\u001b[A\n",
            "Iteration:  41% 204/500 [04:21<06:23,  1.29s/it]\u001b[A\n",
            "Iteration:  41% 205/500 [04:22<06:17,  1.28s/it]\u001b[A\n",
            "Iteration:  41% 206/500 [04:23<06:20,  1.30s/it]\u001b[A\n",
            "Iteration:  41% 207/500 [04:25<06:14,  1.28s/it]\u001b[A\n",
            "Iteration:  42% 208/500 [04:26<06:18,  1.30s/it]\u001b[A\n",
            "Iteration:  42% 209/500 [04:27<06:11,  1.28s/it]\u001b[A\n",
            "Iteration:  42% 210/500 [04:28<06:14,  1.29s/it]\u001b[A\n",
            "Iteration:  42% 211/500 [04:30<06:09,  1.28s/it]\u001b[A\n",
            "Iteration:  42% 212/500 [04:31<06:33,  1.37s/it]\u001b[A\n",
            "Iteration:  43% 213/500 [04:32<06:19,  1.32s/it]\u001b[A\n",
            "Iteration:  43% 214/500 [04:34<06:33,  1.38s/it]\u001b[A\n",
            "Iteration:  43% 215/500 [04:35<06:19,  1.33s/it]\u001b[A\n",
            "Iteration:  43% 216/500 [04:37<06:22,  1.35s/it]\u001b[A\n",
            "Iteration:  43% 217/500 [04:38<06:12,  1.32s/it]\u001b[A\n",
            "Iteration:  44% 218/500 [04:39<06:15,  1.33s/it]\u001b[A\n",
            "Iteration:  44% 219/500 [04:40<06:06,  1.30s/it]\u001b[A\n",
            "Iteration:  44% 220/500 [04:42<06:12,  1.33s/it]\u001b[A\n",
            "Iteration:  44% 221/500 [04:43<06:03,  1.30s/it]\u001b[A\n",
            "Iteration:  44% 222/500 [04:45<06:16,  1.36s/it]\u001b[A\n",
            "Iteration:  45% 223/500 [04:46<06:05,  1.32s/it]\u001b[A\n",
            "Iteration:  45% 224/500 [04:47<06:06,  1.33s/it]\u001b[A\n",
            "Iteration:  45% 225/500 [04:48<05:57,  1.30s/it]\u001b[A\n",
            "Iteration:  45% 226/500 [04:50<06:01,  1.32s/it]\u001b[A\n",
            "Iteration:  45% 227/500 [04:51<05:53,  1.29s/it]\u001b[A\n",
            "Iteration:  46% 228/500 [04:53<06:16,  1.39s/it]\u001b[A\n",
            "Iteration:  46% 229/500 [04:54<06:03,  1.34s/it]\u001b[A\n",
            "Iteration:  46% 230/500 [04:55<06:05,  1.35s/it]\u001b[A\n",
            "Iteration:  46% 231/500 [04:56<05:55,  1.32s/it]\u001b[A\n",
            "Iteration:  46% 232/500 [04:58<06:03,  1.36s/it]\u001b[A\n",
            "Iteration:  47% 233/500 [04:59<05:53,  1.32s/it]\u001b[A\n",
            "Iteration:  47% 234/500 [05:00<05:57,  1.34s/it]\u001b[A\n",
            "Iteration:  47% 235/500 [05:02<05:47,  1.31s/it]\u001b[A\n",
            "Iteration:  47% 236/500 [05:03<05:55,  1.35s/it]\u001b[A\n",
            "Iteration:  47% 237/500 [05:04<05:45,  1.31s/it]\u001b[A\n",
            "Iteration:  48% 238/500 [05:06<06:04,  1.39s/it]\u001b[A\n",
            "Iteration:  48% 239/500 [05:07<05:50,  1.34s/it]\u001b[A\n",
            "Iteration:  48% 240/500 [05:09<05:49,  1.34s/it]\u001b[A\n",
            "Iteration:  48% 241/500 [05:10<05:39,  1.31s/it]\u001b[A\n",
            "Iteration:  48% 242/500 [05:11<05:39,  1.32s/it]\u001b[A\n",
            "Iteration:  49% 243/500 [05:12<05:32,  1.29s/it]\u001b[A\n",
            "Iteration:  49% 244/500 [05:14<05:34,  1.30s/it]\u001b[A\n",
            "Iteration:  49% 245/500 [05:15<05:26,  1.28s/it]\u001b[A\n",
            "Iteration:  49% 246/500 [05:16<05:28,  1.29s/it]\u001b[A\n",
            "Iteration:  49% 247/500 [05:17<05:23,  1.28s/it]\u001b[A\n",
            "Iteration:  50% 248/500 [05:19<05:25,  1.29s/it]\u001b[A\n",
            "Iteration:  50% 249/500 [05:20<05:20,  1.28s/it]\u001b[A\n",
            "Iteration:  50% 250/500 [05:21<05:23,  1.30s/it]\u001b[A\n",
            "Iteration:  50% 251/500 [05:23<05:18,  1.28s/it]\u001b[A\n",
            "Iteration:  50% 252/500 [05:24<05:21,  1.30s/it]\u001b[A\n",
            "Iteration:  51% 253/500 [05:25<05:16,  1.28s/it]\u001b[A\n",
            "Iteration:  51% 254/500 [05:27<05:18,  1.30s/it]\u001b[A\n",
            "Iteration:  51% 255/500 [05:28<05:13,  1.28s/it]\u001b[A\n",
            "Iteration:  51% 256/500 [05:29<05:15,  1.29s/it]\u001b[A\n",
            "Iteration:  51% 257/500 [05:30<05:09,  1.27s/it]\u001b[A\n",
            "Iteration:  52% 258/500 [05:32<05:11,  1.29s/it]\u001b[A\n",
            "Iteration:  52% 259/500 [05:33<05:07,  1.27s/it]\u001b[A\n",
            "Iteration:  52% 260/500 [05:34<05:13,  1.31s/it]\u001b[A\n",
            "Iteration:  52% 261/500 [05:35<05:06,  1.28s/it]\u001b[A\n",
            "Iteration:  52% 262/500 [05:37<05:08,  1.30s/it]\u001b[A\n",
            "Iteration:  53% 263/500 [05:38<05:03,  1.28s/it]\u001b[A\n",
            "Iteration:  53% 264/500 [05:39<05:05,  1.30s/it]\u001b[A\n",
            "Iteration:  53% 265/500 [05:41<05:00,  1.28s/it]\u001b[A\n",
            "Iteration:  53% 266/500 [05:42<05:01,  1.29s/it]\u001b[A\n",
            "Iteration:  53% 267/500 [05:43<04:57,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 268/500 [05:45<04:59,  1.29s/it]\u001b[A\n",
            "Iteration:  54% 269/500 [05:46<04:55,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 270/500 [05:47<04:58,  1.30s/it]\u001b[A\n",
            "Iteration:  54% 271/500 [05:48<04:53,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 272/500 [05:50<04:55,  1.29s/it]\u001b[A\n",
            "Iteration:  55% 273/500 [05:51<04:50,  1.28s/it]\u001b[A\n",
            "Iteration:  55% 274/500 [05:52<04:52,  1.30s/it]\u001b[A\n",
            "Iteration:  55% 275/500 [05:53<04:47,  1.28s/it]\u001b[A\n",
            "Iteration:  55% 276/500 [05:55<04:50,  1.30s/it]\u001b[A\n",
            "Iteration:  55% 277/500 [05:56<04:45,  1.28s/it]\u001b[A\n",
            "Iteration:  56% 278/500 [05:57<04:49,  1.30s/it]\u001b[A\n",
            "Iteration:  56% 279/500 [05:59<04:44,  1.29s/it]\u001b[A\n",
            "Iteration:  56% 280/500 [06:00<04:46,  1.30s/it]\u001b[A\n",
            "Iteration:  56% 281/500 [06:01<04:40,  1.28s/it]\u001b[A\n",
            "Iteration:  56% 282/500 [06:03<04:42,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 283/500 [06:04<04:38,  1.28s/it]\u001b[A\n",
            "Iteration:  57% 284/500 [06:05<04:41,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 285/500 [06:06<04:35,  1.28s/it]\u001b[A\n",
            "Iteration:  57% 286/500 [06:08<04:36,  1.29s/it]\u001b[A\n",
            "Iteration:  57% 287/500 [06:09<04:32,  1.28s/it]\u001b[A\n",
            "Iteration:  58% 288/500 [06:10<04:37,  1.31s/it]\u001b[A\n",
            "Iteration:  58% 289/500 [06:12<04:32,  1.29s/it]\u001b[A\n",
            "Iteration:  58% 290/500 [06:13<04:32,  1.30s/it]\u001b[A\n",
            "Iteration:  58% 291/500 [06:14<04:28,  1.28s/it]\u001b[A\n",
            "Iteration:  58% 292/500 [06:16<04:31,  1.31s/it]\u001b[A\n",
            "Iteration:  59% 293/500 [06:17<04:25,  1.28s/it]\u001b[A\n",
            "Iteration:  59% 294/500 [06:18<04:26,  1.29s/it]\u001b[A\n",
            "Iteration:  59% 295/500 [06:19<04:22,  1.28s/it]\u001b[A\n",
            "Iteration:  59% 296/500 [06:21<04:24,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 297/500 [06:22<04:19,  1.28s/it]\u001b[A\n",
            "Iteration:  60% 298/500 [06:23<04:23,  1.31s/it]\u001b[A\n",
            "Iteration:  60% 299/500 [06:25<04:17,  1.28s/it]\u001b[A\n",
            "Iteration:  60% 300/500 [06:26<04:19,  1.30s/it]\u001b[A\n",
            "Iteration:  60% 301/500 [06:27<04:14,  1.28s/it]\u001b[A\n",
            "Iteration:  60% 302/500 [06:28<04:15,  1.29s/it]\u001b[A\n",
            "Iteration:  61% 303/500 [06:30<04:12,  1.28s/it]\u001b[A\n",
            "Iteration:  61% 304/500 [06:31<04:12,  1.29s/it]\u001b[A\n",
            "Iteration:  61% 305/500 [06:32<04:08,  1.27s/it]\u001b[A\n",
            "Iteration:  61% 306/500 [06:34<04:10,  1.29s/it]\u001b[A\n",
            "Iteration:  61% 307/500 [06:35<04:06,  1.28s/it]\u001b[A\n",
            "Iteration:  62% 308/500 [06:36<04:07,  1.29s/it]\u001b[A\n",
            "Iteration:  62% 309/500 [06:37<04:04,  1.28s/it]\u001b[A\n",
            "Iteration:  62% 310/500 [06:39<04:05,  1.29s/it]\u001b[A\n",
            "Iteration:  62% 311/500 [06:40<04:01,  1.28s/it]\u001b[A\n",
            "Iteration:  62% 312/500 [06:41<04:03,  1.30s/it]\u001b[A\n",
            "Iteration:  63% 313/500 [06:43<03:59,  1.28s/it]\u001b[A\n",
            "Iteration:  63% 314/500 [06:44<04:00,  1.30s/it]\u001b[A\n",
            "Iteration:  63% 315/500 [06:45<03:56,  1.28s/it]\u001b[A\n",
            "Iteration:  63% 316/500 [06:46<04:00,  1.31s/it]\u001b[A\n",
            "Iteration:  63% 317/500 [06:48<03:55,  1.29s/it]\u001b[A\n",
            "Iteration:  64% 318/500 [06:49<03:56,  1.30s/it]\u001b[A\n",
            "Iteration:  64% 319/500 [06:50<03:51,  1.28s/it]\u001b[A\n",
            "Iteration:  64% 320/500 [06:52<03:53,  1.30s/it]\u001b[A\n",
            "Iteration:  64% 321/500 [06:53<03:48,  1.28s/it]\u001b[A\n",
            "Iteration:  64% 322/500 [06:54<04:00,  1.35s/it]\u001b[A\n",
            "Iteration:  65% 323/500 [06:56<03:51,  1.31s/it]\u001b[A\n",
            "Iteration:  65% 324/500 [06:57<03:50,  1.31s/it]\u001b[A\n",
            "Iteration:  65% 325/500 [06:58<03:45,  1.29s/it]\u001b[A\n",
            "Iteration:  65% 326/500 [06:59<03:49,  1.32s/it]\u001b[A\n",
            "Iteration:  65% 327/500 [07:01<03:43,  1.29s/it]\u001b[A\n",
            "Iteration:  66% 328/500 [07:02<03:43,  1.30s/it]\u001b[A\n",
            "Iteration:  66% 329/500 [07:03<03:39,  1.28s/it]\u001b[A\n",
            "Iteration:  66% 330/500 [07:05<03:40,  1.30s/it]\u001b[A\n",
            "Iteration:  66% 331/500 [07:06<03:35,  1.28s/it]\u001b[A\n",
            "Iteration:  66% 332/500 [07:07<03:36,  1.29s/it]\u001b[A\n",
            "Iteration:  67% 333/500 [07:08<03:32,  1.27s/it]\u001b[A\n",
            "Iteration:  67% 334/500 [07:10<03:34,  1.29s/it]\u001b[A\n",
            "Iteration:  67% 335/500 [07:11<03:30,  1.27s/it]\u001b[A\n",
            "Iteration:  67% 336/500 [07:12<03:32,  1.29s/it]\u001b[A\n",
            "Iteration:  67% 337/500 [07:14<03:28,  1.28s/it]\u001b[A\n",
            "Iteration:  68% 338/500 [07:15<03:29,  1.29s/it]\u001b[A\n",
            "Iteration:  68% 339/500 [07:16<03:26,  1.28s/it]\u001b[A\n",
            "Iteration:  68% 340/500 [07:17<03:27,  1.29s/it]\u001b[A\n",
            "Iteration:  68% 341/500 [07:19<03:22,  1.28s/it]\u001b[A\n",
            "Iteration:  68% 342/500 [07:20<03:24,  1.29s/it]\u001b[A\n",
            "Iteration:  69% 343/500 [07:21<03:20,  1.28s/it]\u001b[A\n",
            "Iteration:  69% 344/500 [07:23<03:22,  1.30s/it]\u001b[A\n",
            "Iteration:  69% 345/500 [07:24<03:18,  1.28s/it]\u001b[A\n",
            "Iteration:  69% 346/500 [07:25<03:18,  1.29s/it]\u001b[A\n",
            "Iteration:  69% 347/500 [07:26<03:15,  1.28s/it]\u001b[A\n",
            "Iteration:  70% 348/500 [07:28<03:16,  1.29s/it]\u001b[A\n",
            "Iteration:  70% 349/500 [07:29<03:12,  1.28s/it]\u001b[A\n",
            "Iteration:  70% 350/500 [07:30<03:21,  1.34s/it]\u001b[A\n",
            "Iteration:  70% 351/500 [07:32<03:15,  1.31s/it]\u001b[A\n",
            "Iteration:  70% 352/500 [07:33<03:23,  1.38s/it]\u001b[A\n",
            "Iteration:  71% 353/500 [07:34<03:16,  1.34s/it]\u001b[A\n",
            "Iteration:  71% 354/500 [07:36<03:15,  1.34s/it]\u001b[A\n",
            "Iteration:  71% 355/500 [07:37<03:10,  1.31s/it]\u001b[A\n",
            "Iteration:  71% 356/500 [07:38<03:09,  1.32s/it]\u001b[A\n",
            "Iteration:  71% 357/500 [07:40<03:05,  1.30s/it]\u001b[A\n",
            "Iteration:  72% 358/500 [07:41<03:05,  1.31s/it]\u001b[A\n",
            "Iteration:  72% 359/500 [07:42<03:01,  1.29s/it]\u001b[A\n",
            "Iteration:  72% 360/500 [07:44<03:02,  1.30s/it]\u001b[A\n",
            "Iteration:  72% 361/500 [07:45<02:58,  1.29s/it]\u001b[A\n",
            "Iteration:  72% 362/500 [07:46<02:59,  1.30s/it]\u001b[A\n",
            "Iteration:  73% 363/500 [07:47<02:55,  1.28s/it]\u001b[A\n",
            "Iteration:  73% 364/500 [07:49<02:57,  1.31s/it]\u001b[A\n",
            "Iteration:  73% 365/500 [07:50<02:54,  1.29s/it]\u001b[A\n",
            "Iteration:  73% 366/500 [07:51<02:54,  1.30s/it]\u001b[A\n",
            "Iteration:  73% 367/500 [07:53<02:50,  1.29s/it]\u001b[A\n",
            "Iteration:  74% 368/500 [07:54<02:50,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 369/500 [07:55<02:47,  1.28s/it]\u001b[A\n",
            "Iteration:  74% 370/500 [07:56<02:48,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 371/500 [07:58<02:45,  1.28s/it]\u001b[A\n",
            "Iteration:  74% 372/500 [07:59<02:47,  1.31s/it]\u001b[A\n",
            "Iteration:  75% 373/500 [08:00<02:43,  1.29s/it]\u001b[A\n",
            "Iteration:  75% 374/500 [08:02<02:43,  1.30s/it]\u001b[A\n",
            "Iteration:  75% 375/500 [08:03<02:40,  1.29s/it]\u001b[A\n",
            "Iteration:  75% 376/500 [08:04<02:40,  1.30s/it]\u001b[A\n",
            "Iteration:  75% 377/500 [08:05<02:38,  1.29s/it]\u001b[A\n",
            "Iteration:  76% 378/500 [08:07<02:38,  1.30s/it]\u001b[A\n",
            "Iteration:  76% 379/500 [08:08<02:35,  1.28s/it]\u001b[A\n",
            "Iteration:  76% 380/500 [08:09<02:35,  1.29s/it]\u001b[A\n",
            "Iteration:  76% 381/500 [08:11<02:32,  1.28s/it]\u001b[A\n",
            "Iteration:  76% 382/500 [08:12<02:34,  1.31s/it]\u001b[A\n",
            "Iteration:  77% 383/500 [08:13<02:31,  1.29s/it]\u001b[A\n",
            "Iteration:  77% 384/500 [08:15<02:30,  1.30s/it]\u001b[A\n",
            "Iteration:  77% 385/500 [08:16<02:28,  1.29s/it]\u001b[A\n",
            "Iteration:  77% 386/500 [08:17<02:29,  1.31s/it]\u001b[A\n",
            "Iteration:  77% 387/500 [08:18<02:25,  1.29s/it]\u001b[A\n",
            "Iteration:  78% 388/500 [08:20<02:26,  1.31s/it]\u001b[A\n",
            "Iteration:  78% 389/500 [08:21<02:22,  1.29s/it]\u001b[A\n",
            "Iteration:  78% 390/500 [08:22<02:23,  1.31s/it]\u001b[A\n",
            "Iteration:  78% 391/500 [08:24<02:20,  1.29s/it]\u001b[A\n",
            "Iteration:  78% 392/500 [08:25<02:21,  1.31s/it]\u001b[A\n",
            "Iteration:  79% 393/500 [08:26<02:18,  1.29s/it]\u001b[A\n",
            "Iteration:  79% 394/500 [08:28<02:18,  1.31s/it]\u001b[A\n",
            "Iteration:  79% 395/500 [08:29<02:15,  1.29s/it]\u001b[A\n",
            "Iteration:  79% 396/500 [08:30<02:15,  1.30s/it]\u001b[A\n",
            "Iteration:  79% 397/500 [08:31<02:12,  1.28s/it]\u001b[A\n",
            "Iteration:  80% 398/500 [08:33<02:12,  1.30s/it]\u001b[A\n",
            "Iteration:  80% 399/500 [08:34<02:09,  1.29s/it]\u001b[A\n",
            "Iteration:  80% 400/500 [08:35<02:10,  1.31s/it]\u001b[A\n",
            "Iteration:  80% 401/500 [08:37<02:07,  1.29s/it]\u001b[A\n",
            "Iteration:  80% 402/500 [08:38<02:07,  1.30s/it]\u001b[A\n",
            "Iteration:  81% 403/500 [08:39<02:04,  1.28s/it]\u001b[A\n",
            "Iteration:  81% 404/500 [08:40<02:04,  1.30s/it]\u001b[A\n",
            "Iteration:  81% 405/500 [08:42<02:02,  1.29s/it]\u001b[A\n",
            "Iteration:  81% 406/500 [08:43<02:01,  1.30s/it]\u001b[A\n",
            "Iteration:  81% 407/500 [08:44<01:59,  1.28s/it]\u001b[A\n",
            "Iteration:  82% 408/500 [08:46<01:59,  1.29s/it]\u001b[A\n",
            "Iteration:  82% 409/500 [08:47<01:56,  1.28s/it]\u001b[A\n",
            "Iteration:  82% 410/500 [08:48<01:57,  1.30s/it]\u001b[A\n",
            "Iteration:  82% 411/500 [08:49<01:54,  1.28s/it]\u001b[A\n",
            "Iteration:  82% 412/500 [08:51<01:53,  1.29s/it]\u001b[A\n",
            "Iteration:  83% 413/500 [08:52<01:51,  1.28s/it]\u001b[A\n",
            "Iteration:  83% 414/500 [08:53<01:51,  1.30s/it]\u001b[A\n",
            "Iteration:  83% 415/500 [08:55<01:48,  1.28s/it]\u001b[A\n",
            "Iteration:  83% 416/500 [08:56<01:48,  1.29s/it]\u001b[A\n",
            "Iteration:  83% 417/500 [08:57<01:46,  1.28s/it]\u001b[A\n",
            "Iteration:  84% 418/500 [08:59<01:46,  1.29s/it]\u001b[A\n",
            "Iteration:  84% 419/500 [09:00<01:43,  1.28s/it]\u001b[A\n",
            "Iteration:  84% 420/500 [09:01<01:44,  1.30s/it]\u001b[A\n",
            "Iteration:  84% 421/500 [09:02<01:41,  1.28s/it]\u001b[A\n",
            "Iteration:  84% 422/500 [09:04<01:40,  1.29s/it]\u001b[A\n",
            "Iteration:  85% 423/500 [09:05<01:38,  1.28s/it]\u001b[A\n",
            "Iteration:  85% 424/500 [09:06<01:38,  1.30s/it]\u001b[A\n",
            "Iteration:  85% 425/500 [09:07<01:36,  1.28s/it]\u001b[A\n",
            "Iteration:  85% 426/500 [09:09<01:35,  1.29s/it]\u001b[A\n",
            "Iteration:  85% 427/500 [09:10<01:33,  1.28s/it]\u001b[A\n",
            "Iteration:  86% 428/500 [09:11<01:33,  1.29s/it]\u001b[A\n",
            "Iteration:  86% 429/500 [09:13<01:30,  1.28s/it]\u001b[A\n",
            "Iteration:  86% 430/500 [09:14<01:30,  1.29s/it]\u001b[A\n",
            "Iteration:  86% 431/500 [09:15<01:28,  1.28s/it]\u001b[A\n",
            "Iteration:  86% 432/500 [09:17<01:28,  1.29s/it]\u001b[A\n",
            "Iteration:  87% 433/500 [09:18<01:26,  1.28s/it]\u001b[A\n",
            "Iteration:  87% 434/500 [09:19<01:25,  1.29s/it]\u001b[A\n",
            "Iteration:  87% 435/500 [09:20<01:23,  1.28s/it]\u001b[A\n",
            "Iteration:  87% 436/500 [09:22<01:22,  1.30s/it]\u001b[A\n",
            "Iteration:  87% 437/500 [09:23<01:20,  1.28s/it]\u001b[A\n",
            "Iteration:  88% 438/500 [09:24<01:21,  1.31s/it]\u001b[A\n",
            "Iteration:  88% 439/500 [09:26<01:18,  1.28s/it]\u001b[A\n",
            "Iteration:  88% 440/500 [09:27<01:18,  1.30s/it]\u001b[A\n",
            "Iteration:  88% 441/500 [09:28<01:15,  1.28s/it]\u001b[A\n",
            "Iteration:  88% 442/500 [09:29<01:14,  1.29s/it]\u001b[A\n",
            "Iteration:  89% 443/500 [09:31<01:12,  1.28s/it]\u001b[A\n",
            "Iteration:  89% 444/500 [09:32<01:12,  1.30s/it]\u001b[A\n",
            "Iteration:  89% 445/500 [09:33<01:10,  1.28s/it]\u001b[A\n",
            "Iteration:  89% 446/500 [09:35<01:09,  1.29s/it]\u001b[A\n",
            "Iteration:  89% 447/500 [09:36<01:07,  1.28s/it]\u001b[A\n",
            "Iteration:  90% 448/500 [09:37<01:07,  1.30s/it]\u001b[A\n",
            "Iteration:  90% 449/500 [09:38<01:05,  1.28s/it]\u001b[A\n",
            "Iteration:  90% 450/500 [09:40<01:04,  1.29s/it]\u001b[A\n",
            "Iteration:  90% 451/500 [09:41<01:02,  1.28s/it]\u001b[A\n",
            "Iteration:  90% 452/500 [09:42<01:02,  1.30s/it]\u001b[A\n",
            "Iteration:  91% 453/500 [09:44<01:00,  1.28s/it]\u001b[A\n",
            "Iteration:  91% 454/500 [09:45<00:59,  1.29s/it]\u001b[A\n",
            "Iteration:  91% 455/500 [09:46<00:57,  1.28s/it]\u001b[A\n",
            "Iteration:  91% 456/500 [09:47<00:56,  1.29s/it]\u001b[A\n",
            "Iteration:  91% 457/500 [09:49<00:54,  1.28s/it]\u001b[A\n",
            "Iteration:  92% 458/500 [09:50<00:54,  1.31s/it]\u001b[A\n",
            "Iteration:  92% 459/500 [09:51<00:52,  1.29s/it]\u001b[A\n",
            "Iteration:  92% 460/500 [09:53<00:51,  1.29s/it]\u001b[A\n",
            "Iteration:  92% 461/500 [09:54<00:50,  1.28s/it]\u001b[A\n",
            "Iteration:  92% 462/500 [09:55<00:49,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 463/500 [09:56<00:47,  1.28s/it]\u001b[A\n",
            "Iteration:  93% 464/500 [09:58<00:46,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 465/500 [09:59<00:44,  1.28s/it]\u001b[A\n",
            "Iteration:  93% 466/500 [10:00<00:43,  1.29s/it]\u001b[A\n",
            "Iteration:  93% 467/500 [10:02<00:42,  1.28s/it]\u001b[A\n",
            "Iteration:  94% 468/500 [10:03<00:41,  1.29s/it]\u001b[A\n",
            "Iteration:  94% 469/500 [10:04<00:39,  1.28s/it]\u001b[A\n",
            "Iteration:  94% 470/500 [10:05<00:38,  1.29s/it]\u001b[A\n",
            "Iteration:  94% 471/500 [10:07<00:37,  1.28s/it]\u001b[A\n",
            "Iteration:  94% 472/500 [10:08<00:36,  1.30s/it]\u001b[A\n",
            "Iteration:  95% 473/500 [10:09<00:34,  1.28s/it]\u001b[A\n",
            "Iteration:  95% 474/500 [10:11<00:33,  1.30s/it]\u001b[A\n",
            "Iteration:  95% 475/500 [10:12<00:32,  1.29s/it]\u001b[A\n",
            "Iteration:  95% 476/500 [10:13<00:31,  1.30s/it]\u001b[A\n",
            "Iteration:  95% 477/500 [10:14<00:29,  1.28s/it]\u001b[A\n",
            "Iteration:  96% 478/500 [10:16<00:28,  1.29s/it]\u001b[A\n",
            "Iteration:  96% 479/500 [10:17<00:26,  1.28s/it]\u001b[A\n",
            "Iteration:  96% 480/500 [10:18<00:25,  1.29s/it]\u001b[A\n",
            "Iteration:  96% 481/500 [10:20<00:24,  1.28s/it]\u001b[A\n",
            "Iteration:  96% 482/500 [10:21<00:23,  1.29s/it]\u001b[A\n",
            "Iteration:  97% 483/500 [10:22<00:21,  1.28s/it]\u001b[A\n",
            "Iteration:  97% 484/500 [10:23<00:20,  1.29s/it]\u001b[A\n",
            "Iteration:  97% 485/500 [10:25<00:19,  1.28s/it]\u001b[A\n",
            "Iteration:  97% 486/500 [10:26<00:18,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 487/500 [10:27<00:16,  1.29s/it]\u001b[A\n",
            "Iteration:  98% 488/500 [10:29<00:15,  1.29s/it]\u001b[A\n",
            "Iteration:  98% 489/500 [10:30<00:14,  1.28s/it]\u001b[A\n",
            "Iteration:  98% 490/500 [10:31<00:13,  1.30s/it]\u001b[A\n",
            "Iteration:  98% 491/500 [10:32<00:11,  1.28s/it]\u001b[A\n",
            "Iteration:  98% 492/500 [10:34<00:10,  1.29s/it]\u001b[A\n",
            "Iteration:  99% 493/500 [10:35<00:08,  1.28s/it]\u001b[A\n",
            "Iteration:  99% 494/500 [10:36<00:07,  1.29s/it]\u001b[A\n",
            "Iteration:  99% 495/500 [10:38<00:06,  1.28s/it]\u001b[A\n",
            "Iteration:  99% 496/500 [10:39<00:05,  1.31s/it]\u001b[A\n",
            "Iteration:  99% 497/500 [10:40<00:03,  1.28s/it]\u001b[A\n",
            "Iteration: 100% 498/500 [10:42<00:02,  1.29s/it]\u001b[A\n",
            "Iteration: 100% 499/500 [10:43<00:01,  1.28s/it]\u001b[A\n",
            "Iteration: 100% 500/500 [10:44<00:00,  1.29s/it]\n",
            "08/29/2025 12:56:28 - INFO - __main__ -   ***** Eval results *****\n",
            "08/29/2025 12:56:28 - INFO - __main__ -     T2 = 0.0\n",
            "08/29/2025 12:56:28 - INFO - __main__ -     eval_loss = 0.10827750141421953\n",
            "08/29/2025 12:56:28 - INFO - __main__ -     f1 = 0.2352606095129593\n",
            "08/29/2025 12:56:28 - INFO - __main__ -     global_step = 250\n",
            "08/29/2025 12:56:28 - INFO - __main__ -     loss = 0.14784767914563418\n",
            "Epoch:   5% 1/20 [12:02<3:48:38, 722.03s/it]\n",
            "Iteration:   0% 0/500 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/500 [00:01<10:37,  1.28s/it]\u001b[A\n",
            "Iteration:   0% 2/500 [00:02<11:00,  1.33s/it]\u001b[A\n",
            "Iteration:   1% 3/500 [00:03<10:33,  1.27s/it]\u001b[A\n",
            "Iteration:   1% 4/500 [00:05<10:35,  1.28s/it]\u001b[A\n",
            "Iteration:   1% 5/500 [00:06<10:26,  1.27s/it]\u001b[A\n",
            "Iteration:   1% 6/500 [00:07<10:34,  1.28s/it]\u001b[A\n",
            "Iteration:   1% 7/500 [00:08<10:24,  1.27s/it]\u001b[A\n",
            "Iteration:   2% 8/500 [00:10<10:30,  1.28s/it]\u001b[A\n",
            "Iteration:   2% 9/500 [00:11<10:22,  1.27s/it]\u001b[A\n",
            "Iteration:   2% 10/500 [00:12<10:30,  1.29s/it]\u001b[A\n",
            "Iteration:   2% 11/500 [00:14<10:22,  1.27s/it]\u001b[A\n",
            "Iteration:   2% 12/500 [00:15<10:29,  1.29s/it]\u001b[A\n",
            "Iteration:   3% 13/500 [00:16<10:19,  1.27s/it]\u001b[A\n",
            "Iteration:   3% 14/500 [00:17<10:27,  1.29s/it]\u001b[A\n",
            "Iteration:   3% 15/500 [00:19<10:21,  1.28s/it]\u001b[A\n",
            "Iteration:   3% 16/500 [00:20<10:29,  1.30s/it]\u001b[A\n",
            "Iteration:   3% 17/500 [00:21<10:23,  1.29s/it]\u001b[A\n",
            "Iteration:   4% 18/500 [00:23<10:30,  1.31s/it]\u001b[A\n",
            "Iteration:   4% 19/500 [00:24<10:22,  1.29s/it]\u001b[A\n",
            "Iteration:   4% 20/500 [00:25<10:34,  1.32s/it]\u001b[A\n",
            "Iteration:   4% 21/500 [00:27<10:25,  1.31s/it]\u001b[A\n",
            "Iteration:   4% 22/500 [00:28<10:30,  1.32s/it]\u001b[A\n",
            "Iteration:   5% 23/500 [00:29<10:23,  1.31s/it]\u001b[A\n",
            "Iteration:   5% 24/500 [00:31<10:26,  1.32s/it]\u001b[A\n",
            "Iteration:   5% 25/500 [00:32<10:18,  1.30s/it]\u001b[A\n",
            "Iteration:   5% 26/500 [00:33<10:23,  1.31s/it]\u001b[A\n",
            "Iteration:   5% 27/500 [00:34<10:13,  1.30s/it]\u001b[A\n",
            "Iteration:   6% 28/500 [00:36<10:17,  1.31s/it]\u001b[A\n",
            "Iteration:   6% 29/500 [00:37<10:07,  1.29s/it]\u001b[A\n",
            "Iteration:   6% 30/500 [00:38<10:18,  1.32s/it]\u001b[A\n",
            "Iteration:   6% 31/500 [00:40<10:08,  1.30s/it]\u001b[A\n",
            "Iteration:   6% 32/500 [00:41<10:11,  1.31s/it]\u001b[A\n",
            "Iteration:   7% 33/500 [00:42<10:02,  1.29s/it]\u001b[A\n",
            "Iteration:   7% 34/500 [00:44<10:07,  1.30s/it]\u001b[A\n",
            "Iteration:   7% 35/500 [00:45<09:57,  1.29s/it]\u001b[A\n",
            "Iteration:   7% 36/500 [00:46<10:02,  1.30s/it]\u001b[A\n",
            "Iteration:   7% 37/500 [00:47<09:55,  1.29s/it]\u001b[A\n",
            "Iteration:   8% 38/500 [00:49<10:00,  1.30s/it]\u001b[A\n",
            "Iteration:   8% 39/500 [00:50<09:50,  1.28s/it]\u001b[A\n",
            "Iteration:   8% 40/500 [00:51<10:01,  1.31s/it]\u001b[A\n",
            "Iteration:   8% 41/500 [00:53<09:50,  1.29s/it]\u001b[A\n",
            "Iteration:   8% 42/500 [00:54<09:52,  1.29s/it]\u001b[A\n",
            "Iteration:   9% 43/500 [00:55<09:46,  1.28s/it]\u001b[A\n",
            "Iteration:   9% 44/500 [00:56<09:50,  1.29s/it]\u001b[A\n",
            "Iteration:   9% 45/500 [00:58<09:40,  1.28s/it]\u001b[A\n",
            "Iteration:   9% 46/500 [00:59<09:45,  1.29s/it]\u001b[A\n",
            "Iteration:   9% 47/500 [01:00<09:39,  1.28s/it]\u001b[A\n",
            "Iteration:  10% 48/500 [01:02<09:46,  1.30s/it]\u001b[A\n",
            "Iteration:  10% 49/500 [01:03<09:36,  1.28s/it]\u001b[A\n",
            "Iteration:  10% 50/500 [01:04<09:37,  1.28s/it]\u001b[A\n",
            "Iteration:  10% 51/500 [01:05<09:31,  1.27s/it]\u001b[A\n",
            "Iteration:  10% 52/500 [01:07<09:36,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 53/500 [01:08<09:28,  1.27s/it]\u001b[A\n",
            "Iteration:  11% 54/500 [01:09<09:35,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 55/500 [01:11<09:27,  1.28s/it]\u001b[A\n",
            "Iteration:  11% 56/500 [01:12<09:33,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 57/500 [01:13<09:23,  1.27s/it]\u001b[A\n",
            "Iteration:  12% 58/500 [01:14<09:31,  1.29s/it]\u001b[A\n",
            "Iteration:  12% 59/500 [01:16<09:23,  1.28s/it]\u001b[A\n",
            "Iteration:  12% 60/500 [01:17<09:27,  1.29s/it]\u001b[A\n",
            "Iteration:  12% 61/500 [01:18<09:20,  1.28s/it]\u001b[A\n",
            "Iteration:  12% 62/500 [01:20<09:25,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 63/500 [01:21<09:16,  1.27s/it]\u001b[A\n",
            "Iteration:  13% 64/500 [01:22<09:23,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 65/500 [01:23<09:15,  1.28s/it]\u001b[A\n",
            "Iteration:  13% 66/500 [01:25<09:20,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 67/500 [01:26<09:14,  1.28s/it]\u001b[A\n",
            "Iteration:  14% 68/500 [01:27<09:21,  1.30s/it]\u001b[A\n",
            "Iteration:  14% 69/500 [01:29<09:13,  1.28s/it]\u001b[A\n",
            "Iteration:  14% 70/500 [01:30<09:17,  1.30s/it]\u001b[A\n",
            "Iteration:  14% 71/500 [01:31<09:11,  1.29s/it]\u001b[A\n",
            "Iteration:  14% 72/500 [01:32<09:16,  1.30s/it]\u001b[A\n",
            "Iteration:  15% 73/500 [01:34<09:08,  1.29s/it]\u001b[A\n",
            "Iteration:  15% 74/500 [01:35<09:11,  1.30s/it]\u001b[A\n",
            "Iteration:  15% 75/500 [01:36<09:05,  1.28s/it]\u001b[A\n",
            "Iteration:  15% 76/500 [01:38<09:09,  1.30s/it]\u001b[A\n",
            "Iteration:  15% 77/500 [01:39<09:02,  1.28s/it]\u001b[A\n",
            "Iteration:  16% 78/500 [01:40<09:10,  1.31s/it]\u001b[A\n",
            "Iteration:  16% 79/500 [01:41<09:03,  1.29s/it]\u001b[A\n",
            "Iteration:  16% 80/500 [01:43<09:06,  1.30s/it]\u001b[A\n",
            "Iteration:  16% 81/500 [01:44<08:57,  1.28s/it]\u001b[A\n",
            "Iteration:  16% 82/500 [01:45<09:02,  1.30s/it]\u001b[A\n",
            "Iteration:  17% 83/500 [01:47<08:55,  1.28s/it]\u001b[A\n",
            "Iteration:  17% 84/500 [01:48<09:00,  1.30s/it]\u001b[A\n",
            "Iteration:  17% 85/500 [01:49<08:53,  1.29s/it]\u001b[A\n",
            "Iteration:  17% 86/500 [01:51<09:03,  1.31s/it]\u001b[A\n",
            "Iteration:  17% 87/500 [01:52<08:52,  1.29s/it]\u001b[A\n",
            "Iteration:  18% 88/500 [01:53<08:55,  1.30s/it]\u001b[A\n",
            "Iteration:  18% 89/500 [01:54<08:48,  1.28s/it]\u001b[A\n",
            "Iteration:  18% 90/500 [01:56<08:51,  1.30s/it]\u001b[A\n",
            "Iteration:  18% 91/500 [01:57<08:44,  1.28s/it]\u001b[A\n",
            "Iteration:  18% 92/500 [01:58<08:48,  1.29s/it]\u001b[A\n",
            "Iteration:  19% 93/500 [02:00<08:40,  1.28s/it]\u001b[A\n",
            "Iteration:  19% 94/500 [02:01<08:46,  1.30s/it]\u001b[A\n",
            "Iteration:  19% 95/500 [02:02<08:39,  1.28s/it]\u001b[A\n",
            "Iteration:  19% 96/500 [02:03<08:45,  1.30s/it]\u001b[A\n",
            "Iteration:  19% 97/500 [02:05<08:37,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 98/500 [02:06<08:41,  1.30s/it]\u001b[A\n",
            "Iteration:  20% 99/500 [02:07<08:33,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 100/500 [02:09<08:38,  1.30s/it]\u001b[A\n",
            "Iteration:  20% 101/500 [02:10<08:30,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 102/500 [02:11<08:34,  1.29s/it]\u001b[A\n",
            "Iteration:  21% 103/500 [02:12<08:27,  1.28s/it]\u001b[A\n",
            "Iteration:  21% 104/500 [02:14<08:31,  1.29s/it]\u001b[A\n",
            "Iteration:  21% 105/500 [02:15<08:24,  1.28s/it]\u001b[A\n",
            "Iteration:  21% 106/500 [02:16<08:32,  1.30s/it]\u001b[A\n",
            "Iteration:  21% 107/500 [02:18<08:25,  1.29s/it]\u001b[A\n",
            "Iteration:  22% 108/500 [02:19<08:29,  1.30s/it]\u001b[A\n",
            "Iteration:  22% 109/500 [02:20<08:21,  1.28s/it]\u001b[A\n",
            "Iteration:  22% 110/500 [02:21<08:26,  1.30s/it]\u001b[A\n",
            "Iteration:  22% 111/500 [02:23<08:19,  1.28s/it]\u001b[A\n",
            "Iteration:  22% 112/500 [02:24<08:24,  1.30s/it]\u001b[A\n",
            "Iteration:  23% 113/500 [02:25<08:15,  1.28s/it]\u001b[A\n",
            "Iteration:  23% 114/500 [02:27<08:20,  1.30s/it]\u001b[A\n",
            "Iteration:  23% 115/500 [02:28<08:13,  1.28s/it]\u001b[A\n",
            "Iteration:  23% 116/500 [02:29<08:17,  1.29s/it]\u001b[A\n",
            "Iteration:  23% 117/500 [02:30<08:11,  1.28s/it]\u001b[A\n",
            "Iteration:  24% 118/500 [02:32<08:15,  1.30s/it]\u001b[A\n",
            "Iteration:  24% 119/500 [02:33<08:07,  1.28s/it]\u001b[A\n",
            "Iteration:  24% 120/500 [02:34<08:12,  1.30s/it]\u001b[A\n",
            "Iteration:  24% 121/500 [02:36<08:04,  1.28s/it]\u001b[A\n",
            "Iteration:  24% 122/500 [02:37<08:09,  1.30s/it]\u001b[A\n",
            "Iteration:  25% 123/500 [02:38<08:02,  1.28s/it]\u001b[A\n",
            "Iteration:  25% 124/500 [02:40<08:10,  1.30s/it]\u001b[A\n",
            "Iteration:  25% 125/500 [02:41<08:01,  1.28s/it]\u001b[A\n",
            "Iteration:  25% 126/500 [02:42<08:03,  1.29s/it]\u001b[A\n",
            "Iteration:  25% 127/500 [02:43<08:00,  1.29s/it]\u001b[A\n",
            "Iteration:  26% 128/500 [02:45<08:02,  1.30s/it]\u001b[A\n",
            "Iteration:  26% 129/500 [02:46<07:56,  1.28s/it]\u001b[A\n",
            "Iteration:  26% 130/500 [02:47<07:59,  1.30s/it]\u001b[A\n",
            "Iteration:  26% 131/500 [02:49<07:52,  1.28s/it]\u001b[A\n",
            "Iteration:  26% 132/500 [02:50<07:56,  1.30s/it]\u001b[A\n",
            "Iteration:  27% 133/500 [02:51<07:49,  1.28s/it]\u001b[A\n",
            "Iteration:  27% 134/500 [02:52<07:56,  1.30s/it]\u001b[A\n",
            "Iteration:  27% 135/500 [02:54<07:47,  1.28s/it]\u001b[A\n",
            "Iteration:  27% 136/500 [02:55<07:50,  1.29s/it]\u001b[A\n",
            "Iteration:  27% 137/500 [02:56<07:44,  1.28s/it]\u001b[A\n",
            "Iteration:  28% 138/500 [02:58<07:47,  1.29s/it]\u001b[A\n",
            "Iteration:  28% 139/500 [02:59<07:40,  1.28s/it]\u001b[A\n",
            "Iteration:  28% 140/500 [03:00<07:44,  1.29s/it]\u001b[A\n",
            "Iteration:  28% 141/500 [03:01<07:38,  1.28s/it]\u001b[A\n",
            "Iteration:  28% 142/500 [03:03<07:42,  1.29s/it]\u001b[A\n",
            "Iteration:  29% 143/500 [03:04<07:38,  1.28s/it]\u001b[A\n",
            "Iteration:  29% 144/500 [03:05<07:47,  1.31s/it]\u001b[A\n",
            "Iteration:  29% 145/500 [03:07<07:38,  1.29s/it]\u001b[A\n",
            "Iteration:  29% 146/500 [03:08<07:39,  1.30s/it]\u001b[A\n",
            "Iteration:  29% 147/500 [03:09<07:33,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 148/500 [03:10<07:35,  1.29s/it]\u001b[A\n",
            "Iteration:  30% 149/500 [03:12<07:28,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 150/500 [03:13<07:32,  1.29s/it]\u001b[A\n",
            "Iteration:  30% 151/500 [03:14<07:25,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 152/500 [03:16<07:33,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 153/500 [03:17<07:25,  1.28s/it]\u001b[A\n",
            "Iteration:  31% 154/500 [03:18<07:28,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 155/500 [03:19<07:22,  1.28s/it]\u001b[A\n",
            "Iteration:  31% 156/500 [03:21<07:26,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 157/500 [03:22<07:20,  1.28s/it]\u001b[A\n",
            "Iteration:  32% 158/500 [03:23<07:24,  1.30s/it]\u001b[A\n",
            "Iteration:  32% 159/500 [03:25<07:16,  1.28s/it]\u001b[A\n",
            "Iteration:  32% 160/500 [03:26<07:19,  1.29s/it]\u001b[A\n",
            "Iteration:  32% 161/500 [03:27<07:12,  1.28s/it]\u001b[A\n",
            "Iteration:  32% 162/500 [03:29<07:20,  1.30s/it]\u001b[A\n",
            "Iteration:  33% 163/500 [03:30<07:11,  1.28s/it]\u001b[A\n",
            "Iteration:  33% 164/500 [03:31<07:14,  1.29s/it]\u001b[A\n",
            "Iteration:  33% 165/500 [03:32<07:08,  1.28s/it]\u001b[A\n",
            "Iteration:  33% 166/500 [03:34<07:12,  1.29s/it]\u001b[A\n",
            "Iteration:  33% 167/500 [03:35<07:05,  1.28s/it]\u001b[A\n",
            "Iteration:  34% 168/500 [03:36<07:08,  1.29s/it]\u001b[A\n",
            "Iteration:  34% 169/500 [03:37<07:04,  1.28s/it]\u001b[A\n",
            "Iteration:  34% 170/500 [03:39<07:06,  1.29s/it]\u001b[A\n",
            "Iteration:  34% 171/500 [03:40<07:00,  1.28s/it]\u001b[A\n",
            "Iteration:  34% 172/500 [03:41<07:07,  1.30s/it]\u001b[A\n",
            "Iteration:  35% 173/500 [03:43<06:58,  1.28s/it]\u001b[A\n",
            "Iteration:  35% 174/500 [03:44<07:01,  1.29s/it]\u001b[A\n",
            "Iteration:  35% 175/500 [03:45<06:55,  1.28s/it]\u001b[A\n",
            "Iteration:  35% 176/500 [03:47<06:58,  1.29s/it]\u001b[A\n",
            "Iteration:  35% 177/500 [03:48<06:54,  1.28s/it]\u001b[A\n",
            "Iteration:  36% 178/500 [03:49<06:57,  1.30s/it]\u001b[A\n",
            "Iteration:  36% 179/500 [03:50<06:50,  1.28s/it]\u001b[A\n",
            "Iteration:  36% 180/500 [03:52<06:52,  1.29s/it]\u001b[A\n",
            "Iteration:  36% 181/500 [03:53<06:46,  1.27s/it]\u001b[A\n",
            "Iteration:  36% 182/500 [03:54<06:50,  1.29s/it]\u001b[A\n",
            "Iteration:  37% 183/500 [03:56<06:45,  1.28s/it]\u001b[A\n",
            "Iteration:  37% 184/500 [03:57<06:48,  1.29s/it]\u001b[A\n",
            "Iteration:  37% 185/500 [03:58<06:43,  1.28s/it]\u001b[A\n",
            "Iteration:  37% 186/500 [03:59<06:46,  1.30s/it]\u001b[A\n",
            "Iteration:  37% 187/500 [04:01<06:40,  1.28s/it]\u001b[A\n",
            "Iteration:  38% 188/500 [04:02<06:43,  1.29s/it]\u001b[A\n",
            "Iteration:  38% 189/500 [04:03<06:38,  1.28s/it]\u001b[A\n",
            "Iteration:  38% 190/500 [04:05<06:45,  1.31s/it]\u001b[A\n",
            "Iteration:  38% 191/500 [04:06<06:38,  1.29s/it]\u001b[A\n",
            "Iteration:  38% 192/500 [04:07<06:40,  1.30s/it]\u001b[A\n",
            "Iteration:  39% 193/500 [04:08<06:34,  1.28s/it]\u001b[A\n",
            "Iteration:  39% 194/500 [04:10<06:37,  1.30s/it]\u001b[A\n",
            "Iteration:  39% 195/500 [04:11<06:30,  1.28s/it]\u001b[A\n",
            "Iteration:  39% 196/500 [04:12<06:33,  1.29s/it]\u001b[A\n",
            "Iteration:  39% 197/500 [04:14<06:28,  1.28s/it]\u001b[A\n",
            "Iteration:  40% 198/500 [04:15<06:30,  1.29s/it]\u001b[A\n",
            "Iteration:  40% 199/500 [04:16<06:24,  1.28s/it]\u001b[A\n",
            "Iteration:  40% 200/500 [04:17<06:29,  1.30s/it]\u001b[A\n",
            "Iteration:  40% 201/500 [04:19<06:22,  1.28s/it]\u001b[A\n",
            "Iteration:  40% 202/500 [04:20<06:26,  1.30s/it]\u001b[A\n",
            "Iteration:  41% 203/500 [04:21<06:20,  1.28s/it]\u001b[A\n",
            "Iteration:  41% 204/500 [04:23<06:23,  1.30s/it]\u001b[A\n",
            "Iteration:  41% 205/500 [04:24<06:18,  1.28s/it]\u001b[A\n",
            "Iteration:  41% 206/500 [04:25<06:20,  1.30s/it]\u001b[A\n",
            "Iteration:  41% 207/500 [04:26<06:15,  1.28s/it]\u001b[A\n",
            "Iteration:  42% 208/500 [04:28<06:19,  1.30s/it]\u001b[A\n",
            "Iteration:  42% 209/500 [04:29<06:13,  1.28s/it]\u001b[A\n",
            "Iteration:  42% 210/500 [04:30<06:19,  1.31s/it]\u001b[A\n",
            "Iteration:  42% 211/500 [04:32<06:12,  1.29s/it]\u001b[A\n",
            "Iteration:  42% 212/500 [04:33<06:15,  1.30s/it]\u001b[A\n",
            "Iteration:  43% 213/500 [04:34<06:08,  1.28s/it]\u001b[A\n",
            "Iteration:  43% 214/500 [04:36<06:11,  1.30s/it]\u001b[A\n",
            "Iteration:  43% 215/500 [04:37<06:05,  1.28s/it]\u001b[A\n",
            "Iteration:  43% 216/500 [04:38<06:08,  1.30s/it]\u001b[A\n",
            "Iteration:  43% 217/500 [04:39<06:02,  1.28s/it]\u001b[A\n",
            "Iteration:  44% 218/500 [04:41<06:07,  1.30s/it]\u001b[A\n",
            "Iteration:  44% 219/500 [04:42<05:59,  1.28s/it]\u001b[A\n",
            "Iteration:  44% 220/500 [04:43<06:02,  1.29s/it]\u001b[A\n",
            "Iteration:  44% 221/500 [04:45<05:57,  1.28s/it]\u001b[A\n",
            "Iteration:  44% 222/500 [04:46<06:00,  1.30s/it]\u001b[A\n",
            "Iteration:  45% 223/500 [04:47<05:54,  1.28s/it]\u001b[A\n",
            "Iteration:  45% 224/500 [04:48<05:56,  1.29s/it]\u001b[A\n",
            "Iteration:  45% 225/500 [04:50<05:51,  1.28s/it]\u001b[A\n",
            "Iteration:  45% 226/500 [04:51<05:54,  1.29s/it]\u001b[A\n",
            "Iteration:  45% 227/500 [04:52<05:47,  1.27s/it]\u001b[A\n",
            "Iteration:  46% 228/500 [04:54<05:51,  1.29s/it]\u001b[A\n",
            "Iteration:  46% 229/500 [04:55<05:46,  1.28s/it]\u001b[A\n",
            "Iteration:  46% 230/500 [04:56<05:48,  1.29s/it]\u001b[A\n",
            "Iteration:  46% 231/500 [04:57<05:43,  1.28s/it]\u001b[A\n",
            "Iteration:  46% 232/500 [04:59<05:45,  1.29s/it]\u001b[A\n",
            "Iteration:  47% 233/500 [05:00<05:40,  1.28s/it]\u001b[A\n",
            "Iteration:  47% 234/500 [05:01<05:42,  1.29s/it]\u001b[A\n",
            "Iteration:  47% 235/500 [05:03<05:38,  1.28s/it]\u001b[A\n",
            "Iteration:  47% 236/500 [05:04<05:40,  1.29s/it]\u001b[A\n",
            "Iteration:  47% 237/500 [05:05<05:35,  1.28s/it]\u001b[A\n",
            "Iteration:  48% 238/500 [05:06<05:41,  1.30s/it]\u001b[A\n",
            "Iteration:  48% 239/500 [05:08<05:34,  1.28s/it]\u001b[A\n",
            "Iteration:  48% 240/500 [05:09<05:36,  1.29s/it]\u001b[A\n",
            "Iteration:  48% 241/500 [05:10<05:30,  1.28s/it]\u001b[A\n",
            "Iteration:  48% 242/500 [05:12<05:33,  1.29s/it]\u001b[A\n",
            "Iteration:  49% 243/500 [05:13<05:29,  1.28s/it]\u001b[A\n",
            "Iteration:  49% 244/500 [05:14<05:31,  1.29s/it]\u001b[A\n",
            "Iteration:  49% 245/500 [05:15<05:26,  1.28s/it]\u001b[A\n",
            "Iteration:  49% 246/500 [05:17<05:29,  1.30s/it]\u001b[A\n",
            "Iteration:  49% 247/500 [05:18<05:24,  1.28s/it]\u001b[A\n",
            "Iteration:  50% 248/500 [05:19<05:25,  1.29s/it]\u001b[A\n",
            "Iteration:  50% 249/500 [05:21<05:21,  1.28s/it]\u001b[A\n",
            "Iteration:  50% 250/500 [05:22<05:24,  1.30s/it]\u001b[A\n",
            "Iteration:  50% 251/500 [05:23<05:19,  1.28s/it]\u001b[A\n",
            "Iteration:  50% 252/500 [05:24<05:21,  1.29s/it]\u001b[A\n",
            "Iteration:  51% 253/500 [05:26<05:16,  1.28s/it]\u001b[A\n",
            "Iteration:  51% 254/500 [05:27<05:19,  1.30s/it]\u001b[A\n",
            "Iteration:  51% 255/500 [05:28<05:13,  1.28s/it]\u001b[A\n",
            "Iteration:  51% 256/500 [05:30<05:17,  1.30s/it]\u001b[A\n",
            "Iteration:  51% 257/500 [05:31<05:11,  1.28s/it]\u001b[A\n",
            "Iteration:  52% 258/500 [05:32<05:12,  1.29s/it]\u001b[A\n",
            "Iteration:  52% 259/500 [05:33<05:08,  1.28s/it]\u001b[A\n",
            "Iteration:  52% 260/500 [05:35<05:11,  1.30s/it]\u001b[A\n",
            "Iteration:  52% 261/500 [05:36<05:05,  1.28s/it]\u001b[A\n",
            "Iteration:  52% 262/500 [05:37<05:07,  1.29s/it]\u001b[A\n",
            "Iteration:  53% 263/500 [05:39<05:02,  1.28s/it]\u001b[A\n",
            "Iteration:  53% 264/500 [05:40<05:03,  1.29s/it]\u001b[A\n",
            "Iteration:  53% 265/500 [05:41<04:59,  1.27s/it]\u001b[A\n",
            "Iteration:  53% 266/500 [05:42<05:02,  1.29s/it]\u001b[A\n",
            "Iteration:  53% 267/500 [05:44<04:57,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 268/500 [05:45<04:59,  1.29s/it]\u001b[A\n",
            "Iteration:  54% 269/500 [05:46<04:54,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 270/500 [05:48<04:58,  1.30s/it]\u001b[A\n",
            "Iteration:  54% 271/500 [05:49<04:53,  1.28s/it]\u001b[A\n",
            "Iteration:  54% 272/500 [05:50<04:55,  1.29s/it]\u001b[A\n",
            "Iteration:  55% 273/500 [05:51<04:50,  1.28s/it]\u001b[A\n",
            "Iteration:  55% 274/500 [05:53<04:51,  1.29s/it]\u001b[A\n",
            "Iteration:  55% 275/500 [05:54<04:46,  1.27s/it]\u001b[A\n",
            "Iteration:  55% 276/500 [05:55<04:50,  1.30s/it]\u001b[A\n",
            "Iteration:  55% 277/500 [05:57<04:46,  1.28s/it]\u001b[A\n",
            "Iteration:  56% 278/500 [05:58<04:47,  1.29s/it]\u001b[A\n",
            "Iteration:  56% 279/500 [05:59<04:44,  1.29s/it]\u001b[A\n",
            "Iteration:  56% 280/500 [06:00<04:45,  1.30s/it]\u001b[A\n",
            "Iteration:  56% 281/500 [06:02<04:40,  1.28s/it]\u001b[A\n",
            "Iteration:  56% 282/500 [06:03<04:42,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 283/500 [06:04<04:38,  1.28s/it]\u001b[A\n",
            "Iteration:  57% 284/500 [06:06<04:39,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 285/500 [06:07<04:36,  1.29s/it]\u001b[A\n",
            "Iteration:  57% 286/500 [06:08<04:37,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 287/500 [06:09<04:33,  1.28s/it]\u001b[A\n",
            "Iteration:  58% 288/500 [06:11<04:34,  1.30s/it]\u001b[A\n",
            "Iteration:  58% 289/500 [06:12<04:29,  1.28s/it]\u001b[A\n",
            "Iteration:  58% 290/500 [06:13<04:31,  1.29s/it]\u001b[A\n",
            "Iteration:  58% 291/500 [06:15<04:28,  1.28s/it]\u001b[A\n",
            "Iteration:  58% 292/500 [06:16<04:29,  1.29s/it]\u001b[A\n",
            "Iteration:  59% 293/500 [06:17<04:25,  1.28s/it]\u001b[A\n",
            "Iteration:  59% 294/500 [06:19<04:28,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 295/500 [06:20<04:23,  1.28s/it]\u001b[A\n",
            "Iteration:  59% 296/500 [06:21<04:25,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 297/500 [06:22<04:20,  1.28s/it]\u001b[A\n",
            "Iteration:  60% 298/500 [06:24<04:22,  1.30s/it]\u001b[A\n",
            "Iteration:  60% 299/500 [06:25<04:17,  1.28s/it]\u001b[A\n",
            "Iteration:  60% 300/500 [06:26<04:20,  1.30s/it]\u001b[A\n",
            "Iteration:  60% 301/500 [06:28<04:15,  1.28s/it]\u001b[A\n",
            "Iteration:  60% 302/500 [06:29<04:17,  1.30s/it]\u001b[A\n",
            "Iteration:  61% 303/500 [06:30<04:13,  1.29s/it]\u001b[A\n",
            "Iteration:  61% 304/500 [06:31<04:16,  1.31s/it]\u001b[A\n",
            "Iteration:  61% 305/500 [06:33<04:11,  1.29s/it]\u001b[A\n",
            "Iteration:  61% 306/500 [06:34<04:11,  1.30s/it]\u001b[A\n",
            "Iteration:  61% 307/500 [06:35<04:08,  1.29s/it]\u001b[A\n",
            "Iteration:  62% 308/500 [06:37<04:09,  1.30s/it]\u001b[A\n",
            "Iteration:  62% 309/500 [06:38<04:05,  1.28s/it]\u001b[A\n",
            "Iteration:  62% 310/500 [06:39<04:07,  1.30s/it]\u001b[A\n",
            "Iteration:  62% 311/500 [06:40<04:03,  1.29s/it]\u001b[A\n",
            "Iteration:  62% 312/500 [06:42<04:04,  1.30s/it]\u001b[A\n",
            "Iteration:  63% 313/500 [06:43<04:00,  1.29s/it]\u001b[A\n",
            "Iteration:  63% 314/500 [06:44<04:03,  1.31s/it]\u001b[A\n",
            "Iteration:  63% 315/500 [06:46<03:58,  1.29s/it]\u001b[A\n",
            "Iteration:  63% 316/500 [06:47<03:58,  1.30s/it]\u001b[A\n",
            "Iteration:  63% 317/500 [06:48<03:54,  1.28s/it]\u001b[A\n",
            "Iteration:  64% 318/500 [06:50<03:55,  1.29s/it]\u001b[A\n",
            "Iteration:  64% 319/500 [06:51<03:51,  1.28s/it]\u001b[A\n",
            "Iteration:  64% 320/500 [06:52<03:52,  1.29s/it]\u001b[A\n",
            "Iteration:  64% 321/500 [06:53<03:48,  1.28s/it]\u001b[A\n",
            "Iteration:  64% 322/500 [06:55<03:50,  1.29s/it]\u001b[A\n",
            "Iteration:  65% 323/500 [06:56<03:46,  1.28s/it]\u001b[A\n",
            "Iteration:  65% 324/500 [06:57<03:47,  1.29s/it]\u001b[A\n",
            "Iteration:  65% 325/500 [06:59<03:43,  1.28s/it]\u001b[A\n",
            "Iteration:  65% 326/500 [07:00<03:44,  1.29s/it]\u001b[A\n",
            "Iteration:  65% 327/500 [07:01<03:40,  1.28s/it]\u001b[A\n",
            "Iteration:  66% 328/500 [07:02<03:42,  1.29s/it]\u001b[A\n",
            "Iteration:  66% 329/500 [07:04<03:38,  1.28s/it]\u001b[A\n",
            "Iteration:  66% 330/500 [07:05<03:39,  1.29s/it]\u001b[A\n",
            "Iteration:  66% 331/500 [07:06<03:36,  1.28s/it]\u001b[A\n",
            "Iteration:  66% 332/500 [07:08<03:38,  1.30s/it]\u001b[A\n",
            "Iteration:  67% 333/500 [07:09<03:34,  1.29s/it]\u001b[A\n",
            "Iteration:  67% 334/500 [07:10<03:35,  1.30s/it]\u001b[A\n",
            "Iteration:  67% 335/500 [07:11<03:32,  1.29s/it]\u001b[A\n",
            "Iteration:  67% 336/500 [07:13<03:32,  1.30s/it]\u001b[A\n",
            "Iteration:  67% 337/500 [07:14<03:28,  1.28s/it]\u001b[A\n",
            "Iteration:  68% 338/500 [07:15<03:29,  1.29s/it]\u001b[A\n",
            "Iteration:  68% 339/500 [07:17<03:25,  1.28s/it]\u001b[A\n",
            "Iteration:  68% 340/500 [07:18<03:26,  1.29s/it]\u001b[A\n",
            "Iteration:  68% 341/500 [07:19<03:22,  1.28s/it]\u001b[A\n",
            "Iteration:  68% 342/500 [07:20<03:25,  1.30s/it]\u001b[A\n",
            "Iteration:  69% 343/500 [07:22<03:21,  1.28s/it]\u001b[A\n",
            "Iteration:  69% 344/500 [07:23<03:21,  1.29s/it]\u001b[A\n",
            "Iteration:  69% 345/500 [07:24<03:18,  1.28s/it]\u001b[A\n",
            "Iteration:  69% 346/500 [07:26<03:19,  1.29s/it]\u001b[A\n",
            "Iteration:  69% 347/500 [07:27<03:15,  1.28s/it]\u001b[A\n",
            "Iteration:  70% 348/500 [07:28<03:16,  1.29s/it]\u001b[A\n",
            "Iteration:  70% 349/500 [07:29<03:13,  1.28s/it]\u001b[A\n",
            "Iteration:  70% 350/500 [07:31<03:14,  1.30s/it]\u001b[A\n",
            "Iteration:  70% 351/500 [07:32<03:10,  1.28s/it]\u001b[A\n",
            "Iteration:  70% 352/500 [07:33<03:13,  1.31s/it]\u001b[A\n",
            "Iteration:  71% 353/500 [07:35<03:09,  1.29s/it]\u001b[A\n",
            "Iteration:  71% 354/500 [07:36<03:10,  1.31s/it]\u001b[A\n",
            "Iteration:  71% 355/500 [07:37<03:06,  1.29s/it]\u001b[A\n",
            "Iteration:  71% 356/500 [07:39<03:07,  1.30s/it]\u001b[A\n",
            "Iteration:  71% 357/500 [07:40<03:03,  1.28s/it]\u001b[A\n",
            "Iteration:  72% 358/500 [07:41<03:04,  1.30s/it]\u001b[A\n",
            "Iteration:  72% 359/500 [07:42<03:00,  1.28s/it]\u001b[A\n",
            "Iteration:  72% 360/500 [07:44<03:03,  1.31s/it]\u001b[A\n",
            "Iteration:  72% 361/500 [07:45<02:58,  1.29s/it]\u001b[A\n",
            "Iteration:  72% 362/500 [07:46<02:58,  1.29s/it]\u001b[A\n",
            "Iteration:  73% 363/500 [07:48<02:55,  1.28s/it]\u001b[A\n",
            "Iteration:  73% 364/500 [07:49<02:57,  1.30s/it]\u001b[A\n",
            "Iteration:  73% 365/500 [07:50<02:53,  1.29s/it]\u001b[A\n",
            "Iteration:  73% 366/500 [07:51<02:53,  1.30s/it]\u001b[A\n",
            "Iteration:  73% 367/500 [07:53<02:50,  1.28s/it]\u001b[A\n",
            "Iteration:  74% 368/500 [07:54<02:51,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 369/500 [07:55<02:48,  1.28s/it]\u001b[A\n",
            "Iteration:  74% 370/500 [07:57<02:49,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 371/500 [07:58<02:45,  1.28s/it]\u001b[A\n",
            "Iteration:  74% 372/500 [07:59<02:45,  1.30s/it]\u001b[A\n",
            "Iteration:  75% 373/500 [08:00<02:43,  1.28s/it]\u001b[A\n",
            "Iteration:  75% 374/500 [08:02<02:43,  1.29s/it]\u001b[A\n",
            "Iteration:  75% 375/500 [08:03<02:39,  1.28s/it]\u001b[A\n",
            "Iteration:  75% 376/500 [08:04<02:39,  1.29s/it]\u001b[A\n",
            "Iteration:  75% 377/500 [08:06<02:37,  1.28s/it]\u001b[A\n",
            "Iteration:  76% 378/500 [08:07<02:37,  1.29s/it]\u001b[A\n",
            "Iteration:  76% 379/500 [08:08<02:34,  1.27s/it]\u001b[A\n",
            "Iteration:  76% 380/500 [08:09<02:36,  1.31s/it]\u001b[A\n",
            "Iteration:  76% 381/500 [08:11<02:32,  1.29s/it]\u001b[A\n",
            "Iteration:  76% 382/500 [08:12<02:32,  1.29s/it]\u001b[A\n",
            "Iteration:  77% 383/500 [08:13<02:29,  1.28s/it]\u001b[A\n",
            "Iteration:  77% 384/500 [08:15<02:30,  1.30s/it]\u001b[A\n",
            "Iteration:  77% 385/500 [08:16<02:27,  1.28s/it]\u001b[A\n",
            "Iteration:  77% 386/500 [08:17<02:27,  1.29s/it]\u001b[A\n",
            "Iteration:  77% 387/500 [08:18<02:24,  1.28s/it]\u001b[A\n",
            "Iteration:  78% 388/500 [08:20<02:24,  1.29s/it]\u001b[A\n",
            "Iteration:  78% 389/500 [08:21<02:22,  1.28s/it]\u001b[A\n",
            "Iteration:  78% 390/500 [08:22<02:23,  1.31s/it]\u001b[A\n",
            "Iteration:  78% 391/500 [08:24<02:19,  1.28s/it]\u001b[A\n",
            "Iteration:  78% 392/500 [08:25<02:19,  1.29s/it]\u001b[A\n",
            "Iteration:  79% 393/500 [08:26<02:17,  1.28s/it]\u001b[A\n",
            "Iteration:  79% 394/500 [08:27<02:17,  1.29s/it]\u001b[A\n",
            "Iteration:  79% 395/500 [08:29<02:14,  1.28s/it]\u001b[A\n",
            "Iteration:  79% 396/500 [08:30<02:14,  1.29s/it]\u001b[A\n",
            "Iteration:  79% 397/500 [08:31<02:11,  1.28s/it]\u001b[A\n",
            "Iteration:  80% 398/500 [08:33<02:12,  1.30s/it]\u001b[A\n",
            "Iteration:  80% 399/500 [08:34<02:09,  1.28s/it]\u001b[A\n",
            "Iteration:  80% 400/500 [08:35<02:09,  1.29s/it]\u001b[A\n",
            "Iteration:  80% 401/500 [08:36<02:06,  1.28s/it]\u001b[A\n",
            "Iteration:  80% 402/500 [08:38<02:06,  1.30s/it]\u001b[A\n",
            "Iteration:  81% 403/500 [08:39<02:03,  1.28s/it]\u001b[A\n",
            "Iteration:  81% 404/500 [08:40<02:03,  1.29s/it]\u001b[A\n",
            "Iteration:  81% 405/500 [08:42<02:01,  1.28s/it]\u001b[A\n",
            "Iteration:  81% 406/500 [08:43<02:01,  1.29s/it]\u001b[A\n",
            "Iteration:  81% 407/500 [08:44<01:58,  1.27s/it]\u001b[A\n",
            "Iteration:  82% 408/500 [08:46<01:59,  1.30s/it]\u001b[A\n",
            "Iteration:  82% 409/500 [08:47<01:56,  1.28s/it]\u001b[A\n",
            "Iteration:  82% 410/500 [08:48<01:56,  1.29s/it]\u001b[A\n",
            "Iteration:  82% 411/500 [08:49<01:53,  1.28s/it]\u001b[A\n",
            "Iteration:  82% 412/500 [08:51<01:54,  1.30s/it]\u001b[A\n",
            "Iteration:  83% 413/500 [08:52<01:51,  1.28s/it]\u001b[A\n",
            "Iteration:  83% 414/500 [08:53<01:50,  1.29s/it]\u001b[A\n",
            "Iteration:  83% 415/500 [08:54<01:48,  1.28s/it]\u001b[A\n",
            "Iteration:  83% 416/500 [08:56<01:48,  1.29s/it]\u001b[A\n",
            "Iteration:  83% 417/500 [08:57<01:45,  1.27s/it]\u001b[A\n",
            "Iteration:  84% 418/500 [08:58<01:46,  1.30s/it]\u001b[A\n",
            "Iteration:  84% 419/500 [09:00<01:43,  1.28s/it]\u001b[A\n",
            "Iteration:  84% 420/500 [09:01<01:42,  1.29s/it]\u001b[A\n",
            "Iteration:  84% 421/500 [09:02<01:40,  1.27s/it]\u001b[A\n",
            "Iteration:  84% 422/500 [09:03<01:40,  1.29s/it]\u001b[A\n",
            "Iteration:  85% 423/500 [09:05<01:37,  1.27s/it]\u001b[A\n",
            "Iteration:  85% 424/500 [09:06<01:38,  1.29s/it]\u001b[A\n",
            "Iteration:  85% 425/500 [09:07<01:36,  1.28s/it]\u001b[A\n",
            "Iteration:  85% 426/500 [09:09<01:35,  1.29s/it]\u001b[A\n",
            "Iteration:  85% 427/500 [09:10<01:33,  1.28s/it]\u001b[A\n",
            "Iteration:  86% 428/500 [09:11<01:34,  1.31s/it]\u001b[A\n",
            "Iteration:  86% 429/500 [09:12<01:31,  1.29s/it]\u001b[A\n",
            "Iteration:  86% 430/500 [09:14<01:30,  1.30s/it]\u001b[A\n",
            "Iteration:  86% 431/500 [09:15<01:28,  1.28s/it]\u001b[A\n",
            "Iteration:  86% 432/500 [09:16<01:28,  1.29s/it]\u001b[A\n",
            "Iteration:  87% 433/500 [09:18<01:25,  1.28s/it]\u001b[A\n",
            "Iteration:  87% 434/500 [09:19<01:25,  1.30s/it]\u001b[A\n",
            "Iteration:  87% 435/500 [09:20<01:23,  1.28s/it]\u001b[A\n",
            "Iteration:  87% 436/500 [09:22<01:23,  1.31s/it]\u001b[A\n",
            "Iteration:  87% 437/500 [09:23<01:20,  1.28s/it]\u001b[A\n",
            "Iteration:  88% 438/500 [09:24<01:20,  1.30s/it]\u001b[A\n",
            "Iteration:  88% 439/500 [09:25<01:17,  1.28s/it]\u001b[A\n",
            "Iteration:  88% 440/500 [09:27<01:17,  1.29s/it]\u001b[A\n",
            "Iteration:  88% 441/500 [09:28<01:15,  1.28s/it]\u001b[A\n",
            "Iteration:  88% 442/500 [09:29<01:15,  1.30s/it]\u001b[A\n",
            "Iteration:  89% 443/500 [09:31<01:13,  1.28s/it]\u001b[A\n",
            "Iteration:  89% 444/500 [09:32<01:12,  1.29s/it]\u001b[A\n",
            "Iteration:  89% 445/500 [09:33<01:10,  1.28s/it]\u001b[A\n",
            "Iteration:  89% 446/500 [09:34<01:10,  1.30s/it]\u001b[A\n",
            "Iteration:  89% 447/500 [09:36<01:07,  1.28s/it]\u001b[A\n",
            "Iteration:  90% 448/500 [09:37<01:07,  1.29s/it]\u001b[A\n",
            "Iteration:  90% 449/500 [09:38<01:05,  1.28s/it]\u001b[A\n",
            "Iteration:  90% 450/500 [09:40<01:04,  1.30s/it]\u001b[A\n",
            "Iteration:  90% 451/500 [09:41<01:03,  1.29s/it]\u001b[A\n",
            "Iteration:  90% 452/500 [09:42<01:02,  1.30s/it]\u001b[A\n",
            "Iteration:  91% 453/500 [09:43<01:00,  1.28s/it]\u001b[A\n",
            "Iteration:  91% 454/500 [09:45<00:59,  1.30s/it]\u001b[A\n",
            "Iteration:  91% 455/500 [09:46<00:57,  1.28s/it]\u001b[A\n",
            "Iteration:  91% 456/500 [09:47<00:57,  1.31s/it]\u001b[A\n",
            "Iteration:  91% 457/500 [09:49<00:55,  1.29s/it]\u001b[A\n",
            "Iteration:  92% 458/500 [09:50<00:54,  1.30s/it]\u001b[A\n",
            "Iteration:  92% 459/500 [09:51<00:52,  1.29s/it]\u001b[A\n",
            "Iteration:  92% 460/500 [09:53<00:51,  1.30s/it]\u001b[A\n",
            "Iteration:  92% 461/500 [09:54<00:50,  1.29s/it]\u001b[A\n",
            "Iteration:  92% 462/500 [09:55<00:49,  1.31s/it]\u001b[A\n",
            "Iteration:  93% 463/500 [09:56<00:47,  1.29s/it]\u001b[A\n",
            "Iteration:  93% 464/500 [09:58<00:46,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 465/500 [09:59<00:44,  1.28s/it]\u001b[A\n",
            "Iteration:  93% 466/500 [10:00<00:44,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 467/500 [10:02<00:42,  1.28s/it]\u001b[A\n",
            "Iteration:  94% 468/500 [10:03<00:41,  1.30s/it]\u001b[A\n",
            "Iteration:  94% 469/500 [10:04<00:39,  1.28s/it]\u001b[A\n",
            "Iteration:  94% 470/500 [10:05<00:38,  1.30s/it]\u001b[A\n",
            "Iteration:  94% 471/500 [10:07<00:37,  1.28s/it]\u001b[A\n",
            "Iteration:  94% 472/500 [10:08<00:36,  1.30s/it]\u001b[A\n",
            "Iteration:  95% 473/500 [10:09<00:34,  1.28s/it]\u001b[A\n",
            "Iteration:  95% 474/500 [10:11<00:34,  1.31s/it]\u001b[A\n",
            "Iteration:  95% 475/500 [10:12<00:32,  1.29s/it]\u001b[A\n",
            "Iteration:  95% 476/500 [10:13<00:31,  1.30s/it]\u001b[A\n",
            "Iteration:  95% 477/500 [10:14<00:29,  1.28s/it]\u001b[A\n",
            "Iteration:  96% 478/500 [10:16<00:28,  1.30s/it]\u001b[A\n",
            "Iteration:  96% 479/500 [10:17<00:26,  1.28s/it]\u001b[A\n",
            "Iteration:  96% 480/500 [10:18<00:25,  1.30s/it]\u001b[A\n",
            "Iteration:  96% 481/500 [10:20<00:24,  1.28s/it]\u001b[A\n",
            "Iteration:  96% 482/500 [10:21<00:23,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 483/500 [10:22<00:21,  1.28s/it]\u001b[A\n",
            "Iteration:  97% 484/500 [10:24<00:20,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 485/500 [10:25<00:19,  1.29s/it]\u001b[A\n",
            "Iteration:  97% 486/500 [10:26<00:18,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 487/500 [10:27<00:16,  1.28s/it]\u001b[A\n",
            "Iteration:  98% 488/500 [10:29<00:15,  1.30s/it]\u001b[A\n",
            "Iteration:  98% 489/500 [10:30<00:14,  1.28s/it]\u001b[A\n",
            "Iteration:  98% 490/500 [10:31<00:12,  1.30s/it]\u001b[A\n",
            "Iteration:  98% 491/500 [10:33<00:11,  1.29s/it]\u001b[A\n",
            "Iteration:  98% 492/500 [10:34<00:10,  1.30s/it]\u001b[A\n",
            "Iteration:  99% 493/500 [10:35<00:08,  1.28s/it]\u001b[A\n",
            "Iteration:  99% 494/500 [10:36<00:07,  1.31s/it]\u001b[A\n",
            "Iteration:  99% 495/500 [10:38<00:06,  1.29s/it]\u001b[A\n",
            "Iteration:  99% 496/500 [10:39<00:05,  1.30s/it]\u001b[A\n",
            "Iteration:  99% 497/500 [10:40<00:03,  1.28s/it]\u001b[A\n",
            "Iteration: 100% 498/500 [10:42<00:02,  1.30s/it]\u001b[A\n",
            "Iteration: 100% 499/500 [10:43<00:01,  1.28s/it]\u001b[A\n",
            "Iteration: 100% 500/500 [10:44<00:00,  1.29s/it]\n",
            "08/29/2025 13:08:31 - INFO - __main__ -   ***** Eval results *****\n",
            "08/29/2025 13:08:31 - INFO - __main__ -     T2 = 0.0\n",
            "08/29/2025 13:08:31 - INFO - __main__ -     eval_loss = 0.09202210213989019\n",
            "08/29/2025 13:08:31 - INFO - __main__ -     f1 = 0.2352606095129593\n",
            "08/29/2025 13:08:31 - INFO - __main__ -     global_step = 500\n",
            "08/29/2025 13:08:31 - INFO - __main__ -     loss = 0.04862210993841291\n",
            "Epoch:  10% 2/20 [24:08<3:37:28, 724.92s/it]\n",
            "Iteration:   0% 0/500 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/500 [00:01<10:27,  1.26s/it]\u001b[A\n",
            "Iteration:   0% 2/500 [00:02<10:37,  1.28s/it]\u001b[A\n",
            "Iteration:   1% 3/500 [00:03<10:21,  1.25s/it]\u001b[A\n",
            "Iteration:   1% 4/500 [00:05<10:35,  1.28s/it]\u001b[A\n",
            "Iteration:   1% 5/500 [00:06<10:22,  1.26s/it]\u001b[A\n",
            "Iteration:   1% 6/500 [00:07<10:28,  1.27s/it]\u001b[A\n",
            "Iteration:   1% 7/500 [00:08<10:21,  1.26s/it]\u001b[A\n",
            "Iteration:   2% 8/500 [00:10<10:31,  1.28s/it]\u001b[A\n",
            "Iteration:   2% 9/500 [00:11<10:21,  1.27s/it]\u001b[A\n",
            "Iteration:   2% 10/500 [00:12<10:28,  1.28s/it]\u001b[A\n",
            "Iteration:   2% 11/500 [00:13<10:22,  1.27s/it]\u001b[A\n",
            "Iteration:   2% 12/500 [00:15<10:29,  1.29s/it]\u001b[A\n",
            "Iteration:   3% 13/500 [00:16<10:22,  1.28s/it]\u001b[A\n",
            "Iteration:   3% 14/500 [00:17<10:33,  1.30s/it]\u001b[A\n",
            "Iteration:   3% 15/500 [00:19<10:24,  1.29s/it]\u001b[A\n",
            "Iteration:   3% 16/500 [00:20<10:28,  1.30s/it]\u001b[A\n",
            "Iteration:   3% 17/500 [00:21<10:21,  1.29s/it]\u001b[A\n",
            "Iteration:   4% 18/500 [00:23<10:30,  1.31s/it]\u001b[A\n",
            "Iteration:   4% 19/500 [00:24<10:24,  1.30s/it]\u001b[A\n",
            "Iteration:   4% 20/500 [00:25<10:32,  1.32s/it]\u001b[A\n",
            "Iteration:   4% 21/500 [00:27<10:27,  1.31s/it]\u001b[A\n",
            "Iteration:   4% 22/500 [00:28<10:32,  1.32s/it]\u001b[A\n",
            "Iteration:   5% 23/500 [00:29<10:25,  1.31s/it]\u001b[A\n",
            "Iteration:   5% 24/500 [00:31<10:33,  1.33s/it]\u001b[A\n",
            "Iteration:   5% 25/500 [00:32<10:25,  1.32s/it]\u001b[A\n",
            "Iteration:   5% 26/500 [00:33<10:34,  1.34s/it]\u001b[A\n",
            "Iteration:   5% 27/500 [00:35<10:22,  1.32s/it]\u001b[A\n",
            "Iteration:   6% 28/500 [00:36<10:25,  1.33s/it]\u001b[A\n",
            "Iteration:   6% 29/500 [00:37<10:17,  1.31s/it]\u001b[A\n",
            "Iteration:   6% 30/500 [00:38<10:23,  1.33s/it]\u001b[A\n",
            "Iteration:   6% 31/500 [00:40<10:14,  1.31s/it]\u001b[A\n",
            "Iteration:   6% 32/500 [00:41<10:24,  1.33s/it]\u001b[A\n",
            "Iteration:   7% 33/500 [00:42<10:13,  1.31s/it]\u001b[A\n",
            "Iteration:   7% 34/500 [00:44<10:16,  1.32s/it]\u001b[A\n",
            "Iteration:   7% 35/500 [00:45<10:08,  1.31s/it]\u001b[A\n",
            "Iteration:   7% 36/500 [00:46<10:11,  1.32s/it]\u001b[A\n",
            "Iteration:   7% 37/500 [00:48<10:00,  1.30s/it]\u001b[A\n",
            "Iteration:   8% 38/500 [00:49<10:05,  1.31s/it]\u001b[A\n",
            "Iteration:   8% 39/500 [00:50<09:55,  1.29s/it]\u001b[A\n",
            "Iteration:   8% 40/500 [00:52<10:00,  1.31s/it]\u001b[A\n",
            "Iteration:   8% 41/500 [00:53<09:52,  1.29s/it]\u001b[A\n",
            "Iteration:   8% 42/500 [00:54<09:58,  1.31s/it]\u001b[A\n",
            "Iteration:   9% 43/500 [00:55<09:49,  1.29s/it]\u001b[A\n",
            "Iteration:   9% 44/500 [00:57<09:51,  1.30s/it]\u001b[A\n",
            "Iteration:   9% 45/500 [00:58<09:44,  1.29s/it]\u001b[A\n",
            "Iteration:   9% 46/500 [00:59<09:50,  1.30s/it]\u001b[A\n",
            "Iteration:   9% 47/500 [01:01<09:40,  1.28s/it]\u001b[A\n",
            "Iteration:  10% 48/500 [01:02<09:45,  1.30s/it]\u001b[A\n",
            "Iteration:  10% 49/500 [01:03<09:37,  1.28s/it]\u001b[A\n",
            "Iteration:  10% 50/500 [01:04<09:41,  1.29s/it]\u001b[A\n",
            "Iteration:  10% 51/500 [01:06<09:31,  1.27s/it]\u001b[A\n",
            "Iteration:  10% 52/500 [01:07<09:44,  1.30s/it]\u001b[A\n",
            "Iteration:  11% 53/500 [01:08<09:31,  1.28s/it]\u001b[A\n",
            "Iteration:  11% 54/500 [01:10<09:35,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 55/500 [01:11<09:28,  1.28s/it]\u001b[A\n",
            "Iteration:  11% 56/500 [01:12<09:30,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 57/500 [01:13<09:22,  1.27s/it]\u001b[A\n",
            "Iteration:  12% 58/500 [01:15<09:24,  1.28s/it]\u001b[A\n",
            "Iteration:  12% 59/500 [01:16<09:17,  1.26s/it]\u001b[A\n",
            "Iteration:  12% 60/500 [01:17<09:29,  1.29s/it]\u001b[A\n",
            "Iteration:  12% 61/500 [01:18<09:19,  1.27s/it]\u001b[A\n",
            "Iteration:  12% 62/500 [01:20<09:21,  1.28s/it]\u001b[A\n",
            "Iteration:  13% 63/500 [01:21<09:13,  1.27s/it]\u001b[A\n",
            "Iteration:  13% 64/500 [01:22<09:20,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 65/500 [01:24<09:12,  1.27s/it]\u001b[A\n",
            "Iteration:  13% 66/500 [01:25<09:20,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 67/500 [01:26<09:13,  1.28s/it]\u001b[A\n",
            "Iteration:  14% 68/500 [01:27<09:14,  1.28s/it]\u001b[A\n",
            "Iteration:  14% 69/500 [01:29<09:08,  1.27s/it]\u001b[A\n",
            "Iteration:  14% 70/500 [01:30<09:15,  1.29s/it]\u001b[A\n",
            "Iteration:  14% 71/500 [01:31<09:05,  1.27s/it]\u001b[A\n",
            "Iteration:  14% 72/500 [01:33<09:10,  1.29s/it]\u001b[A\n",
            "Iteration:  15% 73/500 [01:34<09:04,  1.28s/it]\u001b[A\n",
            "Iteration:  15% 74/500 [01:35<09:09,  1.29s/it]\u001b[A\n",
            "Iteration:  15% 75/500 [01:36<09:01,  1.27s/it]\u001b[A\n",
            "Iteration:  15% 76/500 [01:38<09:06,  1.29s/it]\u001b[A\n",
            "Iteration:  15% 77/500 [01:39<09:00,  1.28s/it]\u001b[A\n",
            "Iteration:  16% 78/500 [01:40<09:04,  1.29s/it]\u001b[A\n",
            "Iteration:  16% 79/500 [01:42<08:57,  1.28s/it]\u001b[A\n",
            "Iteration:  16% 80/500 [01:43<09:10,  1.31s/it]\u001b[A\n",
            "Iteration:  16% 81/500 [01:44<09:00,  1.29s/it]\u001b[A\n",
            "Iteration:  16% 82/500 [01:46<09:05,  1.30s/it]\u001b[A\n",
            "Iteration:  17% 83/500 [01:47<08:57,  1.29s/it]\u001b[A\n",
            "Iteration:  17% 84/500 [01:48<09:02,  1.30s/it]\u001b[A\n",
            "Iteration:  17% 85/500 [01:49<08:53,  1.29s/it]\u001b[A"
          ]
        }
      ]
    }
  ]
}