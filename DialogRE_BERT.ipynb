{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOW7Vc8xfNO2R9LkFp0UDd6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UBGidado/My_Research/blob/Notebooks/DialogRE_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA3YiokDnYKy",
        "outputId": "07ea2c72-1a45-47e8-c6ef-4ddb3779187d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets spacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Dof3Vo0kTWv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import re\n",
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "import pickle\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiUcFvS8pp5w"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jvf3nlnnFu",
        "outputId": "d6d9f97d-96a9-4d19-f95c-761535162007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configure logger\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# File paths\n",
        "data_path = \"/content/drive/MyDrive/My Research/dialogre/data_v2/en/data\"\n",
        "\n",
        "train_file = f\"{data_path}/train.json\"\n",
        "dev_file = f\"{data_path}/dev.json\"\n",
        "test_file = f\"{data_path}/test.json\"\n",
        "\n",
        "# Load from JSON data\n",
        "def load_dialogre_data(file_path):\n",
        "    \"\"\"\n",
        "    Load DialogRE data from the official JSON format and convert it\n",
        "    into a standard dictionary structure for processing.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    processed_conversations = []\n",
        "\n",
        "    for conv_idx, conversation in enumerate(raw_data):\n",
        "        # Each conversation is a list: [dialogue_turns, relation_instances]\n",
        "        if len(conversation) != 2:\n",
        "            logger.warning(\n",
        "                f\"Conversation {conv_idx} has unexpected structure: {len(conversation)} elements\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        dialogue_turns = conversation[0]\n",
        "        relation_instances = conversation[1]\n",
        "\n",
        "        # Convert dialogue turns to standard format\n",
        "        formatted_dialogue = []\n",
        "        for turn_idx, turn in enumerate(dialogue_turns):\n",
        "            if isinstance(turn, dict):\n",
        "                formatted_dialogue.append(turn)\n",
        "            else:\n",
        "                # Handle \"Speaker: Text\" format\n",
        "                if isinstance(turn, str) and ':' in turn:\n",
        "                    speaker, text = turn.split(':', 1)\n",
        "                    formatted_dialogue.append({\n",
        "                        'speaker': speaker.strip(),\n",
        "                        'text': text.strip(),\n",
        "                        'turn': turn_idx\n",
        "                    })\n",
        "                else:\n",
        "                    formatted_dialogue.append({\n",
        "                        'speaker': f\"Speaker_{turn_idx}\",\n",
        "                        'text': str(turn),\n",
        "                        'turn': turn_idx\n",
        "                    })\n",
        "\n",
        "        # Create conversation dictionary\n",
        "        conv_dict = {\n",
        "            'dialogue': formatted_dialogue,\n",
        "            'relations': relation_instances,\n",
        "            'id': f\"conv_{conv_idx}\",\n",
        "            'x': [],  # Placeholder: can be populated later\n",
        "            'y': []   # Placeholder: can be populated later\n",
        "        }\n",
        "\n",
        "        processed_conversations.append(conv_dict)\n",
        "\n",
        "    logger.info(f\"Loaded {len(processed_conversations)} conversations from {file_path}\")\n",
        "    return processed_conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HnGs9Y_wpyFJ"
      },
      "outputs": [],
      "source": [
        "train_data = load_dialogre_data(train_file)\n",
        "dev_data = load_dialogre_data(dev_file)\n",
        "test_data = load_dialogre_data(test_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUj8sqeEqWd4"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB-pHMyRqDmn",
        "outputId": "760dcf9f-da35-45e9-a4ad-2fcd67f7bc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Libraries imported and tokenizer initialized\n",
            "Tokenizer vocabulary size: 30527\n"
          ]
        }
      ],
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define special tokens for entity marking\n",
        "SPECIAL_TOKENS = {\n",
        "    'E1_START': '[E1]',\n",
        "    'E1_END': '[/E1]',\n",
        "    'E2_START': '[E2]',\n",
        "    'E2_END': '[/E2]',\n",
        "    'SPEAKER_SEP': '[SPEAKER]'\n",
        "}\n",
        "\n",
        "# Add special tokens to tokenizer\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': list(SPECIAL_TOKENS.values())})\n",
        "\n",
        "print(\"✓ Libraries imported and tokenizer initialized\")\n",
        "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA08Dfv4q-4H"
      },
      "source": [
        "### Entity and Speaker Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngqr-ranq2ss",
        "outputId": "ef3490f0-b4af-4d1a-f4b4-1079b1676715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "ner_model_name = \"dslim/bert-base-NER\"  # High-accuracy model for Person extraction\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
        "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# Load spaCy model for coreference resolution\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTr7gF13OYy"
      },
      "source": [
        "#### Entity Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jrwQeDd63Bax"
      },
      "outputs": [],
      "source": [
        "def extract_entities_with_advanced_ner(conversation: Dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extract PERSON entities with their positions using transformer NER model.\n",
        "    Returns list of entities with text, positions, speaker, and confidence.\n",
        "    \"\"\"\n",
        "    all_entities = []\n",
        "\n",
        "    for turn_idx, utterance in enumerate(conversation['dialogue']):\n",
        "        text = utterance['text']\n",
        "        speaker = utterance['speaker']\n",
        "\n",
        "        # Use NER pipeline to process utterance text\n",
        "        try:\n",
        "            ner_results = ner_pipeline(text)\n",
        "            # Filter for only PERSON entities with confidence > 0.8\n",
        "            person_entities = [entity for entity in ner_results\n",
        "                             if entity['entity_group'] == 'PER' and entity['score'] > 0.8]\n",
        "\n",
        "            for entity in person_entities:\n",
        "                all_entities.append({\n",
        "                    'text': entity['word'].strip(),\n",
        "                    'start': entity['start'],\n",
        "                    'end': entity['end'],\n",
        "                    'speaker': speaker,\n",
        "                    'utterance_idx': turn_idx,\n",
        "                    'confidence': round(entity['score'], 4),\n",
        "                    'source_utterance': text\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing utterance: {text}\\nError: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "def extract_entities_with_spacy(conversation: Dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extract entities using spaCy's built-in NER and rule-based matching.\n",
        "    This helps capture additional entity mentions and provides linguistic features.\n",
        "    \"\"\"\n",
        "    spacy_entities = []\n",
        "\n",
        "    for turn_idx, utterance in enumerate(conversation['dialogue']):\n",
        "        text = utterance['text']\n",
        "        speaker = utterance['speaker']\n",
        "\n",
        "        # Process with spaCy\n",
        "        doc = nlp(text)\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG']:  # Include organizations as they can be social entities\n",
        "                spacy_entities.append({\n",
        "                    'text': ent.text.strip(),\n",
        "                    'start': ent.start_char,\n",
        "                    'end': ent.end_char,\n",
        "                    'speaker': speaker,\n",
        "                    'utterance_idx': turn_idx,\n",
        "                    'confidence': 0.9,  # spaCy doesn't provide confidence, use default high value\n",
        "                    'source_utterance': text,\n",
        "                    'label': ent.label_\n",
        "                })\n",
        "\n",
        "    return spacy_entities\n",
        "\n",
        "def resolve_entities_spacy(conversation: Dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Use spaCy's processing to get additional linguistic information and\n",
        "    basic coreference through noun chunks and dependency parsing.\n",
        "    \"\"\"\n",
        "    entity_info = []\n",
        "\n",
        "    # Process each utterance with spaCy\n",
        "    for turn_idx, utterance in enumerate(conversation['dialogue']):\n",
        "        text = utterance['text']\n",
        "        speaker = utterance['speaker']\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Extract noun chunks that might refer to entities\n",
        "        for chunk in doc.nouns:\n",
        "            if any(token.ent_type_ in ['PERSON', 'ORG'] for token in chunk):\n",
        "                entity_info.append({\n",
        "                    'text': chunk.text,\n",
        "                    'speaker': speaker,\n",
        "                    'utterance_idx': turn_idx,\n",
        "                    'type': 'noun_chunk',\n",
        "                    'root_text': chunk.root.text\n",
        "                })\n",
        "\n",
        "    return entity_info\n",
        "\n",
        "def canonicalize_entities(raw_entities: List[Dict], spacy_entities: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create canonical entity mapping using combined NER results from both models.\n",
        "    \"\"\"\n",
        "    canonical_entities = {}\n",
        "    mention_to_canonical = {}\n",
        "\n",
        "    # Combine entities from both sources\n",
        "    all_entities = raw_entities + spacy_entities\n",
        "\n",
        "    # First pass: use exact string matching for canonicalization\n",
        "    for entity in all_entities:\n",
        "        entity_text = entity['text']\n",
        "\n",
        "        # Check if this is a new entity or a mention of an existing one\n",
        "        found_canonical = None\n",
        "        for canonical_name in canonical_entities:\n",
        "            # Simple matching: exact match or contained within\n",
        "            if (entity_text.lower() == canonical_name.lower() or\n",
        "                entity_text.lower() in canonical_name.lower() or\n",
        "                canonical_name.lower() in entity_text.lower()):\n",
        "                found_canonical = canonical_name\n",
        "                break\n",
        "\n",
        "        if found_canonical:\n",
        "            canonical_name = found_canonical\n",
        "        else:\n",
        "            canonical_name = entity_text\n",
        "            canonical_entities[canonical_name] = {\n",
        "                'mentions': [],\n",
        "                'source_utterances': [],\n",
        "                'speakers': set(),\n",
        "                'confidence_scores': [],\n",
        "                'types': set()\n",
        "            }\n",
        "\n",
        "        # Add this mention to the canonical entity\n",
        "        canonical_entities[canonical_name]['mentions'].append({\n",
        "            'text': entity_text,\n",
        "            'utterance_idx': entity['utterance_idx'],\n",
        "            'speaker': entity['speaker'],\n",
        "            'confidence': entity.get('confidence', 0.8)\n",
        "        })\n",
        "        canonical_entities[canonical_name]['source_utterances'].append(entity['source_utterance'])\n",
        "        canonical_entities[canonical_name]['speakers'].add(entity['speaker'])\n",
        "        canonical_entities[canonical_name]['confidence_scores'].append(entity.get('confidence', 0.8))\n",
        "        if 'label' in entity:\n",
        "            canonical_entities[canonical_name]['types'].add(entity['label'])\n",
        "\n",
        "        mention_to_canonical[entity_text] = canonical_name\n",
        "\n",
        "    return {\n",
        "        'canonical_entities': canonical_entities,\n",
        "        'mention_to_canonical_map': mention_to_canonical\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjwe3VQ3nYu"
      },
      "source": [
        "#### Speaker Processing and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SYxkz2ZL31Ky"
      },
      "outputs": [],
      "source": [
        "def process_speaker_information(conversation: Dict, canonical_entities: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process speaker information and analyze speaker-entity relationships.\n",
        "    \"\"\"\n",
        "    speakers = set()\n",
        "    speaker_utterances = defaultdict(list)\n",
        "    speaker_entity_mentions = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for turn_idx, utterance in enumerate(conversation['dialogue']):\n",
        "        speaker = utterance['speaker']\n",
        "        text = utterance['text']\n",
        "        speakers.add(speaker)\n",
        "\n",
        "        # Record utterance\n",
        "        speaker_utterances[speaker].append({\n",
        "            'turn': turn_idx,\n",
        "            'text': text,\n",
        "            'length': len(text.split()),\n",
        "            'has_question': '?' in text\n",
        "        })\n",
        "\n",
        "        # Count entity mentions per speaker using our canonical entities\n",
        "        for canonical_name in canonical_entities:\n",
        "            # Simple string matching for mention detection\n",
        "            if canonical_name.lower() in text.lower():\n",
        "                speaker_entity_mentions[speaker][canonical_name] += 1\n",
        "\n",
        "    return {\n",
        "        'speakers': list(speakers),\n",
        "        'speaker_utterances': dict(speaker_utterances),\n",
        "        'speaker_entity_mentions': dict(speaker_entity_mentions),\n",
        "        'num_speakers': len(speakers),\n",
        "        'conversation_length': len(conversation['dialogue'])\n",
        "    }\n",
        "\n",
        "# Cell 5: Master Processing Function\n",
        "def process_conversation_entities(conversation: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Master function to process all entities and speakers in a conversation.\n",
        "    \"\"\"\n",
        "    # Extract entities using both transformer NER and spaCy\n",
        "    raw_entities = extract_entities_with_advanced_ner(conversation)\n",
        "    spacy_entities = extract_entities_with_spacy(conversation)\n",
        "    spacy_linguistic = resolve_entities_spacy(conversation)\n",
        "\n",
        "    # Canonicalize entities\n",
        "    entity_data = canonicalize_entities(raw_entities, spacy_entities)\n",
        "\n",
        "    # Process speaker information\n",
        "    speaker_data = process_speaker_information(conversation, entity_data['canonical_entities'])\n",
        "\n",
        "    # Compile comprehensive results\n",
        "    return {\n",
        "        'conversation_id': conversation.get('id', 'unknown'),\n",
        "        'raw_ner_entities': raw_entities,\n",
        "        'spacy_entities': spacy_entities,\n",
        "        'spacy_linguistic_info': spacy_linguistic,\n",
        "        **entity_data,\n",
        "        **speaker_data,\n",
        "        'processing_stats': {\n",
        "            'bert_entities_count': len(raw_entities),\n",
        "            'spacy_entities_count': len(spacy_entities),\n",
        "            'canonical_entities_count': len(entity_data['canonical_entities']),\n",
        "            'speakers_count': speaker_data['num_speakers']\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Cell 6: Example Usage and Validation\n",
        "def validate_processing(dialogre_data: List[Dict], sample_index: int = 0):\n",
        "    \"\"\"\n",
        "    Validate the processing pipeline on a sample conversation.\n",
        "    \"\"\"\n",
        "    sample_conversation = dialogre_data[sample_index]\n",
        "    print(\"=== Processing Sample Conversation ===\")\n",
        "    print(f\"Conversation ID: {sample_conversation.get('id', 'unknown')}\")\n",
        "    print(f\"Number of utterances: {len(sample_conversation['dialogue'])}\")\n",
        "\n",
        "    # Process the conversation\n",
        "    results = process_conversation_entities(sample_conversation)\n",
        "\n",
        "    print(f\"\\n=== Results ===\")\n",
        "    print(f\"BERT entities: {len(results['raw_ner_entities'])}\")\n",
        "    print(f\"spaCy entities: {len(results['spacy_entities'])}\")\n",
        "    print(f\"Canonical Entities: {len(results['canonical_entities'])}\")\n",
        "\n",
        "    print(f\"\\nTop entities:\")\n",
        "    for i, (entity, info) in enumerate(list(results['canonical_entities'].items())[:5]):\n",
        "        print(f\"  {i+1}. {entity}: {len(info['mentions'])} mentions\")\n",
        "\n",
        "    print(f\"\\nSpeakers: {results['speakers']}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNN-TDNA4OXb"
      },
      "source": [
        "##  Relation Extraction and Entity Pair Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9RgFwzhx4O6N"
      },
      "outputs": [],
      "source": [
        "def extract_relation_data(conversation: Dict) -> List[Dict]:\n",
        "    \"\"\"Extract relation information from conversation\"\"\"\n",
        "    relations = []\n",
        "\n",
        "    for rel_instance in conversation.get('relations', []):\n",
        "        x_entity = rel_instance.get('x', '')\n",
        "        y_entity = rel_instance.get('y', '')\n",
        "        relation_types = rel_instance.get('r', [])\n",
        "\n",
        "        for relation_type in relation_types:\n",
        "            relations.append({\n",
        "                'subject': x_entity,\n",
        "                'object': y_entity,\n",
        "                'relation': relation_type,\n",
        "                'conversation_id': conversation.get('id', 'unknown')\n",
        "            })\n",
        "\n",
        "    return relations\n",
        "\n",
        "def create_entity_pairs(conversation: Dict) -> List[Dict]:\n",
        "    \"\"\"Create all possible entity pairs for relation classification\"\"\"\n",
        "    entity_pairs = []\n",
        "\n",
        "    # Extract all unique entities from relation instances\n",
        "    all_entities = set()\n",
        "    entity_idx_map = {}  # Map entity name to index\n",
        "\n",
        "    for rel_instance in conversation.get('relations', []):\n",
        "        x_entity = rel_instance.get('x', '')\n",
        "        y_entity = rel_instance.get('y', '')\n",
        "\n",
        "        if x_entity and x_entity not in entity_idx_map:\n",
        "            entity_idx_map[x_entity] = len(entity_idx_map)\n",
        "        if y_entity and y_entity not in entity_idx_map:\n",
        "            entity_idx_map[y_entity] = len(entity_idx_map)\n",
        "\n",
        "    # Create conversation entities lists\n",
        "    conversation['x'] = list(entity_idx_map.keys())\n",
        "    conversation['y'] = list(entity_idx_map.keys())\n",
        "\n",
        "    # Get actual relations for positive examples\n",
        "    actual_relations = {}\n",
        "    for rel_instance in conversation.get('relations', []):\n",
        "        x_entity = rel_instance.get('x', '')\n",
        "        y_entity = rel_instance.get('y', '')\n",
        "        relation_types = rel_instance.get('r', [])\n",
        "\n",
        "        if x_entity in entity_idx_map and y_entity in entity_idx_map:\n",
        "            x_idx = entity_idx_map[x_entity]\n",
        "            y_idx = entity_idx_map[y_entity]\n",
        "\n",
        "            # For simplicity, take the first relation type if multiple exist\n",
        "            if relation_types:\n",
        "                actual_relations[(x_idx, y_idx)] = relation_types[0]\n",
        "\n",
        "    # Create pairs between all entities\n",
        "    entity_names = list(entity_idx_map.keys())\n",
        "    for i, entity_x in enumerate(entity_names):\n",
        "        for j, entity_y in enumerate(entity_names):\n",
        "            if i == j:\n",
        "                continue  # Skip self-relations if desired\n",
        "\n",
        "            pair_key = (i, j)\n",
        "            relation_type = actual_relations.get(pair_key, 'no_relation')\n",
        "\n",
        "            entity_pairs.append({\n",
        "                'subject': entity_x,\n",
        "                'object': entity_y,\n",
        "                'relation': relation_type,\n",
        "                'subject_idx': i,\n",
        "                'object_idx': j,\n",
        "                'is_positive': relation_type != 'no_relation',\n",
        "                'conversation_id': conversation.get('id', 'unknown')\n",
        "            })\n",
        "\n",
        "    return entity_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc1xDHzc_SPh"
      },
      "source": [
        "### Text Processing and Entity Marking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3arHN747_Omu"
      },
      "outputs": [],
      "source": [
        "def mark_entities_in_text(text: str, subject: str, object: str,\n",
        "                         subject_pos: Tuple[int, int] = None,\n",
        "                         object_pos: Tuple[int, int] = None) -> str:\n",
        "    \"\"\"Mark entities in text with special tokens\"\"\"\n",
        "\n",
        "    if subject_pos and object_pos:\n",
        "        # Use provided positions\n",
        "        s_start, s_end = subject_pos\n",
        "        o_start, o_end = object_pos\n",
        "\n",
        "        # Sort positions to handle overlapping cases\n",
        "        positions = [(s_start, s_end, 'subject'), (o_start, o_end, 'object')]\n",
        "        positions.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Apply marking from right to left to preserve positions\n",
        "        marked_text = text\n",
        "        for start, end, entity_type in reversed(positions):\n",
        "            if entity_type == 'subject':\n",
        "                marked_text = (marked_text[:start] +\n",
        "                             SPECIAL_TOKENS['E1_START'] + marked_text[start:end] +\n",
        "                             SPECIAL_TOKENS['E1_END'] + marked_text[end:])\n",
        "            else:\n",
        "                marked_text = (marked_text[:start] +\n",
        "                             SPECIAL_TOKENS['E2_START'] + marked_text[start:end] +\n",
        "                             SPECIAL_TOKENS['E2_END'] + marked_text[end:])\n",
        "    else:\n",
        "        # Simple string replacement (fallback)\n",
        "        marked_text = text.replace(subject, f\"{SPECIAL_TOKENS['E1_START']}{subject}{SPECIAL_TOKENS['E1_END']}\")\n",
        "        marked_text = marked_text.replace(object, f\"{SPECIAL_TOKENS['E2_START']}{object}{SPECIAL_TOKENS['E2_END']}\")\n",
        "\n",
        "    return marked_text\n",
        "\n",
        "def process_conversation_text(conversation: Dict, entity_pair: Dict) -> str:\n",
        "    \"\"\"Process conversation text and mark entities\"\"\"\n",
        "    dialogue_texts = []\n",
        "    subject = entity_pair['subject']\n",
        "    object = entity_pair['object']\n",
        "\n",
        "    # Handle both string and dictionary dialogue turns\n",
        "    for i, turn in enumerate(conversation['dialogue']):\n",
        "        if isinstance(turn, dict):\n",
        "            speaker = turn.get('speaker', 'Unknown')\n",
        "            text = turn.get('text', '')\n",
        "        else:\n",
        "            # Assume it's a string or needs parsing\n",
        "            speaker = f\"Speaker_{i}\"\n",
        "            text = str(turn)\n",
        "\n",
        "        # Mark entities in this utterance\n",
        "        marked_text = mark_entities_in_text(text, subject, object)\n",
        "\n",
        "        # Add speaker information\n",
        "        processed_text = f\"{SPECIAL_TOKENS['SPEAKER_SEP']} {speaker}: {marked_text}\"\n",
        "        dialogue_texts.append(processed_text)\n",
        "\n",
        "    # Join all utterances\n",
        "    full_text = \" \".join(dialogue_texts)\n",
        "\n",
        "    return full_text\n",
        "\n",
        "def prepare_bert_input(text: str, max_length: int = 512) -> Dict:\n",
        "    \"\"\"Prepare BERT input from marked text\"\"\"\n",
        "    # Tokenize the text with special tokens\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Find positions of entity markers\n",
        "    input_ids = encoding['input_ids'][0].tolist()\n",
        "    e1_start = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['E1_START'])\n",
        "    e1_end = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['E1_END'])\n",
        "    e2_start = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['E2_START'])\n",
        "    e2_end = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['E2_END'])\n",
        "\n",
        "    # Find entity positions\n",
        "    entity_positions = {\n",
        "        'e1_start': input_ids.index(e1_start) if e1_start in input_ids else -1,\n",
        "        'e1_end': input_ids.index(e1_end) if e1_end in input_ids else -1,\n",
        "        'e2_start': input_ids.index(e2_start) if e2_start in input_ids else -1,\n",
        "        'e2_end': input_ids.index(e2_end) if e2_end in input_ids else -1,\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'][0],\n",
        "        'attention_mask': encoding['attention_mask'][0],\n",
        "        'token_type_ids': encoding['token_type_ids'][0],\n",
        "        'entity_positions': entity_positions\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahKTNkz_BKC_"
      },
      "source": [
        "###  Main Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I8mDaR32A7GN"
      },
      "outputs": [],
      "source": [
        "def preprocess_dialogre_dataset_simple(train_data: List[Dict], dev_data: List[Dict], test_data: List[Dict], output_dir: str) -> Dict[str, Any]:\n",
        "    \"\"\"Simplified preprocessing pipeline focusing on BERT baseline without augmentation.\"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    logger.info(\"Starting simplified preprocessing (no augmentation)...\")\n",
        "\n",
        "    # Process each dataset separately\n",
        "    datasets = {\n",
        "        'train': train_data,\n",
        "        'dev': dev_data,\n",
        "        'test': test_data\n",
        "    }\n",
        "\n",
        "    all_processed_samples = []\n",
        "    relation_labels = set()\n",
        "\n",
        "    for dataset_name, data in datasets.items():\n",
        "        logger.info(f\"Processing {dataset_name} set with {len(data)} conversations...\")\n",
        "\n",
        "        dataset_samples = []\n",
        "        for conv_idx, conversation in enumerate(data):\n",
        "            try:\n",
        "                # Extract entity pairs for this conversation\n",
        "                entity_pairs = create_entity_pairs(conversation)\n",
        "\n",
        "                for pair in entity_pairs:\n",
        "                    # Process conversation text\n",
        "                    processed_text = process_conversation_text(conversation, pair)\n",
        "\n",
        "                    # Prepare BERT input\n",
        "                    bert_input = prepare_bert_input(processed_text)\n",
        "\n",
        "                    # Create processed sample\n",
        "                    sample = {\n",
        "                        'conversation_id': conversation.get('id', f'unknown_{conv_idx}'),\n",
        "                        'dataset': dataset_name,\n",
        "                        'subject': pair['subject'],\n",
        "                        'object': pair['object'],\n",
        "                        'relation': pair['relation'],\n",
        "                        'is_positive': pair['is_positive'],\n",
        "                        'processed_text': processed_text,\n",
        "                        'input_ids': bert_input['input_ids'],\n",
        "                        'attention_mask': bert_input['attention_mask'],\n",
        "                        'token_type_ids': bert_input['token_type_ids'],\n",
        "                        'entity_positions': bert_input['entity_positions']\n",
        "                    }\n",
        "\n",
        "                    dataset_samples.append(sample)\n",
        "                    relation_labels.add(pair['relation'])\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing conversation {conv_idx} in {dataset_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save this dataset split\n",
        "        dataset_file = os.path.join(output_dir, f'{dataset_name}_samples.pkl')\n",
        "        with open(dataset_file, 'wb') as f:\n",
        "            pickle.dump(dataset_samples, f)\n",
        "\n",
        "        logger.info(f\"Saved {len(dataset_samples)} samples for {dataset_name}\")\n",
        "        all_processed_samples.extend(dataset_samples)\n",
        "\n",
        "    # Create label mapping\n",
        "    relation_to_id = {rel: idx for idx, rel in enumerate(sorted(relation_labels))}\n",
        "    id_to_relation = {idx: rel for rel, idx in relation_to_id.items()}\n",
        "\n",
        "    # Add label IDs to all samples\n",
        "    for sample in all_processed_samples:\n",
        "        sample['label_id'] = relation_to_id[sample['relation']]\n",
        "\n",
        "    # Save combined data (without augmentation)\n",
        "    output_data = {\n",
        "        'samples': all_processed_samples,\n",
        "        'relation_to_id': relation_to_id,\n",
        "        'id_to_relation': id_to_relation,\n",
        "        'num_relations': len(relation_labels),\n",
        "        'num_samples': len(all_processed_samples)\n",
        "    }\n",
        "\n",
        "    output_file = os.path.join(output_dir, 'bert_baseline_data.pkl')\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(output_data, f)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'num_relations': len(relation_labels),\n",
        "        'num_samples': len(all_processed_samples),\n",
        "        'relation_labels': list(relation_labels),\n",
        "        'relation_to_id': relation_to_id,\n",
        "        'augmentation_applied': False\n",
        "    }\n",
        "\n",
        "    metadata_file = os.path.join(output_dir, 'metadata.json')\n",
        "    with open(metadata_file, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    logger.info(f\"BERT baseline preprocessing completed! Total samples: {len(all_processed_samples)}\")\n",
        "    return output_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z1HBjKLBcW3"
      },
      "source": [
        "### Train/Validation/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s5RV-EOaBMiN"
      },
      "outputs": [],
      "source": [
        "def create_train_val_test_split_simple(processed_data: Dict[str, Any],\n",
        "                                      output_dir: str,\n",
        "                                      train_ratio: float = 0.7,\n",
        "                                      val_ratio: float = 0.15,\n",
        "                                      test_ratio: float = 0.15) -> None:\n",
        "    \"\"\"Create simple train/validation/test splits without stratification\"\"\"\n",
        "\n",
        "    samples = processed_data['samples']\n",
        "\n",
        "    # Simple random split without stratification\n",
        "    train_samples, temp_samples = train_test_split(\n",
        "        samples,\n",
        "        test_size=(val_ratio + test_ratio),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    val_samples, test_samples = train_test_split(\n",
        "        temp_samples,\n",
        "        test_size=test_ratio/(val_ratio + test_ratio),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Save splits (same as above)\n",
        "    splits = {\n",
        "        'train': train_samples,\n",
        "        'validation': val_samples,\n",
        "        'test': test_samples\n",
        "    }\n",
        "\n",
        "    for split_name, split_data in splits.items():\n",
        "        split_file = os.path.join(output_dir, f'{split_name}_split.pkl')\n",
        "        with open(split_file, 'wb') as f:\n",
        "            pickle.dump(split_data, f)\n",
        "\n",
        "        print(f\"{split_name} split: {len(split_data)} samples\")\n",
        "\n",
        "    print(\"Data splits created successfully (non-stratified)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "equT672_NMPA",
        "outputId": "dedd9fd8-a8ae-4bfd-8aca-78b41c73c6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train split: 34294 samples\n",
            "validation split: 7349 samples\n",
            "test split: 7349 samples\n",
            "Data splits created successfully (non-stratified)!\n",
            "\n",
            "Preprocessing Summary:\n",
            "- Total samples: 48992\n",
            "- Number of relation types: 38\n",
            "- Output directory: processed_data\n"
          ]
        }
      ],
      "source": [
        "# DATA_PATH = train_data  # Update this - this variable is no longer needed as we pass the data directly\n",
        "OUTPUT_DIR = \"processed_data\"\n",
        "\n",
        "# Process the data\n",
        "processed_data = preprocess_dialogre_dataset_simple(\n",
        "    train_data=train_data,\n",
        "    dev_data=dev_data,\n",
        "    test_data=test_data,\n",
        "    output_dir=OUTPUT_DIR\n",
        ")\n",
        "\n",
        "# Create splits - USE THE FIXED VERSION\n",
        "create_train_val_test_split_simple(processed_data, OUTPUT_DIR)\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\nPreprocessing Summary:\")\n",
        "print(f\"- Total samples: {processed_data['num_samples']}\")\n",
        "print(f\"- Number of relation types: {processed_data['num_relations']}\")\n",
        "print(f\"- Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Check the rare classes file to see what was removed\n",
        "rare_classes_file = os.path.join(OUTPUT_DIR, 'rare_classes_info.json')\n",
        "if os.path.exists(rare_classes_file):\n",
        "    with open(rare_classes_file, 'r') as f:\n",
        "        rare_info = json.load(f)\n",
        "    print(f\"- Removed {len(rare_info['removed_relations'])} rare relation types\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAe6-8Z2cf5Y"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KYvyF-S1ZyE7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertPreTrainedModel\n",
        "\n",
        "class BertForRelationExtraction(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for relation extraction with entity attention - Optimized version\"\"\"\n",
        "\n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # Classification layer\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size * 2, num_labels)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                e1_positions=None, e2_positions=None, labels=None):\n",
        "\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Get representations for the two entities (vectorized approach)\n",
        "        e1_reps = self._get_entity_representation_vectorized(sequence_output, e1_positions)\n",
        "        e2_reps = self._get_entity_representation_vectorized(sequence_output, e2_positions)\n",
        "\n",
        "        # Concatenate entity representations\n",
        "        combined_reps = torch.cat([e1_reps, e2_reps], dim=-1)\n",
        "        combined_reps = self.dropout(combined_reps)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(combined_reps)\n",
        "\n",
        "        # Calculate loss if labels are provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "    def _get_entity_representation_vectorized(self, sequence_output, entity_positions):\n",
        "        \"\"\"Vectorized version of entity representation extraction\"\"\"\n",
        "        batch_size, seq_len, hidden_size = sequence_output.shape\n",
        "        device = sequence_output.device\n",
        "\n",
        "        # Extract start positions for all samples at once\n",
        "        start_positions = entity_positions[:, 0]  # [batch_size]\n",
        "\n",
        "        # Use advanced indexing to get start token representations\n",
        "        batch_indices = torch.arange(batch_size, device=device)\n",
        "\n",
        "        # Clamp positions to valid range\n",
        "        start_positions = torch.clamp(start_positions, 0, seq_len - 1)\n",
        "\n",
        "        # Get representations at start positions\n",
        "        entity_reps = sequence_output[batch_indices, start_positions]  # [batch_size, hidden_size]\n",
        "\n",
        "        return entity_reps\n",
        "\n",
        "    def _get_entity_representation(self, sequence_output, entity_positions):\n",
        "        \"\"\"Fallback method using loops (same as above but kept for reference)\"\"\"\n",
        "        batch_size, seq_len, hidden_size = sequence_output.shape\n",
        "        device = sequence_output.device\n",
        "\n",
        "        entity_reps = torch.zeros(batch_size, hidden_size, device=device, dtype=sequence_output.dtype)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            start_pos = entity_positions[i, 0].item()\n",
        "            end_pos = entity_positions[i, 1].item()\n",
        "\n",
        "            start_pos = max(0, min(start_pos, seq_len - 1))\n",
        "            end_pos = max(start_pos + 1, min(end_pos, seq_len))\n",
        "\n",
        "            entity_span = sequence_output[i, start_pos:end_pos]\n",
        "            entity_rep = entity_span.mean(dim=0)\n",
        "            entity_reps[i] = entity_rep\n",
        "\n",
        "        return entity_reps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pHBBQwYaFuy"
      },
      "source": [
        "### Create DataLoaders for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sZNs39OiZz0C"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class RelationDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for relation extraction\"\"\"\n",
        "\n",
        "    def __init__(self, samples, tokenizer, max_length=128):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Convert to tensors\n",
        "        return {\n",
        "            'input_ids': torch.tensor(sample['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(sample['attention_mask'], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(sample['token_type_ids'], dtype=torch.long),\n",
        "            'e1_positions': torch.tensor([sample['entity_positions']['e1_start'],\n",
        "                                        sample['entity_positions']['e1_end']], dtype=torch.long),\n",
        "            'e2_positions': torch.tensor([sample['entity_positions']['e2_start'],\n",
        "                                        sample['entity_positions']['e2_end']], dtype=torch.long),\n",
        "            'labels': torch.tensor(sample['label_id'], dtype=torch.long),\n",
        "            'relation': sample['relation'],\n",
        "            'is_positive': sample['is_positive']\n",
        "        }\n",
        "\n",
        "def create_data_loaders(train_samples, val_samples, test_samples, batch_size=32):\n",
        "    \"\"\"Create DataLoader objects for each split\"\"\"\n",
        "\n",
        "    train_dataset = RelationDataset(train_samples, tokenizer)\n",
        "    val_dataset = RelationDataset(val_samples, tokenizer)\n",
        "    test_dataset = RelationDataset(test_samples, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvA4Vreoagc8"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xZponP4maZyA"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import BertConfig\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def initialize_bert_model(num_labels, tokenizer):\n",
        "    \"\"\"Properly initialize BERT model for relation extraction\"\"\"\n",
        "\n",
        "    # Load configuration\n",
        "    config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "    config.num_labels = num_labels\n",
        "\n",
        "    # Initialize model\n",
        "    model = BertForRelationExtraction.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        config=config,\n",
        "        num_labels=num_labels,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # Resize token embeddings AFTER model initialization\n",
        "    model.bert.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Move to device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Verify model is properly initialized\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.device.type == 'meta':\n",
        "            raise RuntimeError(f\"Parameter {name} is still on meta device after initialization!\")\n",
        "\n",
        "    print(f\"Model initialized successfully on {device}\")\n",
        "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "    return model, device\n",
        "\n",
        "\n",
        "# Updated training function\n",
        "def train_bert_model_fixed(train_loader, val_loader, num_labels, tokenizer, learning_rate=2e-5, num_epochs=5):\n",
        "    \"\"\"Fixed training function that properly handles model initialization\"\"\"\n",
        "\n",
        "    # Initialize model properly\n",
        "    model, device = initialize_bert_model(num_labels, tokenizer)\n",
        "\n",
        "    # Create optimizer after model is on correct device\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    best_f1 = 0\n",
        "    train_losses = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            # Move batch to device (only tensor items)\n",
        "            batch_tensors = {}\n",
        "            for k, v in batch.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    batch_tensors[k] = v.to(device)\n",
        "                else:\n",
        "                    batch_tensors[k] = v  # Keep non-tensor items as is\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=batch_tensors['input_ids'],\n",
        "                attention_mask=batch_tensors['attention_mask'],\n",
        "                token_type_ids=batch_tensors['token_type_ids'],\n",
        "                e1_positions=batch_tensors['e1_positions'],\n",
        "                e2_positions=batch_tensors['e2_positions'],\n",
        "                labels=batch_tensors['labels']\n",
        "            )\n",
        "\n",
        "            loss = outputs['loss']\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss\n",
        "            loss_value = loss.item()\n",
        "            total_train_loss += loss_value\n",
        "            num_batches += 1\n",
        "            train_pbar.set_postfix({'loss': loss_value})\n",
        "\n",
        "        avg_train_loss = total_train_loss / num_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "                batch_tensors = {}\n",
        "                for k, v in batch.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        batch_tensors[k] = v.to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=batch_tensors['input_ids'],\n",
        "                    attention_mask=batch_tensors['attention_mask'],\n",
        "                    token_type_ids=batch_tensors['token_type_ids'],\n",
        "                    e1_positions=batch_tensors['e1_positions'],\n",
        "                    e2_positions=batch_tensors['e2_positions']\n",
        "                )\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(batch_tensors['labels'].cpu().numpy())\n",
        "\n",
        "        # Calculate F1 score\n",
        "        from sklearn.metrics import f1_score\n",
        "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "        val_f1_scores.append(val_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss = {avg_train_loss:.4f}, Val F1 = {val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'best_val_f1': best_f1\n",
        "            }, 'best_bert_model.pth')\n",
        "            print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
        "\n",
        "    return model, train_losses, val_f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX7r1u5hatbf"
      },
      "source": [
        "## Evaluation and Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bRawMffKasYJ"
      },
      "outputs": [],
      "source": [
        "         def evaluate_model(model, test_loader, id_to_relation):\n",
        "    \"\"\"Evaluate the trained model on test set\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_relations = []\n",
        "    all_texts = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            # Store text and relations for analysis\n",
        "            all_relations.extend(batch['relation'])\n",
        "            # Note: You might want to store the actual text too\n",
        "\n",
        "            # Move tensors to device\n",
        "            batch_tensors = {k: v.to(model.device) for k, v in batch.items()\n",
        "                           if k not in ['relation', 'is_positive']}\n",
        "\n",
        "            outputs = model(**batch_tensors)\n",
        "            logits = outputs['logits']\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch_tensors['labels'].cpu().numpy())\n",
        "\n",
        "    # Convert back to relation labels\n",
        "    pred_relations = [id_to_relation[pred] for pred in all_preds]\n",
        "    true_relations = [id_to_relation[label] for label in all_labels]\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*50)\n",
        "    print(classification_report(true_relations, pred_relations, zero_division=0))\n",
        "\n",
        "    # Calculate macro F1 score\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "\n",
        "    return pred_relations, true_relations, macro_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umoo1AwVa_0N"
      },
      "source": [
        "## Execute the Full Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCBfzzZ6a_au",
        "outputId": "d45d0aca-002a-4a0e-88a7-d272433c6548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 34294\n",
            "Validation samples: 7349\n",
            "Test samples: 7349\n",
            "Using batch size: 16\n",
            "Train batches: 1072\n",
            "Val batches: 230\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForRelationExtraction were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized successfully on cuda\n",
            "Model has 109544486 parameters\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1072 [00:00<?, ?it/s]/tmp/ipython-input-3431103970.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(sample['input_ids'], dtype=torch.long),\n",
            "/tmp/ipython-input-3431103970.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(sample['attention_mask'], dtype=torch.long),\n",
            "/tmp/ipython-input-3431103970.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'token_type_ids': torch.tensor(sample['token_type_ids'], dtype=torch.long),\n",
            "Training: 100%|██████████| 1072/1072 [52:18<00:00,  2.93s/it, loss=1.05]\n",
            "Validation: 100%|██████████| 230/230 [03:36<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.8422, Val F1 = 0.0414\n",
            "New best model saved with F1: 0.0414\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1072 [00:00<?, ?it/s]/tmp/ipython-input-3431103970.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(sample['input_ids'], dtype=torch.long),\n",
            "/tmp/ipython-input-3431103970.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(sample['attention_mask'], dtype=torch.long),\n",
            "/tmp/ipython-input-3431103970.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'token_type_ids': torch.tensor(sample['token_type_ids'], dtype=torch.long),\n",
            "Training: 100%|██████████| 1072/1072 [52:23<00:00,  2.93s/it, loss=0.444]\n",
            "Validation: 100%|██████████| 230/230 [03:36<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss = 0.6251, Val F1 = 0.1319\n",
            "New best model saved with F1: 0.1319\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1072 [00:00<?, ?it/s]/tmp/ipython-input-3431103970.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(sample['input_ids'], dtype=torch.long),\n",
            "/tmp/ipython-input-3431103970.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(sample['attention_mask'], dtype=torch.long),\n",
            "/tmp/ipython-input-3431103970.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'token_type_ids': torch.tensor(sample['token_type_ids'], dtype=torch.long),\n",
            "Training:  33%|███▎      | 359/1072 [17:34<34:50,  2.93s/it, loss=0.717]"
          ]
        }
      ],
      "source": [
        "# Load your processed data\n",
        "processed_file = os.path.join(OUTPUT_DIR, 'bert_baseline_data.pkl')\n",
        "with open(processed_file, 'rb') as f:\n",
        "    processed_data = pickle.load(f)\n",
        "\n",
        "# Load the splits\n",
        "train_file = os.path.join(OUTPUT_DIR, 'train_split.pkl')\n",
        "val_file = os.path.join(OUTPUT_DIR, 'validation_split.pkl')\n",
        "test_file = os.path.join(OUTPUT_DIR, 'test_split.pkl')\n",
        "\n",
        "with open(train_file, 'rb') as f:\n",
        "    train_samples = pickle.load(f)\n",
        "with open(val_file, 'rb') as f:\n",
        "    val_samples = pickle.load(f)\n",
        "with open(test_file, 'rb') as f:\n",
        "    test_samples = pickle.load(f)\n",
        "\n",
        "print(f\"Train samples: {len(train_samples)}\")\n",
        "print(f\"Validation samples: {len(val_samples)}\")\n",
        "print(f\"Test samples: {len(test_samples)}\")\n",
        "\n",
        "# Set your desired batch size here\n",
        "BATCH_SIZE = 16  # ← ADJUST THIS NUMBER (8, 16, 32, etc.)\n",
        "\n",
        "# Create DataLoaders with your chosen batch size\n",
        "train_loader, val_loader, test_loader = create_data_loaders(\n",
        "    train_samples,\n",
        "    val_samples,\n",
        "    test_samples,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(f\"Using batch size: {BATCH_SIZE}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "model, train_losses, val_f1_scores = train_bert_model_fixed(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_labels=processed_data['num_relations'],\n",
        "    tokenizer=tokenizer,\n",
        "    learning_rate=2e-5,\n",
        "    num_epochs=3\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "pred_relations, true_relations, test_f1 = evaluate_model(\n",
        "    model, test_loader, processed_data['id_to_relation']\n",
        ")\n",
        "\n",
        "print(f\"\\n=== BERT BASELINE RESULTS ===\")\n",
        "print(f\"Test Macro F1: {test_f1:.4f}\")\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bv1Wo2TphMo2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}